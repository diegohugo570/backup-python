{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegohugo570/backup-codigos/blob/main/02_Fundamentos_do_Langgraph_%7C_Curso_de_LangGraph_DascIA_Academy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamentos do Langgraph\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*pvsQBbW2fTYhvYcQIMCt3w.png\" width=700px>\n",
        "\n",
        "A inteligência artificial trouxe diversas possibilidades de aplicações. Uma dessas aplicações é a criação de **Fluxos de trabalho agênticos** — sistemas capazes de executar tarefas de forma autônoma.\n",
        "\n",
        "Apesar do hype em torno dos agentes, colocá-los em produção de forma **controlada e confiável** é um grande desafio.\n",
        "\n",
        "Para resolver isso, foi criado o **LangGraph**: um framework separado do LangChain, projetado para adicionar **controle, precisão e lógica condicional** nos fluxos de agentes e multiagentes.\n",
        "\n",
        "---\n",
        "\n",
        "## Configuração Inicial\n",
        "Antes de continuar, é importante garantir que você tenha sua chave da OpenAI:"
      ],
      "metadata": {
        "id": "Mcwr4J6H4Ect"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação das bibliotecas\n",
        "!pip install -qU langchain-openai langchain-community"
      ],
      "metadata": {
        "id": "fjubqxbC5ITX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSVVGCSt2kaX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = userdata.get(var)\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos de Chat\n",
        "\n",
        "Vamos utilizar **modelos de chat**, que funcionam com base em mensagens. Eles recebem uma sequência de mensagens e retornam uma resposta como se fosse uma conversa.\n",
        "\n",
        "Por padrão, vamos usar o `gpt-4o-mini`, que entrega um bom equilíbrio entre qualidade, preço e velocidade.\n"
      ],
      "metadata": {
        "id": "JMzWDOz349zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando os modelos de conversação\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt4o_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)"
      ],
      "metadata": {
        "id": "LZcDR4Yn44yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4o_chat.invoke(\"Olá!\")"
      ],
      "metadata": {
        "id": "-C57ubhSETW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enviando mensagens\n",
        "\n",
        "Os modelos de chat funcionam com diversos tipos de entradas. [Aqui](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface) você consegue ver todas as maneiras de \"chamar\" um modelo de linguagem de forma apropriada. Exemplo:"
      ],
      "metadata": {
        "id": "tXEzBk7H7_Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando os modelos de mensagem\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Definindo a lista de mensagemns\n",
        "mensagem_humano1 = HumanMessage(content=\"Meu nome é Anwar! Me conhece?\")\n",
        "mensagem_ia1 = AIMessage(content = \"Prazer, Anwar. Bom demais?\")\n",
        "mensagem_humano2 = HumanMessage(content = \"Bom demais, e por aí?\")\n",
        "mensagens = [mensagem_humano1, mensagem_ia1, mensagem_humano2]\n",
        "\n",
        "# Chamando o modelo com o método (invoke)\n",
        "resposta = gpt4o_chat.invoke(mensagens)\n",
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU4B27h96xiQ",
        "outputId": "ff17ff67-4a82-40c1-ad0e-0d2c3e2cb3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Estou aqui, pronto para ajudar! O que você gostaria de conversar ou saber?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 40, 'total_tokens': 57, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'id': 'chatcmpl-BHGr7Vr1XMqkVaRJrxgyPNtn4Oivw', 'finish_reason': 'stop', 'logprobs': None}, id='run-5575eeff-0089-46f3-b76c-f2c63d93160a-0', usage_metadata={'input_tokens': 40, 'output_tokens': 17, 'total_tokens': 57, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enviando uma string somente\n",
        "for chunk in gpt4o_chat.stream(\"Gere um poema sobre a apple.\"):\n",
        "  print(chunk.content, end = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vtKy_-hFD0i",
        "outputId": "39a1dc5d-ff51-4ce6-be60-733ca26720a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No pomar da vida, brilha a maçã,  \n",
            "Fruto do saber, da terra, da mão.  \n",
            "Vermelha como o amor, verde como a esperança,  \n",
            "Em cada mordida, uma doce dança.\n",
            "\n",
            "Cascas que reluzem sob o sol a brilhar,  \n",
            "Sussurros de histórias que vêm nos contar.  \n",
            "Do Éden ao agora, símbolo de escolha,  \n",
            "Na simplicidade, a beleza se acolha.\n",
            "\n",
            "Cores e sabores, um mundo a explorar,  \n",
            "Na mesa, um convite para nos deliciar.  \n",
            "Nutrição e prazer em cada fatia,  \n",
            "A maçã é poesia, é vida, é alegria.\n",
            "\n",
            "E no brilho da tela, a maçã se renova,  \n",
            "Tecnologia e arte, a inovação que prova.  \n",
            "Da fruta ao gadget, um ciclo sem fim,  \n",
            "A essência da maçã, sempre a nos guiar assim.\n",
            "\n",
            "Oh, maçã querida, em ti encontramos,  \n",
            "O doce da vida, os laços que formamos.  \n",
            "Em cada estação, um novo recomeço,  \n",
            "Na simplicidade, reside o nosso apreço."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enviando várias mensagens por vez\n",
        "mensagens = [\"Quanto é 20/2?\", \"Quanto é 10 + 10?\", \"Quanto é 5*3?\"]\n",
        "resposta = gpt4o_chat.batch(mensagens)\n",
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1SbAPdvFZFx",
        "outputId": "5a60028f-014d-4298-eb4b-79f6c8594a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='20 dividido por 2 é igual a 10.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 14, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'id': 'chatcmpl-BHGtUqiDWOmqKrsZZIMp98KzZV5ZY', 'finish_reason': 'stop', 'logprobs': None}, id='run-65e35455-6394-4623-a18a-ef28269f2efe-0', usage_metadata={'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " AIMessage(content='10 + 10 é igual a 20.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 15, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'id': 'chatcmpl-BHGtU2pcFycARZjuZC0F6EdywH9uf', 'finish_reason': 'stop', 'logprobs': None}, id='run-db6d55ed-0685-4e5b-8270-5af4852874b2-0', usage_metadata={'input_tokens': 15, 'output_tokens': 11, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " AIMessage(content='5 * 3 é igual a 15.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 14, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'id': 'chatcmpl-BHGtU7KZOm0MRvxozyJ2PamsO1n4z', 'finish_reason': 'stop', 'logprobs': None}, id='run-7c23307b-26e4-4432-ac9b-a510073fdae4-0', usage_metadata={'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ferramentas de Busca (Tavily)\n",
        "\n",
        "O Tavily é uma ferramenta de busca otimizada para LLMs, ideal para aplicações com RAG que necessitam de busca na web."
      ],
      "metadata": {
        "id": "zDcuC_JRmt9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a variável de ambiente\n",
        "_set_env(\"TAVILY_API_KEY\")\n",
        "\n",
        "# Importando a biblioteca\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Buscando os resultados\n",
        "tavily_search = TavilySearchResults(max_results=3)\n",
        "resultados = tavily_search.invoke(\"Quem é Anwar Hermuche?\")\n",
        "resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYoeethodgLh",
        "outputId": "a0c7aec9-6526-4e6b-f241-147b943d61b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Anwar Hermuche - Fundador e Professor - DascIA - LinkedIn',\n",
              "  'url': 'https://br.linkedin.com/in/anwarhermuche',\n",
              "  'content': 'Anwar  Hermuche\\nFounder of DascIA | | LLM Developer | AI Architect | AI Enginner | LLM | Python | AI Agents | LangChain | CrewAI | LangGraph | Phidata | Prompt Engineer | RAG\\nBarueri, Brazil\\n500 connections, 8664 followers [...] Cientista de dados at dti digital (https://www.linkedin.com/company/dtidigital/)\\nJan 2023 - Mar 2023\\nLavras, Minas Gerais, Brasil\\n\\nAnalista de dados at None (None)\\nJul 2020 - Oct 2022\\nFreelancer em análise de dados, utilizando as principais tecnologias como Python (juntamente a matplotlib, pandas, numpy etc.), SQL, PowerBI e Excel.\\n\\nDesenvolvimento de dashboards e análises para cursinhos pré-vestibular utilizando os microdados do ENEM. [...] Sou professor de diversos módulos, incluindo Portfólio, SQL, Python, Machine Learning e Estatística. Nossa equipe é composta por mais oito especialistas dedicados, todos profissionais atuantes no campo, garantindo uma educação que combina teoria com prática real. Além da minha contribuição como educador, assumo um papel na elaboração de estratégias de marketing, desempenhando a função de copywriter para ampliar nosso alcance e impacto.',\n",
              "  'score': 0.7323053},\n",
              " {'title': 'About – Anwar Hermuche – Medium',\n",
              "  'url': 'https://medium.com/@anwarhermuche/about',\n",
              "  'content': 'About Anwar Hermuche on Medium. Founder of DascIA and AI Professional https://www.dascia.com.br.',\n",
              "  'score': 0.72042775},\n",
              " {'title': 'Anwar Hermuche - Medium',\n",
              "  'url': 'https://medium.com/@anwarhermuche',\n",
              "  'content': 'Read writing from Anwar Hermuche on Medium. Founder of DascIA and AI Professional https://www.dascia.com.br. Every day, Anwar Hermuche and thousands of',\n",
              "  'score': 0.6773251}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saídas Estruradas\n",
        "Nós utilizamos saídas estruturadas para receber como resposta de um LM um output estrurado e com validação nativa de tipos."
      ],
      "metadata": {
        "id": "vZt6O0vcGVj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição do schema\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class Pesquisa(BaseModel):\n",
        "  nome_pessoa: str = Field(description = \"O nome da pessoa que a IA está conversando.\")\n",
        "  idade_pessoa: int = Field(description = \"A idade da pessoa qeu a IA está conversando.\")\n",
        "  conhece_anwar: bool = Field(description = \"Se a pessoa que a IA está conversando conhece o Anwar Hermuche.\")\n",
        "  caracteristas_pessoa: List[str] = Field(description=\"Quais são as características da pessoa que a IA está conversando. No caso, sentimentalmente.\")"
      ],
      "metadata": {
        "id": "-I2vk0aJGhl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionando a saída estruturada\n",
        "gpt4o_estruturado = gpt4o_chat.with_structured_output(Pesquisa)"
      ],
      "metadata": {
        "id": "vDsYMpK9HTNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chamando o GPT-4o estruturado\n",
        "resposta_estruturada = gpt4o_estruturado.invoke([\n",
        "    HumanMessage(content=\"Olá, sou o Carlos e tenho 84 anos.\"),\n",
        "    AIMessage(content = \"Olá, Carlos. Como posso te ajudar hoje?\"),\n",
        "    HumanMessage(content = \"Conheço o Anwar do YouTube. Sou amigável e curioso de IA.\")\n",
        "    ])\n",
        "\n",
        "resposta_estruturada"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwlJ6KznHa1h",
        "outputId": "cc6bc731-02ad-431f-bc4c-43c609cc6b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pesquisa(nome_pessoa='Carlos', idade_pessoa=84, conhece_anwar=True, caracteristas_pessoa=['amigável', 'curioso de IA'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Agora que você configurou seu ambiente e entendeu como usar os modelos de chat, vamos aprofundar no LangGraph e construir fluxos reais com múltiplos agentes e lógica baseada em estado compartilhado."
      ],
      "metadata": {
        "id": "0AowxJGtnl87"
      }
    }
  ]
}