{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JoYbE9QqOkOn",
        "i9hcfhtBbkZM",
        "5kDH_IVDOpn9",
        "I7ldc8ZBuDSA",
        "8o5G9gqQprLf",
        "3-Ko5AwXqLjx",
        "v8IvW2qAqGaa",
        "y5vZrt24qJmF",
        "da2lc7WOQLmb",
        "k0IOUdSU2nFD",
        "khVIZkqaoLg4",
        "ubBzf5PjvXRq",
        "GzC2_geCzTEX",
        "U2jt8xCr2iPd",
        "ErNyQdTc4X95",
        "hFRzmPXI4x8E",
        "veDgfCnD69M0",
        "ECRk2JTx7sBx",
        "dl0yTeDUR4Uj",
        "s917yitmN78t",
        "49XaMcWcVmUq",
        "Jf4uizqzOAM2",
        "g13KmtKYZ9nJ",
        "Tdr1KLQZbZEI",
        "QWVUb09ybdXM",
        "Y6-QcCvFbpeR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegohugo570/backup-python/blob/main/08_Curso_Python_M%C3%B3dulo_8_Forma%C3%A7%C3%A3o_Engenheiro_de_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M√≥dulo 8: LangChain & Agentes (Engenharia Cognitiva)**\n",
        "\n",
        "Link do curso: [clique aqui](https://dascia.academy/)\n",
        "\n",
        "> **Objetivo:** Transformar o Python de uma linguagem de script em uma linguagem de **orquestra√ß√£o de racioc√≠nio**. Voc√™ aprender√° a criar sistemas que n√£o apenas \"falam\", mas que **planejam**, **agem** no mundo real (APIs, Banco de Dados) e **lembram** do passado."
      ],
      "metadata": {
        "id": "47bh7wvFOfOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Aula 1: O Ecossistema LangChain (Arquitetura)**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/langchain-ai/langchain/e1d113ea84a2edcf4a7709fc5be0e972ea74a5d9/docs/static/svg/langchain_stack_062024.svg\">\n",
        "</div>\n",
        "\n",
        "Antes de codar, precisamos entender onde estamos pisando. O ecossistema da LangChain cresceu e se dividiu em tr√™s pilares fundamentais.\n",
        "\n",
        "1. **LangChain (O Orquestrador):** A biblioteca Python que conecta os modelos (OpenAI, Anthropic) ao seu c√≥digo. √â aqui que constru√≠mos as \"Chains\" (cadeias de pensamento).\n",
        "2. **LangGraph (O Controle):** Para criar agentes complexos que precisam de loops, condicionais (if/else) e persist√™ncia de estado. √â a evolu√ß√£o das \"Chains\" lineares.\n",
        "3. **LangSmith (A Observabilidade):** Uma plataforma para debugar o que sua IA est√° pensando. Como o LLM √© uma \"caixa preta\", o LangSmith permite ver os logs de cada etapa do racioc√≠nio.\n",
        "\n",
        "Neste curso, focaremos no **LangChain** para constru√ß√£o de ferramentas e agentes r√°pidos."
      ],
      "metadata": {
        "id": "JoYbE9QqOkOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A Arquitetura Modular do LangChain**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://python.langchain.ac.cn/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png\">\n",
        "</div>\n",
        "\n",
        "Antes de instalar, √© crucial entender que o LangChain moderno n√£o √© mais um \"monolito\". Ele foi refatorado em uma arquitetura de camadas para ser mais seguro e leve em produ√ß√£o.\n",
        "\n",
        "Ao inv√©s de instalar uma coisa gigante, montamos nosso ambiente como pe√ßas de Lego:\n",
        "\n",
        "* **`langchain-core`**: A funda√ß√£o. Define as interfaces padr√£o (o que √© uma \"Mensagem\", o que √© um \"Prompt\"). √â a √∫nica depend√™ncia obrigat√≥ria para tudo funcionar.\n",
        "* **Integra√ß√µes (`langchain-openai`, `langchain-anthropic`)**: Os provedores de IA agora vivem em pacotes isolados. Isso significa que podemos atualizar a vers√£o da OpenAI sem quebrar o resto do sistema.\n",
        "* **`langchain`**: O orquestrador. Cont√©m a l√≥gica das \"Chains\", estrat√©gias de busca (Retrieval) e a \"cola\" que une os componentes.\n",
        "* **`langchain-community`**: Onde vivem as integra√ß√µes mantidas pela comunidade (ferramentas de terceiros, vetores de bancos espec√≠ficos, etc.).\n",
        "\n",
        "√â por isso que, no comando abaixo, n√£o instalamos apenas `langchain`, mas tamb√©m o conector espec√≠fico `langchain-openai`."
      ],
      "metadata": {
        "id": "i9hcfhtBbkZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando o langchain-openai\n"
      ],
      "metadata": {
        "id": "g-ygCVQUm1PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando as vari√°veis de ambiente (userdata e os)\n",
        "\n",
        "# Configura√ß√£o de Chaves (Use os Segredos do Colab ou input manual com getpass)\n"
      ],
      "metadata": {
        "id": "eeVczqsuOmaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aula 2: Chat Models**\n",
        "\n",
        "Um \"Chat Model\" moderno n√£o funciona apenas recebendo texto e cuspindo texto. Na verdade, n√£o APENAS assim...\n",
        "\n",
        "Ele pode operar tamb√©m sobre uma **Lista de Mensagens** que representa o estado atual de uma conversa."
      ],
      "metadata": {
        "id": "5kDH_IVDOpn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 String vs. Lista de Mensagens (O Polimorfismo)**\n",
        "\n",
        "O m√©todo `.invoke()` do LangChain √© polim√≥rfico.\n",
        "\n",
        "1. **Entrada Simples (String):** Se voc√™ fizer `modelo.invoke(\"Oi\")`, o LangChain converte isso automaticamente, \"por baixo do cap√¥\", para `[HumanMessage(content=\"Oi\")]`. √â √≥timo para testes r√°pidos.\n",
        "2. **Entrada de um Chat (Lista de Mensagens):** Para enviar mensagens simulando uma conversa√ß√£o entre um humano e a IA, a utiliza√ß√£o de listas de mensagens √© a mais adequada."
      ],
      "metadata": {
        "id": "I7ldc8ZBuDSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando o ChatOpenAI e enviando uma mensagem\n",
        "\n",
        "\n",
        "# Instanciando o modelo (gpt-4o-mini e temperatura zero)\n",
        "\n",
        "\n",
        "# Enviando a primeira mensagem (string)\n"
      ],
      "metadata": {
        "id": "MuafBzySoLGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 A Anatomia das Mensagens**\n",
        "\n",
        "O LangChain padroniza a comunica√ß√£o com qualquer modelo (seja OpenAI, Llama ou Anthropic) usando tr√™s classes principais:"
      ],
      "metadata": {
        "id": "8o5G9gqQprLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`SystemMessage` (Instru√ß√£o Principal):**\n",
        "- √â a instru√ß√£o que orienta o comportamento do modelo.\n",
        "- *Ex:* \"Voc√™ √© um especialista em SQL que responde apenas com c√≥digo, sem explica√ß√µes.\"\n",
        "- Diferente do `HumanMessage`, o modelo trata isso como uma configura√ß√£o persistente, n√£o como parte do di√°logo."
      ],
      "metadata": {
        "id": "3-Ko5AwXqLjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando a classe SystemMessae\n",
        "\n",
        "\n",
        "# Vari√°veis do aplicativo\n",
        "# - Nome do usu√°rio\n",
        "# - Principal Dificuldade\n",
        "# - Idade\n",
        "# - Experi√™ncia em IA (Pouca, M√©dia, Muita)\n",
        "\n",
        "\n",
        "# Definindo a instru√ß√£o principal\n",
        "\n",
        "\n",
        "# Criando o System Message\n"
      ],
      "metadata": {
        "id": "gtgHhijjqGGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`HumanMessage` (O Input):**\n",
        "* Representa a fala do usu√°rio.\n",
        "* Em modelos multimodais (como GPT-4o), o `content` dessa mensagem pode conter n√£o s√≥ texto, mas tamb√©m imagens em Base64 ou URLs."
      ],
      "metadata": {
        "id": "v8IvW2qAqGaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando a classe HumanMessage\n",
        "\n",
        "\n",
        "# Definindo a mensagem incial\n",
        "\n",
        "\n",
        "# Criando o Human Message\n"
      ],
      "metadata": {
        "id": "YXBq_xuAoK5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`AIMessage` (O Output):**\n",
        "* √â o que o modelo retorna.\n",
        "* N√£o cont√©m apenas o texto (`.content`), mas tamb√©m metadados cruciais (`.response_metadata`) como: contagem de tokens gastos, motivo da parada (finish_reason) e **Logprobs** (que veremos mais a frente)"
      ],
      "metadata": {
        "id": "y5vZrt24qJmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enviando o System e Human Messages para obter o AI Message\n",
        "\n",
        "\n",
        "# Inspecionando o AI Message\n"
      ],
      "metadata": {
        "id": "RrSdl9cRsYsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo append na lista de mensagens e adicionando um novo Human Messae\n",
        "\n",
        "\n",
        "# Enviando nova lista de mensagens\n",
        "\n",
        "\n",
        "# Inspecionando a resposta\n"
      ],
      "metadata": {
        "id": "usO5RiK_smsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† **Exerc√≠cios de Fixa√ß√£o (Aula 2)**\n",
        "\n",
        "**Exerc√≠cio 2.1: Few-Shot Learning (Ensinando por Exemplos)**\n",
        "\n",
        "Muitas vezes, apenas dar instru√ß√µes no System Prompt n√£o √© suficiente; precisamos dar exemplos pr√°ticos para o modelo entender o padr√£o desejado. Isso se chama \"Few-Shot Prompting\".\n",
        "\n",
        "1. Crie uma lista de mensagens que ensine o modelo a converter **g√≠rias** em **linguagem corporativa ultra-formal**.\n",
        "2. **SystemMessage:** \"Voc√™ √© um assistente corporativo que converte linguagem informal em formal.\"\n",
        "3. **Exemplo 1 (Human):** \"E a√≠ mano, beleza?\" -> **(AI):** \"Ol√°, prezado colega. Como tem passado?\"\n",
        "4. **Exemplo 2 (Human):** \"T√¥ fora dessa reuni√£o.\" -> **(AI):** \"Infelizmente, declino o convite para o alinhamento.\"\n",
        "5. **Teste (Human):** \"Bora fechar isso logo, t√° demorando muito.\"\n",
        "6. Invoque o modelo passando essa lista hist√≥rica e veja se ele segue o padr√£o no final."
      ],
      "metadata": {
        "id": "da2lc7WOQLmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "9DXlmyIvQl9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exerc√≠cio 2.2: Contexto Manual (A Mem√≥ria da IA)**\n",
        "\n",
        "Modelos de chat s√£o \"stateless\" (sem mem√≥ria). Se voc√™ perguntar \"Quem √© o Batman?\" e depois perguntar \"E quem √© o parceiro dele?\", ele n√£o saber√° de quem voc√™ est√° falando na segunda vez, a menos que voc√™ envie o hist√≥rico completo.\n",
        "\n",
        "1. Defina um `SystemMessage`.\n",
        "2. Crie uma `HumanMessage` perguntando: \"Quem escreveu Dom Casmurro?\".\n",
        "3. Invoque o modelo e guarde a resposta (o objeto `AIMessage`) numa vari√°vel `resposta_1`.\n",
        "4. Agora, crie uma **nova lista** contendo:\n",
        "* O System Message original.\n",
        "* A pergunta original (\"Quem escreveu...\").\n",
        "* A `resposta_1` que voc√™ guardou.\n",
        "* Uma nova `HumanMessage`: \"Em que ano foi publicado?\".\n",
        "\n",
        "\n",
        "5. Invoque o modelo com E sem essa lista completa. Se ele responder o ano correto (1899), significa que ele manteve o contexto.\n"
      ],
      "metadata": {
        "id": "91y-4RtrQv08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "kH8_dSCuQyHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aula 3: Logprobs e Perplexidade**\n",
        "\n",
        "Um *Chat Model* moderno n√£o gera texto ‚Äúno escuro‚Äù. A cada token produzido, o modelo calcula uma **distribui√ß√£o de probabilidades** sobre o vocabul√°rio, da qual emergem m√©tricas fundamentais como a **perplexidade**.\n",
        "\n",
        "O foco dessa aula n√£o √© \"prompt bonito\", mas **leitura matem√°tica do comportamento do modelo**."
      ],
      "metadata": {
        "id": "k0IOUdSU2nFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Perplexidade e Logprobs (defini√ß√£o t√©cnica)**\n",
        "\n",
        "Considere uma sequ√™ncia de tokens $(t_1, \\dots, t_N)$ e um modelo que atribui probabilidades condicionais\n",
        "$$\n",
        "p_i = P(t_i \\mid t_{<i})\n",
        "$$\n",
        "\n",
        "A **perplexidade** (PPL) √© definida como a exponencial da *cross-entropy* m√©dia por token:\n",
        "\n",
        "$$\n",
        "\\mathrm{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log p_i\\right)\n",
        "$$\n",
        "\n",
        "Se voc√™ tem **logprobs** diretamente (isto √©, $(\\ell_i = \\log p_i)$), ent√£o:\n",
        "\n",
        "$$\n",
        "\\mathrm{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\ell_i\\right)\n",
        "$$\n",
        "\n",
        "Interpreta√ß√£o: PPL √© o \"n√∫mero efetivo de escolhas equiprov√°veis\" por token.  \n",
        "\n",
        "Ex.: PPL = 10 sugere que, em m√©dia, o modelo estava t√£o incerto quanto escolher entre 10 tokens equiprov√°veis a cada passo."
      ],
      "metadata": {
        "id": "khVIZkqaoLg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando numpy\n",
        "\n",
        "\n",
        "# Exemplo: logprobs de tokens (log natural)\n",
        "\n",
        "\n",
        "# C√°lculo da perplexidade\n"
      ],
      "metadata": {
        "id": "HsbIiAfEu8Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Duas implementa√ß√µes equivalentes (probs vs logprobs)**\n",
        "\n",
        "A defini√ß√£o original da perplexidade usa **probabilidades** porque ela √©, essencialmente, uma **m√©dia geom√©trica do inverso das probabilidades** atribu√≠das pelo modelo aos tokens observados.  \n",
        "\n",
        "O uso do **inverso** √© fundamental porque a perplexidade n√£o mede confian√ßa, mas **incerteza**: probabilidades altas indicam tokens f√°ceis de prever e devem contribuir pouco para a m√©trica, enquanto probabilidades baixas indicam surpresa e devem aumentar significativamente o valor final. Ao inverter \\(p_i\\), tokens improv√°veis passam a gerar valores grandes, penalizando o modelo de forma proporcional √† dificuldade da predi√ß√£o.\n",
        "\n",
        "A m√©dia geom√©trica √© a escolha natural quando lidamos com **quantidades multiplicativas** (probabilidades condicionais ao longo de uma sequ√™ncia), pois preserva a escala relativa e penaliza fortemente eventos improv√°veis.\n",
        "\n",
        "Formalmente, para uma sequ√™ncia de probabilidades $(p_1, \\dots, p_N)$, a perplexidade √© definida como:\n",
        "\n",
        "1) **Via probabilidades (m√©dia geom√©trica)**\n",
        "\n",
        "$$\n",
        "\\mathrm{PPL}\n",
        "=\n",
        "\\left(\\prod_{i=1}^{N}\\frac{1}{p_i}\\right)^{\\frac{1}{N}}\n",
        "$$\n",
        "\n",
        "Essa express√£o pode ser lida como:\n",
        "> ‚ÄúEm m√©dia, o modelo se comporta como se estivesse escolhendo entre  \n",
        "$(\\mathrm{PPL})$ alternativas equiprov√°veis a cada token.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "2) **Via logprobs (forma computacionalmente est√°vel)**\n",
        "\n",
        "Para obter uma forma numericamente est√°vel, aplicamos o logaritmo e exploramos suas propriedades:\n",
        "\n",
        "$$\n",
        "\\log(\\mathrm{PPL})\n",
        "=\n",
        "\\log\\!\\left(\\left(\\prod_{i=1}^{N}\\frac{1}{p_i}\\right)^{\\frac{1}{N}}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "=\n",
        "\\frac{1}{N}\\log\\!\\left(\\prod_{i=1}^{N}\\frac{1}{p_i}\\right)\n",
        "=\n",
        "\\frac{1}{N}\\sum_{i=1}^{N}\\log\\!\\left(\\frac{1}{p_i}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "=\n",
        "-\\frac{1}{N}\\sum_{i=1}^{N}\\log(p_i)\n",
        "$$\n",
        "\n",
        "Aplicando a fun√ß√£o exponencial em ambos os lados, obtemos a forma final:\n",
        "\n",
        "$$\n",
        "\\mathrm{PPL}\n",
        "=\n",
        "\\exp\\!\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log(p_i)\\right)\n",
        "$$\n",
        "\n",
        "Essa reformula√ß√£o mant√©m o mesmo significado estat√≠stico da m√©dia geom√©trica,  \n",
        "mas √© **muito mais est√°vel numericamente**, raz√£o pela qual √© a forma usada na pr√°tica em modelos de linguagem."
      ],
      "metadata": {
        "id": "ubBzf5PjvXRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos criar um caso \"realista\" com np.full() de sequ√™ncia longa com probabilidades pequenas e seus logs\n"
      ],
      "metadata": {
        "id": "auoB7t9zynFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma inst√°vel (vai dar inf por overflow em 1/prob, ou underflow dependendo do caminho)\n",
        "\n",
        "\n",
        "# Analisando resultado\n"
      ],
      "metadata": {
        "id": "lkmbrA14nNan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDceYV98Mawp"
      },
      "outputs": [],
      "source": [
        "# Forma est√°vel\n",
        "\n",
        "\n",
        "# Analisando resultado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3 Prova: limite inferior da perplexidade √© 1**\n",
        "\n",
        "Como $(0 < p_i \\le 1)$, ent√£o $(\\log(p_i) \\le 0)$ (a base √© 2.71828...)\n",
        "\n",
        "Logo:\n",
        "\n",
        "$$\n",
        "-\\frac{1}{N}\\sum_{i=1}^{N}\\log(p_i) \\ge 0\n",
        "$$\n",
        "\n",
        "Aplicando exponencial:\n",
        "\n",
        "$$\n",
        "\\mathrm{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log(p_i)\\right) \\ge \\exp(0)=1\n",
        "$$\n",
        "\n",
        "**Quando d√° exatamente 1?**\n",
        "\n",
        "Somente se $(\\log(p_i)=0)$ para todo $(i)$, isto √©, $(p_i=1)$ para todo token.\n",
        "Ou seja: **o modelo tem certeza absoluta do pr√≥ximo token em cada passo (caso degenerado)**.\n",
        "\n",
        "Implica√ß√£o pr√°tica:\n",
        "\n",
        "- $(\\mathrm{PPL} \\approx 1)$ significa previsibilidade extrema (geralmente texto trivial/repetitivo ou \"teacher forcing\" perfeito).\n",
        "- Valores maiores indicam mais incerteza/surpresa m√©dia por token.\n"
      ],
      "metadata": {
        "id": "GzC2_geCzTEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma fun√ß√£o que recebe um array de logprobs e retorna a perplexidade\n",
        "\n",
        "\n",
        "# Caso m√≠nimo: p_i = 1 => log p_i = 0 => PPL = 1\n"
      ],
      "metadata": {
        "id": "ZN86IDLvzw7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.4. Detalhes t√©cnicos que importam**"
      ],
      "metadata": {
        "id": "U2jt8xCr2iPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.4.1 Base do log**\n",
        "\n",
        "- Se voc√™ usa $log$ natural: PPL = $\\exp(\\cdot)$\n",
        "- Se usa $\\log_2$: PPL = $2^{(\\cdot)}$. O valor muda de escala, mas a ordena√ß√£o/compara√ß√£o permanece consistente se voc√™ for coerente.\n"
      ],
      "metadata": {
        "id": "zT34vmYm4N4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.4.2 Perplexidade vs ‚Äúconfian√ßa‚Äù**\n",
        "\n",
        "Logprob alto (pr√≥ximo de 0) por token tende a reduzir PPL, mas:\n",
        "- PPL √© uma **m√©dia geom√©trica** do inverso das probs.\n",
        "- Um √∫nico token muito improv√°vel pode puxar PPL pra cima com for√ßa (isso √© caracter√≠stica, n√£o bug)."
      ],
      "metadata": {
        "id": "Lx5-YyuQ4PbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.4.3 Compara√ß√£o s√≥ faz sentido com mesma tokeniza√ß√£o e mesma distribui√ß√£o**\n",
        "\n",
        "Trocar tokenizer muda N e muda os $p_i$. Comparar PPL entre modelos/tokenizers diferentes pode ser bem enganoso."
      ],
      "metadata": {
        "id": "rMKLCh9S4UbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.4.4. PPL n√£o \"prova alucina√ß√£o\", mas √© um √≥timo sinal**\n",
        "\n",
        "PPL alta sugere que a sequ√™ncia est√° fora do \"caminho comum\" do modelo (pode ser erro, pode ser criatividade, pode ser dom√≠nio raro etc.)."
      ],
      "metadata": {
        "id": "ErNyQdTc4X95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstra√ß√£o do \"um token ruim estraga a festa\"\n"
      ],
      "metadata": {
        "id": "jP4lFXxYzIh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.4. Calculando a perplexidade real**\n",
        "Agora que j√° vimos toda a teoria matem√°tica por tr√°s dos logprobs e da perplexidade, vamos calcular a perplexidade real de uma resposta da IA."
      ],
      "metadata": {
        "id": "hFRzmPXI4x8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando o modelo (com logprobs = True)\n",
        "\n",
        "\n",
        "# Enviando mensagem pedindo um poema\n"
      ],
      "metadata": {
        "id": "JcHYUjZU48Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando a lista de logprobs\n",
        "\n",
        "\n",
        "# Calculando a perplexidade\n"
      ],
      "metadata": {
        "id": "u7MHPSnv5Cgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† Exerc√≠cios de Fixa√ß√£o (Aula 3)**\n",
        "\n",
        "**Exerc√≠cio 3.1: Fato vs. Criatividade (O Teste de Incerteza)**\n",
        "\n",
        "A perplexidade mede o qu√£o \"surpreso\" o modelo fica com a pr√≥pria resposta. Respostas factuais tendem a ter perplexidade baixa (o modelo tem certeza). Respostas criativas ou alucina√ß√µes tendem a ter perplexidade alta.\n",
        "\n",
        "1. Crie dois prompts:\n",
        "* `prompt_fato`: \"Qual √© a capital da Fran√ßa?\"\n",
        "* `prompt_criativo`: \"Invente uma nova cor que n√£o existe e descreva o cheiro dela.\"\n",
        "\n",
        "\n",
        "2. Invoque o modelo duas vezes: a primeira vez com `temperature=0` e a segunda com `temperature=0.8` para ambos.\n",
        "3. Calcule a perplexidade de cada resposta.\n",
        "4. Imprima os quatro valores e compare. Qual √© maior?"
      ],
      "metadata": {
        "id": "veDgfCnD69M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "LpLnbGNj7Ac6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exerc√≠cio 3.5.2: O \"\"\"Detector de Alucina√ß√£o\"\"\" (Threshold)**\n",
        "\n",
        "Em sistemas cr√≠ticos (como um bot m√©dico ou jur√≠dico), n√£o podemos aceitar respostas duvidosas.\n",
        "\n",
        "1. Crie uma fun√ß√£o `verificar_confianca(resposta, limite=1.2) -> str`.\n",
        "2. Ela deve calcular a perplexidade.\n",
        "3. Se a perplexidade for **menor** que o limite, retorne \"‚úÖ Confian√ßa Alta (Seguro)\".\n",
        "4. Se for **maior**, retorne \"‚ö†Ô∏è Confian√ßa Baixa (Risco de Alucina√ß√£o)\".\n",
        "5. Teste a fun√ß√£o com uma pergunta sem sentido (ex: \"Qual o CPF do Pato Donald?\")."
      ],
      "metadata": {
        "id": "XutcL-a37Dw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "RTqLFtZ07FLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aula 4: Structured Output (A Ponte Texto-Software)**\n",
        "\n",
        "Softwares n√£o consomem frases, consomem **JSON**.\n",
        "O m√©todo `.with_structured_output` usa o **Pydantic** para for√ßar o LLM a preencher um formul√°rio r√≠gido. Se o modelo errar o tipo (ex: colocar texto onde deveria ser data), o Pydantic valida e o LangChain pode at√© pedir para ele corrigir automaticamente."
      ],
      "metadata": {
        "id": "ECRk2JTx7sBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando classes do Pydantic e Typing\n",
        "\n",
        "\n",
        "# Definindo schema do Pydantic para extrair de um e-mail:\n",
        "# - Nome do remetente\n",
        "# - Data de envio\n",
        "# - Categoria (comercial, suporte, pessoal, spam)\n",
        "# - Resumo (de at√© 10 palavras)\n",
        "\n"
      ],
      "metadata": {
        "id": "4JvvskTAPMzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando o modelo\n",
        "\n",
        "\n",
        "# Associando a classe ao modelo\n"
      ],
      "metadata": {
        "id": "tmGcZhxJPg62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E-mail exemplo\n"
      ],
      "metadata": {
        "id": "co-p-9oDPqSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisando o resultado\n"
      ],
      "metadata": {
        "id": "c4cuISVhPrqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† Exerc√≠cios de Fixa√ß√£o (Aula 4)**\n",
        "\n",
        "**Exerc√≠cio 1: Triagem Autom√°tica de Suporte (Ticket Routing)**\n",
        "\n",
        "Em empresas grandes, ler cada ticket de suporte para saber para quem mandar √© caro. Sua tarefa √© criar um sistema que l√™ a reclama√ß√£o e retorna um objeto pronto para ser salvo no banco de dados do Jira/Zendesk.\n",
        "\n",
        "1. Crie uma classe Pydantic `TicketSuporte` com os campos:\n",
        "* `cot`: Deve ser uma string representando a cadeia de pensamento da IA para realizar a tarefa\n",
        "* `setor`: Deve ser um `Literal` com as op√ß√µes: \"Financeiro\", \"Suporte T√©cnico\", \"Vendas\".\n",
        "* `prioridade`: Um inteiro de 1 (Baixa) a 5 (Cr√≠tica).\n",
        "* `tags`: Uma lista de strings com palavras-chave do problema (ex: [\"login\", \"senha\", \"erro 404\"]).\n",
        "* `requer_humano`: Booleano. Se o cliente estiver muito irritado, True. Caso contr√°rio, False.\n",
        "\n",
        "\n",
        "2. Passe a reclama√ß√£o: *\"Cara, estou tentando pagar minha fatura h√° 3 dias e o site d√° erro 500! Vou processar voc√™s se cortarem meu servi√ßo!\"*\n",
        "3. Imprima o objeto resultante e verifique se ele detectou corretamente o setor \"Financeiro\" e a prioridade alta."
      ],
      "metadata": {
        "id": "dl0yTeDUR4Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "n_xxZl6uR8A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exerc√≠cio 2: O Extrator de Notas Fiscais (OCR Inteligente)**\n",
        "\n",
        "Voc√™ est√° construindo um app de gest√£o financeira. O usu√°rio tira foto da nota (que vira texto via OCR), mas o texto vem sujo. Voc√™ precisa estruturar isso.\n",
        "\n",
        "1. Crie uma classe `ItemCompra` com `produto` (str) e `valor` (float).\n",
        "2. Crie uma classe principal `NotaFiscal` com:\n",
        "* `nome_loja`: str\n",
        "* `data`: str (formato YYYY-MM-DD)\n",
        "* `itens`: uma `list[ItemCompra]` (aqui est√° o desafio: extrair uma lista de objetos!)\n",
        "* `valor_total`: float\n",
        "\n",
        "\n",
        "3. Passe o texto bagun√ßado:\n",
        "*\"Compra no Mercado do Jo√£o, dia 12 de mar√ßo de 2024. Levei 2 leites por 10 reais cada (total 20) e um p√£o por 5.90. Deu 25.90 tudo.\"*\n",
        "4. Veja se o modelo consegue quebrar os itens individualmente dentro da lista."
      ],
      "metadata": {
        "id": "31h7HVWDR-S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "trFuB6TwSAHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Aula 5: Tool Calling (Dando Bra√ßos √† IA)**\n",
        "\n",
        "LLMs vivem numa caixa fechada. Eles n√£o sabem a temperatura atual nem acessar a internet. **Tools** s√£o a ponte.\n",
        "\n",
        "O fluxo de Tool Calling √©:\n",
        "\n",
        "1. **LLM decide:** \"Preciso chamar a fun√ß√£o `temperatura` com o argumento `cidade='sp'`\".\n",
        "2. **Pausa:** O LLM para de gerar texto e te devolve um objeto `tool_call`.\n",
        "3. **Python Executa:** Voc√™ roda a fun√ß√£o real.\n",
        "4. **Retorno:** Voc√™ devolve o resultado para o LLM.\n",
        "5. **Resposta Final:** O LLM l√™ o resultado e formula a frase final para o usu√°rio."
      ],
      "metadata": {
        "id": "s917yitmN78t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando o decorator tool\n",
        "\n",
        "\n",
        "# Criando a Ferramenta (consultar_temperatura)\n"
      ],
      "metadata": {
        "id": "JWBtRh9jTyuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando o modelo\n",
        "\n",
        "\n",
        "# Associando ao modelo a ferramenta\n"
      ],
      "metadata": {
        "id": "2WQZMGwuT7L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perguntando a temperatura atual de uma cidade\n",
        "\n",
        "\n",
        "# Analisando a resposta\n"
      ],
      "metadata": {
        "id": "lgkWcsH3T97N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ele pediu pra chamar alguma ferramenta? Qual? Quais os argumentos?\n"
      ],
      "metadata": {
        "id": "P4DC-_TdUSXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionando a inten√ß√£o ao hist√≥rico\n"
      ],
      "metadata": {
        "id": "K0eo_IKDUbYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`ToolMessage` (O Retorno da Fun√ß√£o):**\n",
        "* Representa o *resultado* pr√°tico vindo do \"mundo real\" (do seu c√≥digo Python) ap√≥s o modelo solicitar uma a√ß√£o.\n",
        "* *Ex:* \"22¬∞C\" (o retorno de `get_temperatura`) ou \"Erro: Usu√°rio n√£o encontrado\".\n",
        "* **Ponto Chave:** Ele deve conter obrigatoriamente o `tool_call_id`. √â assim que o modelo sabe conectar essa resposta espec√≠fica (\"22¬∞C\") √† pergunta espec√≠fica que ele fez anteriormente."
      ],
      "metadata": {
        "id": "CKMaq_OdVEu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando fluxo de execu√ß√£o das tools\n"
      ],
      "metadata": {
        "id": "_dGx0h_mUkOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo a resposta final\n"
      ],
      "metadata": {
        "id": "3UCnfdlgUvsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† Exerc√≠cios de Fixa√ß√£o (Aula 5)**\n",
        "\n",
        "**Exerc√≠cio 4.1: O Agendador de Salas (Gest√£o de Recursos)**\n",
        "\n",
        "Voc√™ est√° criando o bot interno de uma empresa. Os funcion√°rios vivem perguntando no Slack se as salas de reuni√£o est√£o livres. O sistema sabe quem est√° perguntando (pelo user do Slack), mas n√£o sabe **qual sala** e **qual hor√°rio** eles querem. A IA precisa extrair isso do texto.\n",
        "\n",
        "1. Crie uma ferramenta `@tool` chamada `verificar_disponibilidade_sala`.\n",
        "2. Ela deve receber dois argumentos obrigat√≥rios:\n",
        "* `nome_sala` (str): Pode ser \"Azul\", \"Vermelha\" ou \"Executiva\".\n",
        "* `horario` (str): O hor√°rio desejado (ex: \"14:00\", \"15:00\").\n",
        "\n",
        "\n",
        "3. Dentro da fun√ß√£o, use este dicion√°rio para simular a agenda do dia:\n",
        "```python\n",
        "AGENDA_HOJE = {\n",
        "    \"azul\": [\"09:00\", \"10:00\", \"14:00\"], # Hor√°rios OCUPADOS\n",
        "    \"vermelha\": [\"11:00\", \"15:00\"],\n",
        "    \"executiva\": [] # Livre o dia todo\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "4. A l√≥gica √©:\n",
        "* Se a sala n√£o existir, informe ao agente que a sala n√£o existe\n",
        "* Se o hor√°rio estiver na lista, informe ao agente que aquele hor√°rio est√° ocupado\n",
        "* Se n√£o estiver, informe ao agente que aquele hor√°rio est√° dispon√≠vel\n",
        "\n",
        "\n",
        "5. Pergunte: *\"A sala Azul est√° livre √†s 2 da tarde?\"*.\n",
        "6. Teste tamb√©m: *\"A executiva t√° tranquila √†s 10:00?\"*.\n",
        "7. Veja se o modelo extrai corretamente os par√¢metros (\"azul\"/\"14:00\" e \"executiva\"/\"10:00\") e responde de acordo."
      ],
      "metadata": {
        "id": "49XaMcWcVmUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "QriqOL2gVqro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exerc√≠cio 5.2: O Corretor de Seguros (C√°lculo Determin√≠stico)**\n",
        "\n",
        "LLMs s√£o p√©ssimos em matem√°tica financeira. Uma seguradora quer um bot que d√™ cota√ß√µes r√°pidas de seguro auto, mas o c√°lculo precisa ser exato (Python), e n√£o uma estimativa do GPT.\n",
        "\n",
        "1. Crie uma ferramenta `@tool` chamada `calcular_seguro_auto`.\n",
        "2. Ela deve receber: `idade` (int), `valor_carro` (float) e `tem_sinistro` (bool).\n",
        "3. A l√≥gica de neg√≥cio (Python) √©:\n",
        "* Pre√ßo base √© 3% do valor do carro.\n",
        "* Se `idade < 25`, adiciona 20% de risco sobre o pre√ßo base.\n",
        "* Se `tem_sinistro` for True, adiciona 50% de risco sobre o pre√ßo base.\n",
        "\n",
        "\n",
        "4. Fa√ßa o bind e pergunte: *\"Tenho 22 anos, um carro de 100 mil reais e nunca bati. Quanto fica o seguro?\"*.\n",
        "5. Verifique se o modelo extraiu os argumentos certos (idade=22, valor=100000, sinistro=False) e se ele usou o valor exato retornado pela fun√ß√£o na resposta final."
      ],
      "metadata": {
        "id": "DSIdf8qBVsfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "0ZL8LXg-Vt_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Aula 6: Agentes (Automatizando o Racioc√≠nio)**\n",
        "\n",
        "Fazer o loop acima (if tool_calls...) manualmente √© cansativo.\n",
        "O **Agente** (`create_agent`) √© uma abstra√ß√£o que faz esse loop (ReAct - Reason, Act) automaticamente at√© chegar na resposta ou atingir um limite de passos."
      ],
      "metadata": {
        "id": "Jf4uizqzOAM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando a fun√ß√£o create_agent, do m√≥dulo agents do Langchain\n",
        "\n",
        "\n",
        "# Instanciando o modelo\n",
        "\n",
        "\n",
        "# Usando as mesmas ferramentas anteriores e criando o agente\n"
      ],
      "metadata": {
        "id": "_x0DZIMiYo98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executando um prompt\n"
      ],
      "metadata": {
        "id": "l-1QBoAFY-DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† Exerc√≠cios de Fixa√ß√£o (Aula 6)**\n",
        "\n",
        "**Exerc√≠cio 6.1: O Assistente de Viagens (Agrega√ß√£o de Informa√ß√£o)**\n",
        "\n",
        "Muitas vezes, uma resposta simples para o usu√°rio exige consultar v√°rias fontes diferentes. O usu√°rio pergunta \"Quanto custa ir para Paris?\", e o Agente precisa consultar o a√©reo E a hospedagem separadamente para somar.\n",
        "\n",
        "1. Crie duas ferramentas:\n",
        "* `cotar_passagem(destino: str)`: Retorna o pre√ßo do voo. Use um mock: `{\"paris\": 4500, \"ny\": 3000}`.\n",
        "* `cotar_hotel(destino: str)`: Retorna o pre√ßo da di√°ria. Use um mock: `{\"paris\": 1200, \"ny\": 1500}`.\n",
        "\n",
        "\n",
        "2. Crie um Agente (`create_tool_calling_agent`) com essas duas tools.\n",
        "3. Pergunte: *\"Quanto fica uma viagem para Paris considerando a passagem e 3 di√°rias de hotel?\"*.\n",
        "4. **O Desafio:** O Agente deve ser capaz de:\n",
        "1. Chamar `cotar_passagem(\"paris\")`.\n",
        "2. Chamar `cotar_hotel(\"paris\")`.\n",
        "3. Fazer a conta (4500 + 3*1200) sozinho ou usando uma calculadora interna.\n",
        "4. Responder o valor final."
      ],
      "metadata": {
        "id": "g13KmtKYZ9nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "eqL-z-7uaCb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exerc√≠cio 6.2: O Agente de Reembolso (L√≥gica Condicional)**\n",
        "\n",
        "Este √© um cl√°ssico de Customer Experience (CX). O Agente precisa verificar uma regra de neg√≥cio (Data da Compra) antes de executar uma a√ß√£o (Gerar Reembolso). Ele n√£o pode gerar o reembolso se o prazo expirou.\n",
        "\n",
        "1. Crie tr√™s ferramentas:\n",
        "* `obter_data_compra(id_pedido: str)`: Retorna uma data simulada.\n",
        "* Se ID for \"PED-ANTIGO\", retorne \"2020-01-01\".\n",
        "* Se ID for \"PED-NOVO\", retorne a data de hoje (use `datetime.now()`).\n",
        "\n",
        "* `calcular_dias(data: str)`: Retorna a diferen√ßa em n√∫mero de dias da data fornecida em rela√ß√£o a hoje.\n",
        "\n",
        "\n",
        "* `gerar_reembolso(id_pedido: str)`: Retorna \"Reembolso processado com sucesso!\".\n",
        "\n",
        "\n",
        "2. Crie um Agente e d√™ a ele o seguinte System Prompt:\n",
        "*\"Voc√™ √© um agente financeiro. Verifique a data da compra. Se a compra foi feita h√° mais de 7 dias, RECUSE o reembolso educadamente. Se for recente, processe o reembolso.\"*\n",
        "3. Teste 1: *\"Quero reembolso do pedido PED-ANTIGO\"*. (O agente deve checar a data, calcular os dias e negar, **sem** chamar a ferramenta de gerar reembolso).\n",
        "4. Teste 2: *\"Quero reembolso do pedido PED-NOVO\"*. (O agente deve checar a data, calcular os dias e, vendo que √© menor do que 7, chamar a ferramenta de reembolso).\n"
      ],
      "metadata": {
        "id": "QTTgSFw0aEc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "oHdNvMMHZ5A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aula 7: Streaming (A Arte da Resposta em Tempo Real)**\n",
        "\n",
        "At√© agora, usamos `.invoke()`, que espera a resposta completa ficar pronta para entregar tudo de uma vez. Isso √© ruim para o usu√°rio, que fica encarando uma tela parada.\n",
        "\n",
        "O **Streaming** entrega a resposta \"token a token\" (letra por letra), assim que √© gerada. Isso cria a sensa√ß√£o de velocidade imediata."
      ],
      "metadata": {
        "id": "Tdr1KLQZbZEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.1 Streaming Simples (Chat Models)**\n",
        "\n",
        "Para modelos de chat, basta trocar `.invoke()` por `.stream()`. O retorno n√£o √© mais uma mensagem, √© um **Iterador** (Generator) que cospe pedacinhos de texto (`chunks`)."
      ],
      "metadata": {
        "id": "QWVUb09ybdXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando a biblioteca time e o ChatOpenAI\n",
        "\n",
        "\n",
        "# Instanciando o modelo\n",
        "\n",
        "\n",
        "# Iniciando o processo de straming\n",
        "\n",
        "\n",
        "# Em vez de esperar tudo, recebemos peda√ßos\n",
        "# Chunk √© um peda√ßo da AIMessage (para cada chunk no stream, printamos o content (com end='' e flush = True))\n"
      ],
      "metadata": {
        "id": "N2SIGtRlbeki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† **Exerc√≠cios de Fixa√ß√£o (Aula 7)**\n",
        "\n",
        "**Exerc√≠cio 7.1: O Efeito M√°quina de Escrever**\n",
        "\n",
        "Crie uma fun√ß√£o `stream_texto(texto: str)` que recebe um texto longo pronto.\n",
        "Dentro dela, simule um streaming \"falso\": percorra o texto letra por letra, imprima com `end=\"\"`, d√™ um `time.sleep(0.05)` e fa√ßa `flush`. Isso ajuda a entender a UX do streaming."
      ],
      "metadata": {
        "id": "Y6-QcCvFbpeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "Y5TohWP3bsNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exerc√≠cio 7.2: Streaming com Stop**\n",
        "\n",
        "Use o `llm.stream(\"Conte at√© 100\")`.\n",
        "Crie um loop que imprime os tokens. Adicione uma l√≥gica: se o n√∫mero \"10\" aparecer no chunk, pare o streaming (`break`) imediatamente e imprima \"üö´ Interrompido pelo filtro de seguran√ßa.\"."
      ],
      "metadata": {
        "id": "D_ZKAkDnbt-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva seu c√≥digo aqui\n"
      ],
      "metadata": {
        "id": "kx5bonFGbvPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **üöÄ Pr√≥ximo Passo: Projeto no Cursor**\n",
        "\n",
        "Agora voc√™ tem o poder de criar \"seres digitais\".\n",
        "\n",
        "**Vamos para a IDE Cursor construir o \"The CLI Assistant\".**\n",
        "\n",
        "Neste projeto final, vamos elevar a barra. Al√©m da mem√≥ria e das ferramentas b√°sicas, voc√™ implementar√° uma ferramenta de **Integra√ß√£o de API Real**.\n",
        "\n",
        "### **O Desafio Extra: Pok√©API**\n",
        "\n",
        "Voc√™ deve criar uma tool que usa a biblioteca `requests` para consultar a API p√∫blica do Pok√©mon (`https://pokeapi.co/api/v2/pokemon/{nome}`).\n",
        "Seu assistente dever√° ser capaz de responder:\n",
        "*\"Que horas s√£o e qual √© a altura do Pikachu?\"*\n",
        "Ele ter√° que chamar `get_current_time`, depois `get_pokemon_info`, e consolidar tudo.\n",
        "\n",
        "**Estrutura do Projeto:**\n",
        "\n",
        "* `cli_assistant.py`: A classe que gerencia o hist√≥rico e o Agente.\n",
        "* `tools.py`: Onde voc√™ escrever√° a fun√ß√£o `get_pokemon_info` usando `requests`.\n",
        "* `main.py`: O loop de intera√ß√£o com o usu√°rio.\n",
        "\n",
        "V√° para o Cursor. √â hora de virar um Engenheiro de IA."
      ],
      "metadata": {
        "id": "r4M1Pd9_OD57"
      }
    }
  ]
}