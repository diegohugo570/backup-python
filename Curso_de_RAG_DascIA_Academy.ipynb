{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oBz9vzUxM9sG",
        "4q7VpYBBkY7j",
        "etjM2N2rr8qm",
        "2P1UIrDlME1F",
        "XhMMgT15Uc3M",
        "0GynjUz0HhNM",
        "9N_RKhEUVv8o",
        "1brDp2wXvsV2",
        "WQ45j2BZHkQY",
        "CL8CliwEJe9M",
        "HwfdHhfkydXY",
        "YeI0Lqfl9Djm",
        "IZyqpsT5JKof",
        "EKNUEv1mox8G",
        "Nu6A5DmdLun4",
        "7CjoeYIFSGBi",
        "UuEvxLyzfRgO",
        "uHZsduyGfTOz",
        "lulBCJWfjGuc",
        "zPeqtmoucZ16",
        "svNa0fx5bV8U",
        "w83ZPL9LlAGZ",
        "3feBL_563Phe",
        "qcVXHPYlXUbS",
        "i-kXqSCvXWYy",
        "INTO00sDLJWV",
        "K2OiYdj8gllM",
        "H9v9oIKmYGcY",
        "WH3cvRASb0AI",
        "irJexkjIobGi",
        "2NCkDfXkzGQq",
        "DILfPaA25u8c"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac9c03de349f454ea09c87d5887e65aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9a4f9910c7747cbb772506bf97fa959",
              "IPY_MODEL_0bf517c549f141fcb4ad6fd74c5a30ab",
              "IPY_MODEL_f928fc583ac74beabd2f2cb987bfc895"
            ],
            "layout": "IPY_MODEL_340be93b4047441b88aee74e047c368c"
          }
        },
        "e9a4f9910c7747cbb772506bf97fa959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09af667772aa43a881244d98d8e163cd",
            "placeholder": "​",
            "style": "IPY_MODEL_dd9b6e7c3d1344ac8993905c8ad92d32",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0bf517c549f141fcb4ad6fd74c5a30ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_016e8f68aba041acafe96337bd5852d5",
            "max": 399,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_299fb0ee5de140da83d1bca42173e0db",
            "value": 399
          }
        },
        "f928fc583ac74beabd2f2cb987bfc895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79eac04b57e4469db83e584cafaec435",
            "placeholder": "​",
            "style": "IPY_MODEL_f3208e7d205f475eb0a678daa1d70658",
            "value": " 399/399 [00:00&lt;00:00, 19.3kB/s]"
          }
        },
        "340be93b4047441b88aee74e047c368c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09af667772aa43a881244d98d8e163cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd9b6e7c3d1344ac8993905c8ad92d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "016e8f68aba041acafe96337bd5852d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "299fb0ee5de140da83d1bca42173e0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79eac04b57e4469db83e584cafaec435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3208e7d205f475eb0a678daa1d70658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "416eeb4a21894f1e8df8b18542907b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e593658278c1484aa3d2753e76bcd495",
              "IPY_MODEL_32fd7b66b3a84f9da9e746987f4c5a14",
              "IPY_MODEL_57e839238bb04ecc999ac579ff13cff4"
            ],
            "layout": "IPY_MODEL_0fc3dac3de214936aa50d78e9f6b8a64"
          }
        },
        "e593658278c1484aa3d2753e76bcd495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_038c3a40404d4d20bd2d0d04c9cb5b6d",
            "placeholder": "​",
            "style": "IPY_MODEL_f5571501b044402e9e0c53322c57eb1c",
            "value": "config.json: 100%"
          }
        },
        "32fd7b66b3a84f9da9e746987f4c5a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ea9105e1574890925afca734d034b8",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d273dee4865b4e52819d92da648699df",
            "value": 625
          }
        },
        "57e839238bb04ecc999ac579ff13cff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edaada598ac64504b8c8f49f72c8af1d",
            "placeholder": "​",
            "style": "IPY_MODEL_e6e48907455b41629add7193505a3773",
            "value": " 625/625 [00:00&lt;00:00, 27.1kB/s]"
          }
        },
        "0fc3dac3de214936aa50d78e9f6b8a64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "038c3a40404d4d20bd2d0d04c9cb5b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5571501b044402e9e0c53322c57eb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03ea9105e1574890925afca734d034b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d273dee4865b4e52819d92da648699df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "edaada598ac64504b8c8f49f72c8af1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6e48907455b41629add7193505a3773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e156666d24664dfc99264001f76070a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_242851e0815f46d5be1f6655ff81b1ca",
              "IPY_MODEL_b9c14e42fef44083a0dc42bea9d9b490",
              "IPY_MODEL_51c7c3d9d0654d549ad01d6d1b7d4058"
            ],
            "layout": "IPY_MODEL_c6098bd4540048da9d72ad741e989e15"
          }
        },
        "242851e0815f46d5be1f6655ff81b1ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa650da113914363a90e386530c6efb9",
            "placeholder": "​",
            "style": "IPY_MODEL_2681d09bccf64aed9a3384facf0b83b1",
            "value": "vocab.txt: 100%"
          }
        },
        "b9c14e42fef44083a0dc42bea9d9b490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01d84585c11645d5b1b95e682656c4de",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae4983dacb8548108f51ff9e906147ce",
            "value": 231508
          }
        },
        "51c7c3d9d0654d549ad01d6d1b7d4058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_188b9b155add4f669dc4e4e50c7a044a",
            "placeholder": "​",
            "style": "IPY_MODEL_8f04f15d4faf497aac6db3b39a82a7b2",
            "value": " 232k/232k [00:00&lt;00:00, 4.14MB/s]"
          }
        },
        "c6098bd4540048da9d72ad741e989e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa650da113914363a90e386530c6efb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2681d09bccf64aed9a3384facf0b83b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01d84585c11645d5b1b95e682656c4de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae4983dacb8548108f51ff9e906147ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "188b9b155add4f669dc4e4e50c7a044a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f04f15d4faf497aac6db3b39a82a7b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01506fe3711d4fc2968a603b864ab3d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff856ac03b8145ceae09d27deb88df14",
              "IPY_MODEL_f6ea3b344497476ebd6417111eb01e5e",
              "IPY_MODEL_307f9b56ace846fda6f9144de1b6f398"
            ],
            "layout": "IPY_MODEL_5d8d005ac69c4482bb7f5986bd64a874"
          }
        },
        "ff856ac03b8145ceae09d27deb88df14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6705b0d8352b4855a571ee2c35d2b460",
            "placeholder": "​",
            "style": "IPY_MODEL_361a5a02cf60431fbbdd2a6660915efb",
            "value": "tokenizer.json: 100%"
          }
        },
        "f6ea3b344497476ebd6417111eb01e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd2e2b06746c4a74b201210d69e324c4",
            "max": 466081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9998340a8004cc489cc52d7dd517f5d",
            "value": 466081
          }
        },
        "307f9b56ace846fda6f9144de1b6f398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bad1df891c84d0ab00e98580d1bca5e",
            "placeholder": "​",
            "style": "IPY_MODEL_bc30515f17cd4ff4b1f2b6972abf7bc1",
            "value": " 466k/466k [00:00&lt;00:00, 9.09MB/s]"
          }
        },
        "5d8d005ac69c4482bb7f5986bd64a874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6705b0d8352b4855a571ee2c35d2b460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "361a5a02cf60431fbbdd2a6660915efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd2e2b06746c4a74b201210d69e324c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9998340a8004cc489cc52d7dd517f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bad1df891c84d0ab00e98580d1bca5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc30515f17cd4ff4b1f2b6972abf7bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faebe2880a25404aacfb76f44015f07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab4375f64cd54536a4fd6743c34b4038",
              "IPY_MODEL_6cab11e31c48477b80fbb32b6ef61e36",
              "IPY_MODEL_3222e1deef3a41c6b81245e8946fb91a"
            ],
            "layout": "IPY_MODEL_550df6ae14f64c6b8dadc8191ab89550"
          }
        },
        "ab4375f64cd54536a4fd6743c34b4038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_534eb95b42294b0e95715eeb0f8dbee3",
            "placeholder": "​",
            "style": "IPY_MODEL_8c094e590f24445eac025e382c871692",
            "value": "added_tokens.json: 100%"
          }
        },
        "6cab11e31c48477b80fbb32b6ef61e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fdc5a674e7e4ac0938758919d2c8e88",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8593267088d4eeb9d7eceddb84c413a",
            "value": 2
          }
        },
        "3222e1deef3a41c6b81245e8946fb91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e91a5e882e4a4191401840c4675f4e",
            "placeholder": "​",
            "style": "IPY_MODEL_312793a120f4458e886028f7fd99acf8",
            "value": " 2.00/2.00 [00:00&lt;00:00, 143B/s]"
          }
        },
        "550df6ae14f64c6b8dadc8191ab89550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "534eb95b42294b0e95715eeb0f8dbee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c094e590f24445eac025e382c871692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fdc5a674e7e4ac0938758919d2c8e88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8593267088d4eeb9d7eceddb84c413a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46e91a5e882e4a4191401840c4675f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "312793a120f4458e886028f7fd99acf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fe49b38e35d4de4accef02dce369c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fd847e531c54324a6142d9b39756e7d",
              "IPY_MODEL_b18d42f61bab4ed692da2fff0a5351c3",
              "IPY_MODEL_3f89ec63a48645af8ef24bef04393e90"
            ],
            "layout": "IPY_MODEL_851a3692b4da4414aafce071805af991"
          }
        },
        "6fd847e531c54324a6142d9b39756e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84baf766a3e34eba8baadb5f0052f091",
            "placeholder": "​",
            "style": "IPY_MODEL_880b30d963ae468cad5d75c2568cc3d6",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "b18d42f61bab4ed692da2fff0a5351c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d22a8d7d994145058991ea314a3817cf",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21acc85ff5244b62b7576c33b23ea64b",
            "value": 112
          }
        },
        "3f89ec63a48645af8ef24bef04393e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb4d5612a77f4522bc3a7987c7164d20",
            "placeholder": "​",
            "style": "IPY_MODEL_0ca3ffc0c820439fa9d526c773c10fb4",
            "value": " 112/112 [00:00&lt;00:00, 8.59kB/s]"
          }
        },
        "851a3692b4da4414aafce071805af991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84baf766a3e34eba8baadb5f0052f091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880b30d963ae468cad5d75c2568cc3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d22a8d7d994145058991ea314a3817cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21acc85ff5244b62b7576c33b23ea64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb4d5612a77f4522bc3a7987c7164d20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca3ffc0c820439fa9d526c773c10fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f7fef59041942708cc9da065196a4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af4e534b8c994b7394241ad1f83a6257",
              "IPY_MODEL_845821a5fba2457bb49cdc9120e86ea2",
              "IPY_MODEL_b57ec29db3344c4f947cf1577a754de1"
            ],
            "layout": "IPY_MODEL_9dd32872332c4137bdb801ef73d865c6"
          }
        },
        "af4e534b8c994b7394241ad1f83a6257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de8cfe203c6948db953cbd95bb949a16",
            "placeholder": "​",
            "style": "IPY_MODEL_c44156963c4a49f4a855b0bd4d0e1642",
            "value": "model.safetensors: 100%"
          }
        },
        "845821a5fba2457bb49cdc9120e86ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14550a707e2d4e46a126a65ab6fa4fe3",
            "max": 437955512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f7b19f48f7b42e58b5a7696e912690c",
            "value": 437955512
          }
        },
        "b57ec29db3344c4f947cf1577a754de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f9989a58bd49d184bc816e91991536",
            "placeholder": "​",
            "style": "IPY_MODEL_3e6ff308341c410283450dee710d0cb1",
            "value": " 438M/438M [00:03&lt;00:00, 140MB/s]"
          }
        },
        "9dd32872332c4137bdb801ef73d865c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de8cfe203c6948db953cbd95bb949a16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c44156963c4a49f4a855b0bd4d0e1642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14550a707e2d4e46a126a65ab6fa4fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f7b19f48f7b42e58b5a7696e912690c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6f9989a58bd49d184bc816e91991536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e6ff308341c410283450dee710d0cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegohugo570/backup-codigos/blob/main/Curso_de_RAG_DascIA_Academy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# R.A.G. (Retrieval-Augmented Generation)\n",
        "\n",
        "**Este material foi preparado por [Anwar Hermuche](https://instagram.com/anwar.hermuche) para os alunos da [Formação em IA da DascIA](https://dascia.academy).**\n",
        "\n",
        "Ele contém todas as explicações e aplicações que você precisa saber sobre R.A.G. para utilizar da melhor maneira possível. Sendo esse módulo 100% baseado na documentação oficial do Langchain, o maior framework de IA do mundo.\n",
        "\n",
        "Dito isso, é um prazer estar contruindo esse material para vocês e bora pra cima."
      ],
      "metadata": {
        "id": "oBz9vzUxM9sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando as bibliotecas necessárias\n",
        "!pip install langchain langchain-core langchain-community langchain-openai langchain-chroma beautifulsoup4 tiktoken pdfplumber faiss-cpu chromadb lark --quiet"
      ],
      "metadata": {
        "id": "fTkeY1Qsdcz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8cd4a9b-66c1-4401-dd8a-f7b0bd18d0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando a rastreabilidade da aplicação\n",
        "import os # operational system\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "YdlEoNw4dp2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução ao RAG\n",
        "\n",
        "Antes de falarmos sobre R.A.G. especificamente, é importantíssimo lembrarmos de uma coisa - ou vê-la em um primeiro momento caso você ainda não saiba: **95% dos dados do mundo são \"privados\"**, ou seja, pertencentes a empresas.\n",
        "\n",
        "Apesar disso, podemos fornecer esses dados aos LLMs! Ou seja, se você possui, na sua empresa ou na empresa que você está atendendo, um relatório privado ou dados internos pertencentes a alguma planilha e quer construir uma aplicação em cima desses documentos, você precisa, de alguma maneira, fornecer esses dados ao modelo.\n",
        "\n",
        "E essa é uma necessidade básica que temos, até mesmo por conta das alucinações. Observe a imagem abaixo:\n",
        "\n",
        "<img src=\"https://i.ibb.co/VYpfbM6X/Usua-rios-similares-37.png\">\n",
        "\n",
        "## LLMs como SOs?\n",
        "\n",
        "Veja que um LLM hoje consegue ter acesso a entrada e saída de dados (I/O), conexão com a internet, executar código e muito mais. É realmente como se o LLM fosse o sistema operacional de um computador, a parte que controla todo o funcionamento do computador.\n",
        "\n",
        "No canto inferior esquerdo, note o \"File system\", ou seja, os arquivos do sistema, que na analogia é o disco do computador, seu HD, sua memória. Os arquivos que ele tem acesso. E é justamente esse ponto que iremos trabalhar no R.A.G.. De tudo o que o LLM tem acesso, vamos focar um módulo inteiro nesse \"disco\".\n",
        "\n",
        "## A problemática\n",
        "\n",
        "E como você vai fazer isso? Vamos supor que você tem um documento de 1500 páginas para responder perguntas sobre ele. Como você vai fazer? Vai fornecer tudo de uma vez só para o modelo? Bem... **não é assim que funciona**. Já vimos sobre janela de contexto e ele não iria suportar.\n",
        "\n",
        "Observe o código abaixo (que, por sinal, vamos aprender a como construir isso do zero):\n"
      ],
      "metadata": {
        "id": "nMfZiBjQPMMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas necessárias\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "## INDEXAÇÃO ##\n",
        "\n",
        "# Carregando os documentos\n",
        "loader = PDFPlumberLoader(\n",
        "    file_path = \"/content/A Startup Enxuta - Eric Ries.pdf\"\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "# Pegando todo o conteúdo do livro\n",
        "conteudo = \"\\n\".join([doc.page_content for doc in docs])"
      ],
      "metadata": {
        "id": "V_2LHxbcdaSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GERAÇÃO ##\n",
        "\n",
        "# Fazendo uma pergunta ao documento\n",
        "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0)\n",
        "template = f\"\"\"\n",
        "Com base no contexto, responda a pergunta entre crases triplas. Se não souber, diga que não sabe.\n",
        "\n",
        "Contexto:\n",
        "{conteudo}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template + \"\"\"\n",
        "\n",
        "Pergunta:\n",
        "```{pergunta}```\"\"\")\n",
        "\n",
        "rag = (\n",
        "    {\"pergunta\": RunnablePassthrough()} |\n",
        "    prompt |\n",
        "    llm |\n",
        "    StrOutputParser()\n",
        ")\n",
        "\n",
        "resposta = rag.invoke(\"O que é motor de crescimento?\")\n",
        "\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "yRfGyB-MiNPh",
        "outputId": "21550929-0e55-4fdf-d06b-c1a7b628083c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 129371 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-70c3a36c061a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mresposta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"O que é motor de crescimento?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresposta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         return cast(\n\u001b[1;32m    306\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    842\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 results.append(\n\u001b[0;32m--> 683\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    684\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    909\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 129371 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Possível solução?\n",
        "E se pudéssemos usar apenas os chunks - que chamaremos de documentos ou chunks - que respondem as perguntas que fazemos?\n",
        "\n",
        "Ou seja, extrair do livro apenas os chunks abaixo (que respondem a pergunta \"**O que é motor de crescimento?**\")\n",
        "\n",
        "<hr>\n",
        "\n",
        "Documento 0:\n",
        "\n",
        "motores de crescimento. Cada um é como um motor de combustão, girando repetidas vezes.\n",
        "Quanto mais rápido o ciclo é completado, mais rápido a empresa crescerá. Cada motor possui\n",
        "um conjunto intrínseco de métricas que determinam com que rapidez uma empresa pode\n",
        "crescer ao utilizá-lo.\n",
        "OS TRÊS MOTORES DE CRESCIMENTO\n",
        "Vimos na Parte II como é importante que as startups utilizem o tipo certo de métricas –\n",
        "métricas acionáveis – para avaliar o progresso. No entanto, isso deixa uma grande quantidade\n",
        "em termos de que números devemos medir. De fato, uma das formas mais onerosas de possível\n",
        "<hr>\n",
        "\n",
        "Documento 1:\n",
        "\n",
        "DE ONDE VEM O CRESCIMENTO?\n",
        "O motor de crescimento é o mecanismo que as startups utilizam para alcançar o crescimento\n",
        "sustentável. Utilizo a palavra sustentável para excluir todas as atividades ocasionais que\n",
        "geram um surto de clientes, mas não têm impacto a longo prazo, tais como anúncios isolados\n",
        "ou uma proeza publicitária que pode ser utilizada para revitalizar o crescimento, mas não\n",
        "consegue sustentá-lo a longo prazo.\n",
        "O crescimento sustentável se caracteriza por uma regra simples:\n",
        "Os novos clientes surgem das ações dos clientes passados.\n",
        "<hr>\n",
        "\n",
        "Documento 2:\n",
        "\n",
        "todos os motores de crescimento acabam ficando sem gasolina. Todos os motores estão\n",
        "relacionados a um determinado conjunto de clientes e seus hábitos, preferências, canais\n",
        "publicitários e interconexões. Em algum momento, esse conjunto de clientes será exaurido.\n",
        "Pode levar muito ou pouco tempo, dependendo do setor e do timing.\n",
        "O Capítulo 6 enfatizou a importância de construir o produto mínimo viável de maneira a não\n",
        "incluir nenhum recurso adicional além do requerido pelos adotantes iniciais. Seguir essa\n",
        "estratégia com êxito destravará um motor de crescimento que pode alcançar a audiência-alvo.\n",
        "\n",
        "<hr>\n",
        "\n",
        "Quando jogamos isso no prompt a IA consegue responder a pergunta sem dar o erro de limite de contexto, observe abaixo:"
      ],
      "metadata": {
        "id": "ETNwSkJrnrfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pergunta e documentos que usaremos de contexto\n",
        "pergunta = \"O que é motor de crescimento?\"\n",
        "\n",
        "docs = \"\"\"\n",
        "Documento 0:\n",
        "motores de crescimento. Cada um é como um motor de combustão, girando repetidas vezes.\n",
        "Quanto mais rápido o ciclo é completado, mais rápido a empresa crescerá. Cada motor possui\n",
        "um conjunto intrínseco de métricas que determinam com que rapidez uma empresa pode\n",
        "crescer ao utilizá-lo.\n",
        "OS TRÊS MOTORES DE CRESCIMENTO\n",
        "Vimos na Parte II como é importante que as startups utilizem o tipo certo de métricas –\n",
        "métricas acionáveis – para avaliar o progresso. No entanto, isso deixa uma grande quantidade\n",
        "em termos de que números devemos medir. De fato, uma das formas mais onerosas de possível\n",
        "\n",
        "Documento 1:\n",
        "DE ONDE VEM O CRESCIMENTO?\n",
        "O motor de crescimento é o mecanismo que as startups utilizam para alcançar o crescimento\n",
        "sustentável. Utilizo a palavra sustentável para excluir todas as atividades ocasionais que\n",
        "geram um surto de clientes, mas não têm impacto a longo prazo, tais como anúncios isolados\n",
        "ou uma proeza publicitária que pode ser utilizada para revitalizar o crescimento, mas não\n",
        "consegue sustentá-lo a longo prazo.\n",
        "O crescimento sustentável se caracteriza por uma regra simples:\n",
        "Os novos clientes surgem das ações dos clientes passados.\n",
        "\n",
        "Documento 2:\n",
        "todos os motores de crescimento acabam ficando sem gasolina. Todos os motores estão\n",
        "relacionados a um determinado conjunto de clientes e seus hábitos, preferências, canais\n",
        "publicitários e interconexões. Em algum momento, esse conjunto de clientes será exaurido.\n",
        "Pode levar muito ou pouco tempo, dependendo do setor e do timing.\n",
        "O Capítulo 6 enfatizou a importância de construir o produto mínimo viável de maneira a não\n",
        "incluir nenhum recurso adicional além do requerido pelos adotantes iniciais. Seguir essa\n",
        "estratégia com êxito destravará um motor de crescimento que pode alcançar a audiência-alvo.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Com base no contexto, responda a pergunta entre crases triplas. Se não souber, diga que não sabe.\n",
        "\n",
        "Pergunta:\n",
        "```{pergunta}```\n",
        "\n",
        "Contexto:\n",
        "{docs}\n",
        "\"\"\"\n",
        "\n",
        "resposta = llm.invoke(prompt)\n",
        "\n",
        "print(resposta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCvkpSb9pkwR",
        "outputId": "884797b9-c5a2-4967-a362-b4cd78b72df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O motor de crescimento é o mecanismo que as startups utilizam para alcançar o crescimento sustentável. Ele se caracteriza por um processo em que os novos clientes surgem das ações dos clientes passados, excluindo atividades ocasionais que geram apenas surtos temporários de clientes sem impacto a longo prazo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notou que usamos apenas os chunks que respondem a pergunta do usuário? **É justamente esse o propósito do RAG**.\n",
        "\n",
        "Ao invés, de colocar todo o livro no modelo para usar de contexto, vamos **colocar apenas o chunk que responde à pergunta feita**? Faz mais sentido, não? E outra, não teria o problema da janela de contexto.\n",
        "\n",
        "Nu! Se sua cabeça não explodiu agora, volte e leia de novo, porque era pra ter explodido. Na imagem abaixo você consegue ver a arquitetura do R.A.G.\n",
        "\n",
        "Para facilitar a explicação, vou dividir esse fluxo em três etapas: Indexação, Recuperação e Geração. Observe abaixo:\n",
        "\n",
        "<img src=\"https://i.ibb.co/39hZh10z/Lead-Entra-na-Lista-2.png\">\n",
        "\n",
        "A parte que faltou no exemplo anterior foi um pedaço da indexação e a recuperação inteira. Vamos refazer o exemplo anterior finalizando o projeto."
      ],
      "metadata": {
        "id": "ZL8oPzfKdauX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAcECTa0KvUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df9f6c2-3e69-4489-f384-d35e9c5e5ee6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['47c224ce-e0c8-4537-a8fd-9f155b68f6a3',\n",
              " '7d1bc5ac-db64-4540-b2b8-3fcfbbb84c7e',\n",
              " '495be8a3-6f50-4616-baf8-1167dfec6d4a',\n",
              " 'ab89ee91-8a62-47ef-99bb-55d52e164dfd',\n",
              " '393ce2a3-0467-49ca-a8b2-d3be5787aeca',\n",
              " 'd8fd6a19-bc22-4835-8dc5-8ebd1b1b3835',\n",
              " 'e1e1399f-2181-4d64-8432-fd993229de3f',\n",
              " 'debcda9a-ddfb-40e4-85ce-5064b79ad5f5',\n",
              " 'd9cdf6ab-17fe-4816-89c0-582b38a33573',\n",
              " '40132de8-c7c9-43bc-8503-e7fd4b5c0539',\n",
              " '0c3e7366-fe35-489c-ba8e-20d4740a19ca',\n",
              " 'b28f132e-c04e-46dd-a0de-9392fadacd95',\n",
              " '0cd52469-d1c7-4121-9713-d8e743dc332e',\n",
              " '288cc4e6-870c-41b2-96f7-c994d034ae1f',\n",
              " 'ed2e70ab-f408-49e6-bd93-d500654fc411',\n",
              " '4ecbd5e0-790e-4251-86be-550a5bb6fd99',\n",
              " 'e4b0045b-f107-4505-a267-0dec0f33d3a6',\n",
              " 'c1d9811b-c760-435e-abb0-8f66f1425220',\n",
              " '38036dae-a240-4efd-b24c-8741eceb3f32',\n",
              " 'e36dede9-ff5a-4ec2-a04c-ecef97b3420f',\n",
              " 'ee7ae837-1648-4edd-a71a-40c522fe638c',\n",
              " '7e3b1485-1a1f-4e53-a4df-ee203ab47e80',\n",
              " '79244b9e-309a-48f2-ba34-c6c1c06a9df1',\n",
              " 'e27fa678-836c-4602-a234-5605189fd21c',\n",
              " '02950803-d98d-473b-9a83-8d121d0dc2b0',\n",
              " '239b3570-40ee-4889-8737-8840533d9b6f',\n",
              " '2b8bb7fc-d3ee-4fc7-981a-1ffd24eac7ce',\n",
              " '1ea983e7-e59f-4371-a1ef-17727fa513bb',\n",
              " '62036695-e1b9-44fa-b150-876836cde055',\n",
              " 'f465f1d5-061d-45b4-aafe-e6bbb58fa589',\n",
              " '1388cbbd-2aa8-4a09-a81d-4ff0a4ede33b',\n",
              " 'c29dce4d-ddff-4eed-a584-6545a460ef3e',\n",
              " '37030c46-0d84-468c-956e-a6d3bda9c7e2',\n",
              " 'ee18c631-8594-471a-b0d8-1fa449d88d82',\n",
              " 'dafe30a9-8e68-41d6-a865-6b228f532536',\n",
              " '44a6ea56-761a-47b5-b288-aa0af49ce242',\n",
              " '48ccc984-99c3-470e-84bf-1cda2c12eeb1',\n",
              " 'b5ac2271-fa7f-4882-b676-2596b8924e3e',\n",
              " 'a775d0c9-8f4a-40a1-b42c-545e46a20d74',\n",
              " '047ae29f-30a3-443e-a21a-15021ea3818a',\n",
              " '937f295e-d6fc-4507-b597-0df016c5308c',\n",
              " '6e227be7-95dd-4d35-adb1-988f0fccfcc0',\n",
              " 'fcd10e4c-116c-4e58-a81e-32c6237eb5c0',\n",
              " '2cd6f801-bebf-4498-aad2-49bd09855566',\n",
              " 'cbd4194f-fbfc-4bcc-97fb-b32a0548b494',\n",
              " '1459f774-b278-4dba-a4a1-48bf05e216ae',\n",
              " '79aadb5f-fda5-4239-91ae-b15b74aecc0c',\n",
              " '3cc9deb0-4362-4c49-91ce-4dd5fab8f8bf',\n",
              " '9a3b49bf-50d0-438d-9a96-1edc7c41343e',\n",
              " '60d91b93-0caa-499a-974c-f8e9467e4fe7',\n",
              " '59cfc422-b1b6-4476-911f-cedcc870037a',\n",
              " 'e7a2fcb0-7b52-4b00-89de-34b62b646450',\n",
              " '31dcc97d-db2f-4f7f-b0b5-74dd5ac48132',\n",
              " '4aef545e-1fac-4599-aeee-3695000ce065',\n",
              " '9bea2668-2f3b-4fb7-9e77-218cb71be4ef',\n",
              " '09e9a9ba-a618-4827-8cd9-8a6d74024ac9',\n",
              " 'deac840b-d16f-4a27-b87d-448a5df6ce49',\n",
              " 'c3ac2221-5b40-4529-a655-72dcbd981831',\n",
              " '0fa0c430-d070-4d65-a4e5-d57d2dade3c8',\n",
              " '6f7d230e-7d2f-42b5-a8d2-c2321bbaa6c4',\n",
              " '3b85d35b-26e4-4064-b894-011976490b46',\n",
              " 'bfa482c8-e187-4df3-93ce-3a2ac49c2fd2',\n",
              " '30135c26-abc5-4a7a-9466-1490569bfac0',\n",
              " '216b980a-dd09-4637-9fcc-188e98a696ab',\n",
              " '46f3737c-0cf1-436a-8035-79eb4ef37260',\n",
              " '08c09890-c71c-44cb-ae9f-7dce00f7e2ff',\n",
              " '4c16c405-fc28-4321-8668-ef98f2231365',\n",
              " 'c78b7106-793a-44ff-9bd7-57096d789864',\n",
              " 'eefa9b33-f54d-4b53-a7b0-8193036a0229',\n",
              " 'f21a7ae2-2687-4d97-90ae-e1d5193fd5e0',\n",
              " '029a45b5-23f2-4dab-ba5c-4988ee0e3707',\n",
              " '46f7a287-f5fa-4132-88eb-ef095bae1a2e',\n",
              " '769582da-4dcd-4aaa-99ae-275b7aef7b1c',\n",
              " 'e26a6ca0-5a1a-49f7-9362-e5e62bdc2970',\n",
              " '724182fe-691f-4147-a06d-9863792246cd',\n",
              " '41d6af53-076b-4ee2-a4aa-9e1586f89e06',\n",
              " '111e034f-3547-4da0-a16a-4daeab9d4cd5',\n",
              " 'b4075b4f-4169-4ea1-9f1d-902cdad1bb39',\n",
              " 'c19e5f6b-5882-431d-a550-ec45a78054d6',\n",
              " 'f8c71198-f9d0-4f89-b620-fbf9cb9aac75',\n",
              " '6a5fb057-a3de-493f-8395-f712f7dbad42',\n",
              " 'e2c542c4-db31-404b-9b88-100732dd2d91',\n",
              " 'b68e2816-d552-461b-bbd8-94ee3722f05c',\n",
              " '2849a0ac-c413-4a7f-8482-5ec39e2bc24e',\n",
              " 'aa4bedf3-0d29-4ccb-a5fb-8f61b509c833',\n",
              " '76568627-0d54-4896-8e63-5de1864fc977',\n",
              " 'fc06fc45-929a-4a9e-b3da-3484b6b0df57',\n",
              " 'd56b886a-2c90-431e-ab3b-86c7dc4a5a0f',\n",
              " '79270b42-2384-49ac-af4e-1cc4f569323e',\n",
              " '16f665ec-1be6-445f-a542-3d54683101dd',\n",
              " '392e48ce-68f6-4e3f-861e-5b42cbd0ea7b',\n",
              " 'da40b1f6-108d-4d74-9c01-0528c5dc0027',\n",
              " '9679e996-c93f-41f7-bc30-b7a529f25f21',\n",
              " 'cd60d64b-138d-4b17-8efb-07a202ba0c80',\n",
              " 'f3a2757a-c89b-4e42-934a-ff4a72f569cd',\n",
              " '44558d3b-a74c-463d-87db-aaff017f3e54',\n",
              " 'cc520b56-aa06-49a5-8c72-e1ca3a2840dd',\n",
              " '77c0ff2a-3356-48d3-86d8-da94aba41ddb',\n",
              " '6e6a544d-9ab8-46d8-a7ea-0f5ea7bf90b2',\n",
              " '78feff0e-ac57-453b-8e2e-bb507fb8c866',\n",
              " 'f2da560b-02b2-4847-8884-9d61c4bb93d9',\n",
              " '0e537ee8-bfaf-413b-a36f-782252209317',\n",
              " '52f9d399-b2b7-4054-ab95-699cbd829a11',\n",
              " '1bdeba87-643d-4d5b-b0ba-5ebe13961367',\n",
              " '889930d6-5c49-4a6c-8364-d74e474bac4c',\n",
              " 'a8cf7991-9cf6-4021-9603-b8e3fd6c82ce',\n",
              " 'f208c0f4-86c0-4f0e-b16b-387a3cc55d1c',\n",
              " '8dc461d1-dee0-4e97-89cc-3dadb281922e',\n",
              " 'ecaa58a0-d804-489c-8480-7fd585e4f15b',\n",
              " '57faf7e7-16a0-4d2f-be13-5e98baea9110',\n",
              " '5c0646ae-dc49-42ce-b94e-4823e4654777',\n",
              " 'a57ddcf8-5171-496a-92e0-adce8c0121fe',\n",
              " '66961eac-5b7a-4499-bf44-99866fdce818',\n",
              " '817396fa-24e1-4d1e-9891-3a86639cdec0',\n",
              " '237df353-4497-4c4c-a5b5-9a7a5532d1c8',\n",
              " '3ba8e4df-ad19-4236-883e-37c6bac63af7',\n",
              " '1969d973-05e2-4b66-8ff1-4c21cb5e2f8d',\n",
              " '970f2e8c-f119-42d7-b685-803253d3b86c',\n",
              " 'e65f42ff-1660-48af-b86f-5922c599fb5c',\n",
              " '8053daa5-d50b-47dd-b883-0bd950fe8f83',\n",
              " 'ed81e9f2-37e7-43c6-aede-f73586363cf3',\n",
              " '859a5ee8-1c97-4369-bdc5-9ca3f8615baa',\n",
              " '6ba8ec52-56fe-411d-b649-70a46c271ea1',\n",
              " '9c8037f1-32f5-48f8-8b98-5170e8bb33c2',\n",
              " '9299ac54-2ec4-497e-ada8-39f203c7a011',\n",
              " 'e12f2ae2-7220-46ec-9a9c-3cac7d9a35a6',\n",
              " '60c2e499-6967-4b5c-8890-9f13b8b21f4a',\n",
              " '5315716f-b726-4f32-ae1e-9b9c756cbdc6',\n",
              " 'd26aac6e-e586-4f6e-bccc-f6afe5a01573',\n",
              " 'b0d5a42c-39a4-4d4c-aff1-df0ded572101',\n",
              " 'dc86a6ec-e4b1-40de-8c07-073d15eff592',\n",
              " '8437cd44-8130-4966-b431-9d6d17a1403b',\n",
              " '464f5c2b-82a2-4c45-8682-40e8f283562e',\n",
              " '76381661-96c9-4f07-ac1b-ad7fbcbd7de8',\n",
              " 'f4ffc9f4-6a95-43c8-a108-2a40627ec1ac',\n",
              " '613c1c52-21aa-42b3-952a-6d14f34b2f56',\n",
              " '6034984e-935d-482a-b681-d60e4eacfc31',\n",
              " 'b73c689e-42f5-483d-af71-e5af28db1d4c',\n",
              " 'f91b7904-ed78-4683-b515-885ef1b0bb5f',\n",
              " '836f7e85-dea4-4a71-bbc5-690faa71ecfc',\n",
              " 'bb59960c-1422-4b11-83df-ad137f2d8249',\n",
              " 'b4ed25a3-79bb-44df-8880-c86279493a32',\n",
              " 'da1bee43-8066-4d70-8016-04dc3b707d69',\n",
              " '06067d8c-b6af-440f-9c24-26ea58a401e1',\n",
              " '8e8a0f06-8d8a-4b96-b6d3-a12e4311fece',\n",
              " '246682b2-10a7-4f3b-bcb5-6d8a836484c6',\n",
              " 'e1c0c275-1df2-45af-945d-d436c63869b6',\n",
              " 'b0356758-04a0-4576-b3ce-f7f667d10e5d',\n",
              " 'cd2a6223-25cf-4642-92b2-39c237c4412d',\n",
              " '1e5b69a2-2915-48b2-abe3-892ebb0f0dc2',\n",
              " '259d5ba6-a44d-4d38-abca-e9f0bf1690a9',\n",
              " 'c44a51e2-ea89-4bf4-9f62-f6558e656d1b',\n",
              " 'b752b00b-8562-4e51-8f71-f8fd1e1d0c06',\n",
              " 'ad311d32-1a7f-4de7-83d5-a2b50a1977f2',\n",
              " '0ac88191-cb94-43a3-894e-76ea8729262b',\n",
              " 'b6a191c3-0b13-4070-ae11-b20d9e5fe61b',\n",
              " '452523e7-a11d-4313-92ad-01ede818a924',\n",
              " 'c1f17037-1885-43b5-9c91-32a9f40ce577',\n",
              " '45cb9788-2cbc-4932-b8a3-ab7bcc3515a0',\n",
              " '515ceed8-27f9-4970-a767-cc91a904ddbb',\n",
              " 'ca2b2a2f-84cd-4b93-b5e7-e3171ee5ae78',\n",
              " '0fd72298-7755-467a-bf0e-c16efc7db6c2',\n",
              " '74cb1d5a-7a2d-46c4-8b74-d12662c15f27',\n",
              " '5d34fb23-b341-47d2-bd99-4adda23afe9e',\n",
              " '6b9af481-a350-447c-8c43-ed0e0f5ac397',\n",
              " '1054e435-8a07-464e-bc40-c09021f4e617',\n",
              " 'b5acc63f-d0a1-4c16-bc74-9e1c671fcf8b',\n",
              " '2a154ac7-df73-41df-8b8b-afeff81d790e',\n",
              " '0f02013c-bdaf-40b2-9208-7ef886a9a8aa',\n",
              " 'c7ca24ed-2651-4d0a-9952-23ac9f21f327',\n",
              " '4aec4de2-bf34-4f2f-86ea-755e282e188a',\n",
              " 'e72aa7f3-9536-4e1d-ba31-1d8d493ba8e3',\n",
              " '690aa29f-7713-4c01-883d-7ede525a70a8',\n",
              " 'f3d510c5-7401-460f-a847-1184d3387374',\n",
              " '0b046b1b-5b1a-4db4-9f79-c8658535e0c8',\n",
              " '5161d917-da4e-44d3-8769-c994e05c718a',\n",
              " '667e7a0e-2f71-4cd1-b3be-53e3b58d6498',\n",
              " 'fe724ea1-7ef6-4bc5-98e8-4ea9d92e204d',\n",
              " '3dd82074-c693-42bf-ba96-09a02b6e0a0f',\n",
              " '51e01a3e-490c-43c5-9642-5880aa043ddc',\n",
              " '5e61be24-7274-4528-89b3-25338c5761d7',\n",
              " '4e73f9c3-8eba-43ad-923c-95011cdb043e',\n",
              " 'f30fd616-dfbe-4a56-80ac-02d3805e9b7a',\n",
              " '2362ee9b-96eb-4bc4-ba2d-4920e09cbcfd',\n",
              " '7dad2ce8-a1f3-4977-be8d-2708af7ae33b',\n",
              " '834a326f-7780-45ff-8645-290655083f49',\n",
              " '5bc6916e-dcce-42b4-b8c1-fcedc5f7f912',\n",
              " 'd90bdbe3-0b95-44ef-824f-125a66b5899e',\n",
              " 'b60dbb4f-69a0-4c18-b64d-a590312b6430',\n",
              " 'ebdc4ee5-6a2e-4fce-a527-a4840121f0d5',\n",
              " '50595f1d-c604-46f4-80f6-fd93cd14b575',\n",
              " 'f42a1cf3-912d-4c89-87b9-580e79a8f29c',\n",
              " 'f313246b-c054-49b9-8a8d-a9024ad5b442',\n",
              " 'd8765bb5-401c-45ce-9934-aeeeeace3777',\n",
              " 'fa93a224-9e20-4569-bdcd-b008b857e20f',\n",
              " 'd29f463d-cc12-4fae-b1b9-913bf83ca9c9',\n",
              " '8f92b6e3-95c9-4b0f-b45a-e769c93ff149',\n",
              " '1dc2d8b8-350c-4a91-b784-4541f753b844',\n",
              " '140e4f7c-a9c8-4ba3-bf25-cfe11b363d81',\n",
              " '8b8e8fc7-c195-4464-8e2f-c5a31cd6eb97',\n",
              " '4388f598-04ce-466c-b3d5-56edf207bcf4',\n",
              " 'cd67afaf-672c-4a24-a88e-cf7c5ca3e674',\n",
              " 'eeb41259-f4ce-4866-9fb7-0df0e12562e1',\n",
              " '95646614-275c-4ced-9a9b-baaf50856de5',\n",
              " '0898dbc6-ad0e-4b14-ad1d-7eed8665a97c',\n",
              " 'b48143a7-e29e-4edb-b513-9a253896b858',\n",
              " '18509709-c4c5-4f23-ab92-bb3be21442a9',\n",
              " 'dc136653-2e40-4158-95e5-01d6b58f609b',\n",
              " 'd974e3a3-5a90-4bd0-9139-f8b51a0a6479',\n",
              " 'aed4b4a0-34eb-419b-9ff5-e6ac3bd9c2e9']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Bibliotecas necessárias\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain_chroma.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from uuid import uuid4\n",
        "\n",
        "## INDEXAÇÃO ##\n",
        "\n",
        "# Carregando os documentos\n",
        "loader = PDFPlumberLoader(\n",
        "    file_path = \"/content/A Startup Enxuta - Eric Ries.pdf\"\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "# Dividindo o documento em chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embedding\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# Vector Store\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"eric_ries_lean_startup\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"/content/chroma_langchain_db\",\n",
        ")\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(docs))]\n",
        "vector_store.add_documents(documents=docs, ids=uuids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformando o vector store em um objeto \"pesquisável\"\n",
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "_ekrW1cskB6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## RECUPERAÇÃO E GERAÇÃO ##\n",
        "\n",
        "# Função que formata os documentos retornados\n",
        "def formataDocumentos(docs):\n",
        "  return \"\\n\\n\".join([f\"Documento {k}:\\n{doc.page_content}\" for k, doc in enumerate(docs)])\n",
        "\n",
        "# Fazendo uma pergunta ao documento\n",
        "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature = 0)\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "Com base no contexto, responda a pergunta do usuário. Se não souber, diga que não sabe.\n",
        "\n",
        "Contexto:\n",
        "{conteudo}\n",
        "\n",
        "Pergunta:\n",
        "{pergunta}\n",
        "\n",
        "Sua resposta:\n",
        "\"\"\")\n",
        "\n",
        "rag = (\n",
        "    {\"conteudo\": retriever | formataDocumentos, \"pergunta\": RunnablePassthrough()} |\n",
        "    prompt |\n",
        "    llm |\n",
        "    StrOutputParser()\n",
        ")\n",
        "\n",
        "resposta = rag.invoke(\"De onde vem o crescimento de uma empresa?\")\n",
        "\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geltlf5foTyy",
        "outputId": "282a31a6-7343-4ccb-ffd8-dcdabb2ed67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O crescimento de uma empresa vem de um motor de crescimento, que é um mecanismo utilizado para alcançar um crescimento sustentável. Esse crescimento sustentável se caracteriza pelo fato de que novos clientes surgem das ações dos clientes passados. Existem quatro maneiras principais pelas quais os clientes passados podem impulsionar esse crescimento:\n",
            "\n",
            "1. **Boca a boca**: Clientes satisfeitos falam sobre o produto, atraindo novos clientes.\n",
            "2. **Efeito colateral da utilização do produto**: Produtos que geram status ou são virais podem influenciar outros a comprá-los.\n",
            "3. **Publicidade financiada**: A publicidade deve ser paga com a receita recorrente gerada pelos clientes, permitindo a aquisição de novos clientes de forma sustentável.\n",
            "4. **Compra ou uso repetido**: Produtos projetados para serem comprados repetidamente, como assinaturas ou recompras, também contribuem para o crescimento.\n",
            "\n",
            "Essas fontes de crescimento sustentável ajudam a movimentar ciclos de feedback que são essenciais para o sucesso a longo prazo da empresa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Busca dos documentos\n",
        "retriever.invoke(\"De onde vem o crescimento de uma empresa?\", k = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDE3Yh_kWvz6",
        "outputId": "a41e14db-ac24-415d-c146-56f1b3da5786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='259d5ba6-a44d-4d38-abca-e9f0bf1690a9', metadata={'Author': 'Eric Ries', 'CreationDate': \"D:20150330135456+00'00'\", 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Title': 'A Startup Enxuta', 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf', 'page': 150, 'source': '/content/A Startup Enxuta - Eric Ries.pdf', 'total_pages': 210}, page_content='DE ONDE VEM O CRESCIMENTO?\\nO motor de crescimento é o mecanismo que as startups utilizam para alcançar o crescimento\\nsustentável. Utilizo a palavra sustentável para excluir todas as atividades ocasionais que\\ngeram um surto de clientes, mas não têm impacto a longo prazo, tais como anúncios isolados\\nou uma proeza publicitária que pode ser utilizada para revitalizar o crescimento, mas não\\nconsegue sustentá-lo a longo prazo.\\nO crescimento sustentável se caracteriza por uma regra simples:\\nOs novos clientes surgem das ações dos clientes passados.\\nHá quatro maneiras principais de os clientes passados impulsionarem o crescimento\\nsustentável:\\n1. Boca a boca. Um nível natural de crescimento está integrado na maioria dos produtos\\nprovocado pelo entusiasmo dos clientes satisfeitos com o produto. Por exemplo, quando\\ncomprei meu primeiro DVR TiVo, não conseguia parara de falar dele para meus amigos e\\nfamiliares. Em pouco tempo, toda minha família estava usando esse aparelho.\\n2. Como efeito colateral da utilização do produto. Produtos de moda ou status, tais como\\nbens de luxo, promovem a consciência de si mesmos sempre que são usados. Quando você vê\\nalguém vestido com uma roupa de grife ou dirigindo um certo carro, você pode ser\\ninfluenciado a comprar aquele produto. Isso também é verdade em relação aos assim\\nchamados produtos virais, como Facebook ou PayPal. Quando um cliente envia dinheiro para\\num amigo usando o PayPal, o amigo fica exposto automaticamente ao produto PayPal.\\n3. Por meio de publicidade financiada. A maioria das empresas utiliza a publicidade para\\nincitar novos clientes a usar seus produtos. Para isso ser uma fonte de crescimento sustentável,\\na publicidade deve ser paga como resultado da receita recorrente, não de fontes ocasionais,\\ncomo o capital de investimento. Enquanto o custo de adquirir um novo cliente (o assim\\nchamado custo marginal) é menor do que a receita gerada pelos clientes (a receita marginal), o\\nexcesso (o lucro marginal) pode ser utilizado para conseguir mais clientes. Quanto maior o\\nlucro marginal, mais rápido o crescimento.\\n4. Por meio da compra ou do uso repetido. Alguns produtos são projetados para ser\\ncomprados repetidas vezes por meio de um plano de assinatura (uma empresa de tevê a cabo)\\nou de recompras voluntárias (comestíveis ou lâmpadas). Porém, diversos produtos e serviços\\nsão deliberadamente projetados como eventos ocasionais: por exemplo, o planejamento de um\\ncasamento.\\nEssas fontes de crescimento sustentável movimentam os ciclos de feedback que denominei\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depois de cobrirmos essa parte mais básica, você terá feito o seu primeiro sistema de R.A.G. e vamos partir para técnicas mais avançadas. Aí o trem vai ficar interessante.\n",
        "\n",
        "Fontes:\n",
        "- [LLMs como SOs](https://x.com/karpathy/status/1707437820045062561)\n",
        "- [Dados privados](https://x.com/RihardJarc/status/1778082161595208124)"
      ],
      "metadata": {
        "id": "VyCarHH0n24v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexação\n",
        "Anteriormente, tivemos uma visão geral sobre R.A.G.. Você viu conceitos de uma maneira mais ampla e viu o porquê de ele ser extremamente necessário. Agora, vamos dar um *zoom in* para ver os detalhes mais de perto.\n",
        "\n",
        "Vamos ver - com profundidade - uma das três etapas majoritárias que disse para vocês: **indexação, recuperação e geração**. Agora, vamos nos aprofundar mais na parte da indexação.\n",
        "\n",
        "## O que é indexação?\n",
        "Indexação é um processo onde fazemos os dados externos serem \"pesquisáveis\". É uma transformação que fazemos nos dados afim de deixá-los prontos para serem \"pesquisáveis\".\n",
        "\n",
        "## O processo macro\n",
        "<img src=\"https://i.ibb.co/39hZh10z/Lead-Entra-na-Lista-2.png\">\n",
        "\n",
        "A imagem mostra de uma forma muito didática o processo de indexação. Você pode ver que ele começa com o carregamento dos documentos, depois faz a divisão desses documentos em pedaços menores - chamados \"chunks\" - e, por fim, armazena esses pedaços menores em um *vector store*.\n",
        "\n",
        "Para ter indexação, claro que precisamos de um documento, certo? Bom, o Langchain possui mais de 160 integrações diferentes para podermos realizar o processo de R.A.G..\n",
        "\n",
        "**A primeira etapa é justamente obter o documento, que vou chamar de *Document Loading***: seja lendo um .pdf, pegando dados de uma página web, extraindo o texto de uma foto e muito mais.\n",
        "\n",
        "**Já a segunda etapa consiste em pegar esses documentos, como um livro inteiro, e dividir em chunks menores, que vou chamar de *Splitting***. Isso é necessário porque posteriormente vamos retornar os K chunks mais similares para utilizar de contexto no LLM, e se tivermos o documento inteiro vai dar aquele mesmo erro de *tokens exceeded*.\n",
        "\n",
        "Essa é apenas uma das razões pelas quais precisamos fazer a divisão dos documentos em chunks menores. Essa é uma razão necessária, porque, sem ela, o nosso R.A.G. nem funciona devido ao erro de tokens. Há outros motivos que não causam erros, mas pioram o desempenho do R.A.G.. Abaixo, você pode ver alguns benefícios de fazer a divisão em chunks\n",
        "- Menor taxa de alucinação\n",
        "- Respostas mais precisas\n",
        "- Mais especificidade dos metadados (veremos mais a frente sobre metadados)\n",
        "\n",
        "**Por fim, a última etapa é o processo de armazenar os dados em um banco de dados vetorial para que ele seja \"pesquisável\"**. O fato é: essa última etapa possui um passo intermediário chamado de *Criação de Embeddings*. Transformamos nosso documento textual em uma lista de números e armazenamos essa lista de números - chamada de Embedding - no banco de dados vetorial (vetorial porque, mais tecnicamente, chamamos essa lista de números de vetor de números).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4q7VpYBBkY7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Loading\n",
        "O bom de utilizar o Langchain é que ele já nos fornece integração nativa com diversos aplicativos para já extrair o conteúdo dos nossos materiais em formato de *Document*.\n",
        "\n",
        "### O formato Document\n",
        "Esse fomato Document é utilizado pelo Langchain para representar todo e qualquer tipo de conteúdo que as IAs vão utilizar. Ele possui dois atributos principais:\n",
        "- page_content\n",
        "- metadata\n",
        "\n",
        "#### page_content\n",
        "O atributo *page_content* possui o conteúdo textual daquele chunk. É fácil confundir e pensar que trata-se do conteúdo de uma página inteira por causa do **page**_content, mas não. É o conteúdo textual de um chunk!\n",
        "\n",
        "#### metadata\n",
        "Já o atributo *metadata* possui informações específicas sobre aquele chunk específico. No exemplo de um PDF de um livro, os metadados poderiam ser o **nome do autor**, a **página**, o **total de páginas** e muito mais. Se o material utilizado fosse a transcrição de um vídeo do YouTube, um dos metadados poderia ser o ***timestamp***, isto é, o tempo de início daquela fala da transcrição.\n",
        "\n",
        "**Sempre que possível considere adicionar metadados aos chunks! Pode ajudar futuramente.**\n",
        "\n",
        "### Exemplo\n",
        "Para exemplificar, vou utilizar um dos chunks do .pdf do livro do Eric Ries. Mas antes, precisamos criar a nossa classe Document."
      ],
      "metadata": {
        "id": "etjM2N2rr8qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a classe Document\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Document(BaseModel):\n",
        "  page_content: str\n",
        "  metadata: dict\n",
        "\n",
        "# Não precisamos criar a classe Document, podemos simplesmente importar\n",
        "# from langchain.schema import Document"
      ],
      "metadata": {
        "id": "ZmIEnRCVxIoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Documento de exemplo\n",
        "doc = Document(metadata={'Author': 'Eric Ries', 'CreationDate': \"D:20150330135456+00'00'\", 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Title': 'A Startup Enxuta', 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf', 'page': 150, 'source': '/content/A Startup Enxuta - Eric Ries.pdf', 'total_pages': 210}, page_content='DE ONDE VEM O CRESCIMENTO?\\nO motor de crescimento é o mecanismo que as startups utilizam para alcançar o crescimento\\nsustentável. Utilizo a palavra sustentável para excluir todas as atividades ocasionais que\\ngeram um surto de clientes, mas não têm impacto a longo prazo, tais como anúncios isolados\\nou uma proeza publicitária que pode ser utilizada para revitalizar o crescimento, mas não\\nconsegue sustentá-lo a longo prazo.\\nO crescimento sustentável se caracteriza por uma regra simples:\\nOs novos clientes surgem das ações dos clientes passados.\\nHá quatro maneiras principais de os clientes passados impulsionarem o crescimento\\nsustentável:\\n1. Boca a boca. Um nível natural de crescimento está integrado na maioria dos produtos\\nprovocado pelo entusiasmo dos clientes satisfeitos com o produto. Por exemplo, quando\\ncomprei meu primeiro DVR TiVo, não conseguia parara de falar dele para meus amigos e\\nfamiliares. Em pouco tempo, toda minha família estava usando esse aparelho.\\n2. Como efeito colateral da utilização do produto. Produtos de moda ou status, tais como\\nbens de luxo, promovem a consciência de si mesmos sempre que são usados. Quando você vê\\nalguém vestido com uma roupa de grife ou dirigindo um certo carro, você pode ser\\ninfluenciado a comprar aquele produto. Isso também é verdade em relação aos assim\\nchamados produtos virais, como Facebook ou PayPal. Quando um cliente envia dinheiro para\\num amigo usando o PayPal, o amigo fica exposto automaticamente ao produto PayPal.\\n3. Por meio de publicidade financiada. A maioria das empresas utiliza a publicidade para\\nincitar novos clientes a usar seus produtos. Para isso ser uma fonte de crescimento sustentável,\\na publicidade deve ser paga como resultado da receita recorrente, não de fontes ocasionais,\\ncomo o capital de investimento. Enquanto o custo de adquirir um novo cliente (o assim\\nchamado custo marginal) é menor do que a receita gerada pelos clientes (a receita marginal), o\\nexcesso (o lucro marginal) pode ser utilizado para conseguir mais clientes. Quanto maior o\\nlucro marginal, mais rápido o crescimento.\\n4. Por meio da compra ou do uso repetido. Alguns produtos são projetados para ser\\ncomprados repetidas vezes por meio de um plano de assinatura (uma empresa de tevê a cabo)\\nou de recompras voluntárias (comestíveis ou lâmpadas). Porém, diversos produtos e serviços\\nsão deliberadamente projetados como eventos ocasionais: por exemplo, o planejamento de um\\ncasamento.\\nEssas fontes de crescimento sustentável movimentam os ciclos de feedback que denominei\\n')"
      ],
      "metadata": {
        "id": "_dGA6Kynw6Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc.page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "aZ-sJXL5xtUa",
        "outputId": "e9d185bf-b1e7-4f93-c14a-0fd47a837df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DE ONDE VEM O CRESCIMENTO?\\nO motor de crescimento é o mecanismo que as startups utilizam para alcançar o crescimento\\nsustentável. Utilizo a palavra sustentável para excluir todas as atividades ocasionais que\\ngeram um surto de clientes, mas não têm impacto a longo prazo, tais como anúncios isolados\\nou uma proeza publicitária que pode ser utilizada para revitalizar o crescimento, mas não\\nconsegue sustentá-lo a longo prazo.\\nO crescimento sustentável se caracteriza por uma regra simples:\\nOs novos clientes surgem das ações dos clientes passados.\\nHá quatro maneiras principais de os clientes passados impulsionarem o crescimento\\nsustentável:\\n1. Boca a boca. Um nível natural de crescimento está integrado na maioria dos produtos\\nprovocado pelo entusiasmo dos clientes satisfeitos com o produto. Por exemplo, quando\\ncomprei meu primeiro DVR TiVo, não conseguia parara de falar dele para meus amigos e\\nfamiliares. Em pouco tempo, toda minha família estava usando esse aparelho.\\n2. Como efeito colateral da utilização do produto. Produtos de moda ou status, tais como\\nbens de luxo, promovem a consciência de si mesmos sempre que são usados. Quando você vê\\nalguém vestido com uma roupa de grife ou dirigindo um certo carro, você pode ser\\ninfluenciado a comprar aquele produto. Isso também é verdade em relação aos assim\\nchamados produtos virais, como Facebook ou PayPal. Quando um cliente envia dinheiro para\\num amigo usando o PayPal, o amigo fica exposto automaticamente ao produto PayPal.\\n3. Por meio de publicidade financiada. A maioria das empresas utiliza a publicidade para\\nincitar novos clientes a usar seus produtos. Para isso ser uma fonte de crescimento sustentável,\\na publicidade deve ser paga como resultado da receita recorrente, não de fontes ocasionais,\\ncomo o capital de investimento. Enquanto o custo de adquirir um novo cliente (o assim\\nchamado custo marginal) é menor do que a receita gerada pelos clientes (a receita marginal), o\\nexcesso (o lucro marginal) pode ser utilizado para conseguir mais clientes. Quanto maior o\\nlucro marginal, mais rápido o crescimento.\\n4. Por meio da compra ou do uso repetido. Alguns produtos são projetados para ser\\ncomprados repetidas vezes por meio de um plano de assinatura (uma empresa de tevê a cabo)\\nou de recompras voluntárias (comestíveis ou lâmpadas). Porém, diversos produtos e serviços\\nsão deliberadamente projetados como eventos ocasionais: por exemplo, o planejamento de um\\ncasamento.\\nEssas fontes de crescimento sustentável movimentam os ciclos de feedback que denominei\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZW5KVitxuFl",
        "outputId": "5972040c-9f11-4dcd-e3f7-d98434987996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Author': 'Eric Ries',\n",
              " 'CreationDate': \"D:20150330135456+00'00'\",\n",
              " 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]',\n",
              " 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]',\n",
              " 'Title': 'A Startup Enxuta',\n",
              " 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf',\n",
              " 'page': 150,\n",
              " 'source': '/content/A Startup Enxuta - Eric Ries.pdf',\n",
              " 'total_pages': 210}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, os metadados desse chunk indicam diversas informações úteis para podermos utilizar durante o R.A.G..\n",
        "\n",
        "### Exemplos de *loading*\n",
        "Agora, vou fazer dois exemplos de integrações que o Langchain possui para fazermos o corregamento do conteúdo dos materiais. Lembrando que acima no material possui um exemplo mostrando uma integração com PDFs.\n",
        "\n",
        "É importante dizer que a maioria deles você pode encontrar pesquisando no google: \"**langchain [NOME]**\", como **langchain arxiv** ou **langchain youtube**, ou **langchain pdf**. Geralmente, o primeiro link será a resposta para a sua dúvida."
      ],
      "metadata": {
        "id": "2P1UIrDlME1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ArXiv\n",
        "O ArXiv possui diversos artifos científicos para podermos trabalhar em cima e fazer a busca no R.A.G.. A maioria das integrações específicas vai necessitar de instalação adicional de bibliotecas, como é necessário aqui no caso do ArXiv.\n",
        "\n",
        "Observe o código abaixo realizando a integração."
      ],
      "metadata": {
        "id": "2zKiipKPHuJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando o ArXiv e PyMuPDF\n",
        "!pip install arxiv pymupdf --quiet"
      ],
      "metadata": {
        "id": "xQ3J_QgVN97L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0e92c5-7c41-4c3c-cc43-9fed77c1d826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo a integração\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "# Instanciando o objeto de carregamento\n",
        "loader = ArxivLoader(\n",
        "    query = \"large language models\",\n",
        "    load_max_docs = 3,\n",
        "    load_all_available_meta = True,\n",
        "\n",
        ")\n",
        "\n",
        "# Carregando os materiais\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "ioTLt4XGOAjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Olhando o primeiro material\n",
        "doc = docs[0]\n",
        "doc.page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rPj1rs5sOakA",
        "outputId": "b195aec9-ea3f-4015-95db-7eb21b83c736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lost in Translation\\nMay 2023\\nA report from\\nGabriel Nicholas\\nAliya Bhatia\\nLarge Language Models in \\nNon-English Content Analysis\\nGABRIEL NICHOLAS\\nResearch Fellow at the Center for Democracy & Technology.\\nALIYA BHATIA\\nPolicy Analyst, Free Expression Project at the Center for \\nDemocracy & Technology.\\nThe Center for Democracy & Technology (CDT) is the leading \\nnonpartisan, nonprofit organization fighting to advance civil rights and \\ncivil liberties in the digital age. We shape technology policy, governance, \\nand design with a focus on equity and democratic values. Established in \\n1996, CDT has been a trusted advocate for digital rights since the earliest \\ndays of the internet. The organization is headquartered in Washington, \\nD.C., and has a Europe Office in Brussels, Belgium.\\nA report from\\nGabriel Nicholas and Aliya Bhatia\\nWITH CONTRIBUTIONS BY\\nSamir Jain, Mallory Knodel, Emma Llansó, Michal Luria, Nathalie Maréchal, Dhanaraj Thakur, and \\nCaitlin Vogus.\\nACKNOWLEDGMENTS \\nWe thank Pratik Joshi, Sebastin Santy, and Aniket Kesari for their invaluable feedback on the technical \\naspects of this report. We also thank Jacqueline Rowe, Damini Satija, and Ángel Díaz for their \\ninsightful comments and suggestions. The translation of our executive summary is made possible by \\nGlobal Voices Translations and with the help of Iverna McGowan, Maria Villamar, Ophélie Stockhem, \\nand Tomás Pomar. All views in this report are those of CDT. \\nThis work is made possible through a grant from the John S. and James L. Knight Foundation.\\nSuggested Citation: Nicholas, G. and Bhatia, A. (2023) Lost in Translation: Large Language Models \\nin Non-English Content Analysis. Center for Democracy & Technology. https://cdt.org/insights/lost-\\nin-translation-large-language-models-in-non-english-content-analysis/\\nReferences in this report include original links and links archived and shortened by the Perma.cc service. \\nThe Perma.cc links also contain information on the date of retrieval and archive. \\nThis report is licensed under a Creative Commons Attribution 4.0 International License.\\nLost in Translation\\nLarge Language Models in Non-\\nEnglish Content Analysis\\nLost in Translation\\nCDT Research\\n4\\nCDT Research\\n4\\nContents\\nExecutive Summary\\x08\\n5\\nIntroduction\\x08\\n8\\nI.\\t Background\\x08\\n12\\nA. How Large Language Models Work\\x08\\n12\\nB. The Resourcedness Gap: Why the Largest Language \\nModels are in English\\x08\\n15\\nC. Multilingual Language Models: Efforts to Bridge the \\nResourcedness Gap\\x08\\n19\\nII.\\t Limitations of Language Models in English and \\nNon-English Contexts\\x08\\n23\\nA. Concerns with Building and Deploying Large \\nLanguage Models\\x08\\n23\\nB. Limitations of Multilingual Language Models\\x08\\n25\\nIII.\\tRecommendations\\x08\\n31\\nA. Companies\\x08\\n31\\nB. Researchers and Funders\\x08\\n33\\nC. Governments\\x08\\n36\\nWorks Cited\\x08\\n39\\n5\\nLost in Translation\\nExecutive \\nSummary\\nT\\nhe internet is the primary source of information, economic \\nopportunity, and community for many around the world. \\nHowever, the automated systems that increasingly mediate our \\ninteractions online — such as chatbots, content moderation \\nsystems, and search engines — are primarily designed for and work far \\nmore effectively in English than in the world’s other 7,000 languages.\\nIn recent years, large language models have become the dominant \\napproach for building AI systems to analyze and generate language \\nonline, but again, they have been built primarily for the English \\nlanguage. A large language model (e.g., Open AI’s GPT-4, Meta’s \\nLLaMa, Google’s PaLM) is a machine learning algorithm that scans \\nenormous volumes of text to learn which words and sentences \\nfrequently appear near one another and in what context. Large language \\nmodels can be adapted to perform a wide range of tasks across different \\ndomains. They are most known for being used to build chatbots, \\nsuch as ChatGPT, but researchers and technology companies also \\nuse them for content analysis tasks, such as sentiment analysis, text \\nsummarization, and hate speech detection. Google, Meta, Microsoft, \\nand other companies have already incorporated large language models \\ninto their core product functions, such as content moderation and \\nsearch. Other vendors soon may incorporate them into automated \\ndecision-making systems, such as resume scanners.\\nRecently though, researchers and technology companies have attempted \\nto extend the capabilities of large language models into languages other \\nthan English by building what are called multilingual language models. \\nInstead of being trained on text from only one language, multilingual \\nlanguage models are trained on text from dozens or hundreds of \\nlanguages at once. Researchers posit that multilingual language models \\ninfer connections between languages, allowing them to apply word \\nassociations and underlying grammatical rules learned from languages \\nwith more text data available to train on (in particular English) to \\nthose with less. In some applications, multilingual language models \\noutperform models trained on only one language — for instance, a \\nmodel trained on lots of text from lots of languages, including Hindi, \\nmight perform better in Hindi contexts than a model just trained on \\nHindi text.\\nMultilingual language models give technology companies a way to scale \\ntheir AI systems to many languages at once, and some have already \\nbegun to integrate them into their products. Online service providers \\nin particular have deployed multilingual language models to moderate \\nLost in Translation\\nCDT Research\\n6\\ncontent: Meta uses a multilingual language model to detect harmful content on its \\nplatforms in over 100 languages; Alphabet’s Perspective API uses one to detect toxic \\ncontent in eighteen different languages; Bumble uses one to detect and take action on \\nunwanted sexual messages around the world.\\nMultilingual language models allow technologists to attempt to build models in languages \\nfor which they otherwise might not have enough digitized text. Languages vary widely \\nin resourcedness, or the volume, quality, and diversity of text data they have available to \\ntrain language models on. English is the highest resourced language by multiple orders of \\nmagnitude, but Spanish, Chinese, German, and a handful of other languages are sufficiently \\nhigh resource enough to build language models in. Medium resource languages, with fewer \\nbut still high-quality data sets, such as Russian, Hebrew, and Vietnamese, and low resource \\nlanguages, with almost no training data sets, such as Amharic, Cherokee, and Haitian \\nCreole, have too little text for training their own large language models. Language data in \\nlow resource languages is also often of particularly poor quality: either it is mistranslated or \\neven nonsensical language scraped from the internet, or is limited to sources with narrow \\ndomains, such as religious texts and Wikipedia. This gap in data availability between \\nlanguages is known as the resourcedness gap.\\nMultilingual language models are designed to address these gaps in data availability by \\ninferring semantic and grammatical connections between higher- and lower-resource \\nlanguages, allowing the former to bootstrap the latter. However, this architecture \\nraises its own concerns. Multilingual language models are still usually trained \\ndisproportionately on English language text and thus end up transferring values and \\nassumptions encoded in English into other language contexts where they may not \\nbelong. For example, a multilingual model might associate the word “dove” in all \\nlanguages with “peace” even though the Basque word for dove (“uso”) can be an insult. \\nThe disparity in available data also means multilingual language models work far better \\nin higher resource languages and languages similar to them than lower resource ones. \\nModel developers will sometimes try to fill in these gaps with machine-translated text, \\nbut translation errors may further compound language misrepresentation. And when \\nmultilingual language models do fail, their unintuitive connections between languages \\ncan make those problems harder to identify, diagnose, and fix.\\nLarge language models’ general use in content analysis raises further concerns. \\nComputational linguists argue that large language models are limited in their capacity \\nto analyze forms of expression not included in their training data, meaning they may \\nstruggle to perform in new contexts. They may also reproduce any biases present in \\ntheir training data. Often, this text is scraped from the internet, meaning that large \\nlanguage models may encode and reinforce dominant views expressed online.\\nLarge Language Models in Non-English Content Analysis\\n7\\n\\u200bCompanies, researchers, and governments each have a role to play in protecting the \\npublic from the potential dangers of multilingual language model content analysis \\nsystems. To ensure better public accountability, companies that deploy large language \\nmodels should always be transparent about how they use them and in which languages. \\nCompanies should deploy language models with narrow remits and adequate channels \\nfor human review.\\nResearchers and research funders meanwhile should invest in efforts to improve the \\nuse and performance of language models in languages other than English, in particular, \\nto reduce failures that disparately impact speakers of lower-resourced languages. The \\nbest way to do this is by supporting language-specific research communities, who can \\npromote the virtuous cycle of collecting data, curating datasets, training language \\nmodels, publishing, and building applications. Local language speakers and context \\nexperts need to be part of each step of this process and also be curating the data and \\nassessing the language models deployed by large, global online services.\\nFinally, governments need to be careful about how they use or encourage the use of \\nlarge language models. Large language models should never power systems used to make \\nhigh-stakes decisions without oversight, such as decisions about immigration status or \\nhealthcare, nor should governments mandate or inadvertently require by law the use of \\nlarge language model-powered systems to moderate content from online services. Instead, \\ngovernments should convene different stakeholders to align on what norms and guardrails \\nshould be around developing and deploying large language models.\\nLarge language models in general and multilingual language models in particular \\nhave the potential to create new economic opportunities and improve the web for \\nall. However, mis- or over-application of these technologies poses real threats to \\nindividuals’ rights, such as undermining their right to free expression by inaccurately \\ntaking down a person’s post on social media or their right to be free of discrimination \\nby misinterpreting an individual’s job or visa application. Multilingual language \\nmodels specifically can inadvertently further entrench the Anglocentrism they are \\nintended to address. In light of these limitations, technology companies, researchers, \\nand governments must consider potential human and civil rights risks when studying, \\nprocuring, developing, or using multilingual language models to power systems, in \\nparticular when they are used to make critical information available or play a role \\nin decisions affecting people’s access to economic opportunities, liberty, or other \\nimportant interests or rights.\\nExecutive Summary\\nLost in Translation\\nCDT Research\\n8\\nIntroduction\\nD\\nespite the modern internet’s power to mobilize and connect \\npeople around the world, the web still does not reflect the \\nlinguistic diversity of its users. In particular, the automated \\nsystems that increasingly mediate our interactions online — \\nsuch as chatbots, search engines, and content moderation systems — \\nare built using and perform far better on English-language text than \\nthe world’s other 7,000 languages (Kornai, 2013; Sengupta, 2022). \\nIndividuals speaking languages other than English face barriers to \\nexpressing themselves freely online and may face greater challenges \\nwhen it comes to accessing critical information, public services, and \\neven asylum and safety (Torbati, 2019).\\nIn the last few years, however, there have been rapid advancements in \\ndeveloping machine learning tools that can analyze content in a wide \\nvariety of languages and across different domains. Large language \\nmodels, machine learning tools trained on enormous amounts of text \\nto recognize patterns in language, power many of these systems. Large \\nlanguage models already underlie translation apps, search autocomplete, \\nand chatbots such as ChatGPT. They are known for being adaptable to \\nmany different language tasks, and today, researchers and technologists \\nare constantly on the lookout for new applications and contexts \\nin which to deploy them. Since the late 2010s, major U.S.-based \\ntechnology companies have mostly invested in building large language \\nmodels that work primarily for English, such as Open AI’s GPT-4, \\nMeta’s LLaMa, and Google’s PaLM.\\nRecently, companies and researchers have begun building and researching \\nmultilingual language models, large language models trained on text \\ndata from several different languages at once. Meta’s XLM-RoBERTa \\n(XLM-R) for instance is trained on text from 100 languages (Meta AI, \\n2019) at once. Google’s mBERT, a multilingual version of its popular \\nBERT model, is trained on 104 languages. Researchers claim that these \\nmodels extend the multifaceted capabilities of large language models to \\nlanguages other than English, even to languages for which there is little or \\nno text data for the model to learn from (Artetxe & Schwenk, 2019; Wu \\n& Dredze, 2019).\\nTechnology companies have their own interests in improving how \\nwell large language models work in different languages. Some may \\nwant to make their products available in multiple languages to gain a \\ncompetitive edge in emerging and populous markets. Online services \\nLarge Language Models in Non-English Content Analysis\\n9\\nthat host user-generated content may especially be interested in using multilingual \\nlanguage models to detect and take action on hate speech, disinformation, and other \\ncontent that violates their policies or the law (Dulhanty et al., 2019). This is top of \\nmind for services after facing criticism for not taking more aggressive action against \\ncontent that incited violence and genocide in Ethiopia, Nigeria, and Myanmar, among \\nothers. Services have begun to deploy multilingual language models into their content \\nmoderation systems: Meta claims their XLM-R model can detect harmful content \\nin all 100 languages it is trained on (Meta AI, 2021); Alphabet’s Perspective API uses \\na large language model to detect toxic content in eighteen different languages (Lees \\net al., 2022); Bumble uses one to detect rude and abusive messages in at least fifteen \\nlanguages (Belloni, 2021). Technology companies are also repurposing these models to \\nmake health care information available and soon may reach into other domains as well \\n(Lunden, 2023).\\nIn the future, governments could also seek to use automated systems built using \\nlarge language models to make information available, answer questions in languages \\nspoken by their constituents (in the form of chatbots), or, more dangerously, analyze \\ninformation to make critical decisions such as benefits allocation or refugee status \\ndeterminations (Kinchin & Mougouei, 2022).\\nStill, studies show that even multilingual language models struggle to deal with the \\nwide disparities between different languages in how much text data they have available \\nto train and test language models. English has, by multiple orders of magnitude, more \\ntext data available than any other language and commands most of the attention of the \\nnatural language processing research community. The abundance of English language \\ndata stems from its position as the official or de facto language of international business, \\npolitics, and media, itself a legacy of British colonialism and American neocolonialism \\nand the subsequent erasure of regional and indigenous languages. American technology \\ncompanies have further entrenched English as the predominant language of the internet \\nby rolling out early standards, coding languages, and social media platforms in English \\nlong before other languages.\\nThe hegemony of English data means that most large language models, even \\nmultilingual ones, are built predominantly using Standard English language text and \\nwork best in Standard English language contexts. Spanish, Chinese, Arabic, and a few \\nother “high resource” languages also have significant amounts of text data available, but \\nmany “medium resource” languages, such as Hindi and Portuguese, and “low resource” \\nlanguages, such as Haitian Creole and Swahili, have hardly any data available at all, and \\nmultilingual language models perform much worse in those languages. This skewed \\nemphasis fails to reflect the diversity of languages spoken by the world’s internet users \\nand further perpetuates the dominance of the English language.\\nIntroduction\\nLost in Translation\\nCDT Research\\n10\\nDespite being deployed in real-world systems, multilingual language models have largely \\nbeen absent from public discourse, particularly about digital rights and public policy, \\nand have instead been relegated to computer science academia and tech company public \\nrelations. This paper seeks to address this gap by offering several resources to bolster \\npolicy discussions. Part I provides a simple technical explanation of how large language \\nmodels work in general, why there is a gap in available data between English and other \\nlanguages, and how multilingual language models attempt to bridge that gap. Part II \\naccounts for the challenges of doing content analysis with large language models in \\ngeneral and multilingual language models in particular, namely:\\n1.\\t Multilingual language models often rely on machine-translated text that can \\ncontain errors or terms native language speakers don’t actually use. \\n2.\\t When multilingual language models fail, their problems are hard to identify, \\ndiagnose, and fix.\\n3.\\t Multilingual language models do not and cannot work equally well in all languages.\\n4.\\t Multilingual language models fail to account for the contexts of local language \\nspeakers.\\nFinally, Part III provides recommendations for companies, researchers, and \\npolicymakers to keep in mind when considering studying, developing, and deploying \\nlarge and multilingual language models to do content analysis. These recommendations \\noffer guidance concerning when large language models should or should not be \\ndeployed, how to improve their performance in non-English languages, and how to \\nensure better accountability and transparency to local language stakeholders.\\nBefore proceeding, two notes on the terminology used in this primer. First, this paper \\nfocuses specifically on one category of applications for large language models: content \\nanalysis, or, the inference and extraction of information, themes, and concepts from \\ntext. The Center for Democracy & Technology (CDT) has written many times about \\nthe limitations of automated content analysis systems (Duarte et al., 2017; Shenkman \\net al., 2021) and the civil liberty risks they can pose, particularly in areas such as content \\nmoderation, student activity monitoring, hiring and more (Grant-Chapman et al., \\n2021; Nicholas, 2022; Vallee & Duarte, 2019). Large language models are already deeply \\nintegrated into many of these technical systems, particularly content moderation, and will \\nsoon become part of many more. Public discourse about large language models has so far \\ndisproportionately focused on text generation, an important area but not the only one. \\nMany of the shortcomings of large language models presented in this report also apply \\nto text generation. As such, this report can be read as a primer on some of the limits of \\ngenerative AI systems as well. However, we choose to focus on content analysis for this \\nreport because of the potential dangers associated with using these models to host and \\nmake information available and the impacts on free expression rights.\\nLarge Language Models in Non-English Content Analysis\\n11\\nSecond, this paper focuses on how multilingual language models perform in languages \\nother than English. We use the shorthand of “non-English languages” for easy reading \\nand because it is the terminology used in the machine learning and policy literature. \\nWe recognize the irony that this term centers the English language and misleadingly \\nimplies all other languages are a monolith. Where possible, we elaborate upon the types \\nof languages we are writing about and make distinct references to specific languages and \\ncultural contexts that will elude models trained primarily in English. In some instances, \\nwe think the term “non-English” captures the sheer Anglocentrism of many of these \\nmodels well by articulating the limited scope in which they are trained and tested.\\nIntroduction\\nLost in Translation\\nCDT Research\\n12\\nI. Background\\nA. How Large Language Models Work\\nNatural language processing (NLP) is a subfield of artificial intelligence \\nand linguistics concerned with building computer systems that can \\nprocess and analyze language. NLP underlies many technologies we \\nencounter every day — spellcheck, voice assistants like Siri or Alexa, \\nresume scanners, language translators, and automated hate speech \\ndetection tools, to name a few. Until only a few years ago, when \\ntechnologists wanted to teach a computer to perform a given NLP task, \\nthey would build a system specifically tailored to that task. To create a \\nspam detection system for instance, a technologist might gather many \\nemails, mark which ones are and are not spam, use some of those emails to \\ntrain an algorithm and use others to test how well that algorithm works.\\nToday though, the field has fundamentally reoriented itself around \\nrepurposing large language models to solve nearly every problem. \\nA language model is a mathematical function trained to solve a text \\nprediction task like the following, “Given a sequence of words, predict \\nwhat word will likely come next.” For example, a language model might be \\ngiven the phrase “I was a bad student, I used to skip ____,” and generate \\nas an output that there is a high percent chance the missing word is \\n“class,” a low percent it is “rope,” and a near zero percent it is “clamoring.”\\nThe distribution of language that the model learns in the process can \\neasily be repurposed to many different language tasks. The most often \\ndiscussed application is text generation: conversational agents like \\nChatGPT can repurpose this text prediction task to answer questions, \\nsummarize text, and generate overall “human”-sounding speech. \\nHowever, chatbots are just one application of large language models. \\nOnce a large language model is built, it can be further trained on a \\nsmaller dataset to improve its performance in a specific task, a process \\ncalled fine-tuning. Today, for example, a developer building a spam \\ndetection system might take a general large language model already \\nbuilt by someone else — say Google’s BERT — and fine-tune it to the \\nspecific task of spam detection using a handful of emails already labeled \\nspam or not spam. By building it on top of a language model, the spam \\ndetection system will do a better job of detecting spam that doesn’t \\nperfectly match the language available in the email dataset.\\nLanguage models are not new. Computational linguists have used \\nstatistical models to try to infer rules about language since the 1980s \\n(Nadkarni et al., 2011) and have used “neural networks” (an algorithm \\nloosely modeled on how neurons connect in the brain) to do so since \\nthe early 2010s (Mikolov et al., 2013). What is new though is their \\nLarge Language Models in Non-English Content Analysis\\n13\\nlargeness. Early language models could not be trained on as much data, since they had \\nto read text in sequence, a process that could not be sped up by using more computing \\npower. These early language models struggled to analyze words within the broader \\ncontext of a sentence or document: for instance, one fine-tuned to detect suicidal \\nideation might have difficulty distinguishing between expressions of self-harm (“I \\njust wish I was dead”) and humor (“omg I’m dead”). But in 2017, Google researchers \\nreleased a paper on a new architecture called transformers, which allowed language \\nmodels to train on lots of data at the same time, in parallel rather than in sequence \\n(Vaswani et al., 2017). These transformer-based language models could ingest so much \\ndata simultaneously that they could learn associations between entire sequences of \\nwords, not just individual words. Instead of being shown just {“dead”}, the model \\nwould see a word in its entire context, {“dead”, [“omg”, “I’m”, “_____”]}, thus creating \\na much richer representation of language. Today, the only limit on the size of a language \\nmodel — how much data it ingests and how many connections it makes between \\ndifferent sequences of words (i.e. parameters) — is how much data one can find and \\nhow much developers are willing to spend on processing power.\\nThe output a language model produces is called a representation space, a map of the \\nsequences of words that commonly appear near one another in the training text. For \\nexample, the phrases “It’s so cold outside!” and “I better wear a jacket” may be near one \\nanother in a language model’s representation space, since those sentences often appear \\nclose to one another in writing. This kind of proximity can lead to language models \\ninferring patterns within language that can then help them conduct tasks that it is not \\nexplicitly trained in. In this case, sentences about cold weather being mapped near each \\nother mean the large language model could be trained to detect whether a given phrase \\nis about temperature.\\nWith enough data, a large language model may have such a rich and multifaceted \\nrepresentation of a language that it can learn to do new tasks with only a few, or even \\nzero examples to fine-tune on. For instance, the spam detection system described earlier \\ncould be built with little to no spam to fine-tune on. This capability is called “few-shot” \\nor “zero-shot learning” and is one of the greatest promises of large language models, so \\nmuch so that the original GPT-3 white paper is entitled “Language Models are Few-\\nShot Learners” (Brown et al., 2020).\\nImportantly though, large language models only learn the distribution of language, not \\nits meaning (Bender & Koller, 2020). In the previous “cold” example, the model has not \\nlearned that when one is cold, one puts on a jacket or anything about the deeper meanings \\nof “cold” and “jacket,” only that the words often appear near one another. If one of the \\ndocuments a large language model trains on is a humorous blogpost about the best shorts \\nto wear in cold temperatures, the model could just as easily learn that “shorts” and “cold” \\nare related. Similarly, if a model is trained only on very formal language data, it may never \\nlearn that “nippy” or “brick” (New York City slang) can refer to cold as well.\\nI. Background\\nLost in Translation\\n14\\nTechnologists often try to address these shortcomings by training language models \\non more and more data. If a model is exposed to more data, the idea is that it will be \\nfamiliar with more contexts, and outliers like the ironic cold-weather shorts blogpost \\nwill be outweighed by more representative data. This has led to ballooning in the size \\nof large language models. BERT, a popular open-source model built by Google in \\n2018, was trained on 800 million words from free books and 2.5 billion words from \\nEnglish Wikipedia (Devlin et al., 2019). Two years later, OpenAI released its closed \\nsource GPT-3, which was trained on half a trillion mostly-English words crawled from \\nthe internet (Brown et al., 2020). Google’s PaLM, released in 2022, trained on 780 \\nbillion words, mostly from English-language websites and social media conversations \\n(Chowdhery et al., 2022). As models have grown in size, so have the computation costs \\nof training them. While BERT costs a few thousand dollars in computing power to \\ntrain from scratch and is often trained by academics to build new topic- or language-\\nspecific models (Izsak et al., 2021), GPT-3 and PaLM-sized models cost millions or \\ntens of millions of dollars to train (Sharir et al., 2020). Future models will only be \\nmore expensive, leaving only the most well-off companies able to afford to build them \\n(Bommasani et al., 2021).\\nFigure 1. Language model \\nrepresentation space. A langauge model’s \\nreprsentation space, collapsed into two \\ndimensions. In reality, these models often \\nhave thousands or tens of thousands of \\ndimensions.\\nSource: (Amer, 2022)\\nLost in Translation\\n14\\nWhen is \\nBoxing Day?\\nWhat is the date \\nof Boxing Day?\\nHow many species \\nof sharks are there?\\nHow many species of the \\nGreat White shark are there?\\nIt’s so cold \\noutside!\\nI better wear \\na jacket.\\nLarge Language Models in Non-English Content Analysis\\n15\\nModels are expensive to initially train, but once built, their representations are relatively \\ncheap to use and be fine-tuned for different tasks. Thus, many technologists simply \\nuse pretrained large language models built by others (usually large companies, with the \\nexpertise and resources) instead of paying to create their own. The few big pretrained \\nmodels that exist have thus become a sort of infrastructure, known as “foundation \\nmodels” (Bommasani et al., 2021). This gives many technologists access to the state of \\nthe art capabilities, but it also creates a single point of failure for the sector as a whole: \\nif a foundation model has a problem, it will persist across many applications. And these \\nmodels are so large and complicated that even when they are open source, researchers \\ncannot understand the underlying logic they use to come up with individual decisions.\\nMany of the largest and most advanced of these foundation models — such as \\nOpenAI’s GPT-4, Google’s PaLM, and Meta’s LLaMa — are trained primarily on \\nEnglish language data. In the next section, we explore one reason why that may be: the \\nresourcedness gap.\\nB. The Resourcedness Gap: Why the Largest \\nLanguage Models are in English\\nEnglish is the closest thing there is to a global lingua franca. It is the dominant language \\nin science, popular culture, higher education, international politics, and global \\ncapitalism; it has the most total speakers and the third-most first-language speakers \\n(Ethnologue, 2023b). It is the primary language spoken on the internet, accounting \\nfor 63.7% of websites, despite being spoken by only 16% of the world’s population \\n(Richter, n.d.). This dominance does not stem from any sort of inherent linguistic \\nsuperiority: rather it is the colonial and neocolonial legacy of nearly three hundred \\nyears of the preeminent global superpower speaking English — first Great Britain, \\nthen the United States. The British government prioritized the English language \\nthrough official language policies to facilitate trade and in an attempt to “modernize” \\nits colonies, and as British, and later American trade became globally dominant, so too \\ndid English (Corradi, 2017; Phillipson, 1992). Prioritization of the English language \\ncame at the expense of other regional and indigenous languages and accelerated \\nlanguage endangerment and economic marginalization, which still impedes digital \\ninvestment into these languages worldwide (Rowe, 2022; S. Zhang et al., 2022). \\nAmerican companies continue to perpetuate the dominance of the English language in \\na new more insidious form, by making online services available to global users without \\ncomparable investment into the languages they speak (Amrute et al., 2022; Kupfer & \\nMuyumba, 2022).\\nI. Background\\nLost in Translation\\nCDT Research\\n16\\nAs a result of these forces, English also dominates the field of natural language \\nprocessing, and there is vastly more raw text data available in English than in any other \\nlanguage by orders of magnitude (Joshi et al., 2020).\\xa0English has the most digitized \\nbooks and patents, the largest Wikipedia, and the biggest internet presence. English is \\nalso by far the language paid the most attention by the global NLP research community. \\nIt is so hegemonic within the field that NLP papers about the English language \\ntypically do not even mention the language in the title or abstract (Bender, 2019). As \\nFigure 2 shows, even among NLP papers that do mention a language in the abstract, \\nEnglish is mentioned over ten times as often as the next most mentioned language, \\nGerman (ACL Rolling Review Dashboard, 2022).\\nThis wealth of data and research makes it significantly easier to build large language \\nmodels in English than in any other language. More raw text data, also known as \\nunlabeled data, means more data for the model to be trained on; more research means \\nthat there are more datasets annotated with information, also known as labeled data, \\nthat can be used to test how well models complete different types of language tasks. \\nThis creates a virtuous cycle for English-language NLP —\\xa0more labeled and unlabeled \\ndata leads to more research attention, which leads to increased demand for labeled and \\nunlabeled data, and so on.\\nEnglish is the prime example of a high resource language, a language for which a lot of \\nhigh-quality data resources exist. Though it has the most data available of any language \\n(English could be called an “extremely” high resource language), there are six other \\nlanguages that could be considered high resource — the official UN languages list, \\nminus Russian, plus Japanese (see Table 1). There are also a few dozen medium resource \\nlanguages, such as Urdu, Italian, and Tagalog, with another one or two orders of \\nmagnitude less data, or about one hundredth or one-thousandth of available English data. \\nThe rest of the world’s 6,000 plus languages can be considered low resource or extremely \\nlow resource, with only small amounts of written text available (Joshi et al., 2020).\\nResourcedness can vary within languages as well. Languages such as Arabic and Spanish \\ndiffer so much between dialects that many are mutually incomprehensible, even if \\nthey mostly use the same written form. Languages can also have different sociolects, \\nvarying across different social groups, identity groups, and contexts (e.g. formal versus \\ninformal). Regional dialects and sociolects can vary in degrees of difference from \\nhaving different vocabulary and grammatical structures (e.g. Australian English or \\nAfrican American English versus Standard American English) to make extensive use of \\nborrowed words from other languages (e.g. Nigerian English, Indian English), to fully \\nhybrid bilingual dialects (e.g. Spanglish, Hinglish). However, the available digitized \\ntext of language often doesn’t reflect the full spectrum of variation that exists within a \\nlanguage. (Bergman & Diab, 2022). Data scraped from the internet in particular over-\\nindexes Standard English spoken by younger people in developed countries (Luccioni \\n& Viviano, 2021). Other languages have just as much dialectical diversity as English and \\nalso likely over-index on certain dialects.\\nLarge Language Models in Non-English Content Analysis\\n17\\nFigure 2. Languages mentioned in \\npaper abstracts. Top most mentioned \\nlanguages in abstracts of papers published \\nby the Association for Computational \\nLinguistics, May 2022-January 2023.\\nSource: (Santy et al., 2023)\\nPaper Abstracts\\nLanguages with less data available also often have lower quality data available, either \\nbecause it is mislabeled or otherwise not representative of how people actually speak \\nthe language. This is particularly true with web-crawled data, a key data source for \\nlarge language models (Khan & Hanna, 2023). Non-English language data scraped \\nfrom the internet is more often machine translated, scanned from an image, or both, \\nand each of those processes introduces opportunities for error (Dodge et al., 2021). \\nLow- and medium-resource language data on the internet is more often pornographic, \\nnonsensical, or non-linguistic content (Kreutzer et al., 2022). It is also often labeled as \\nthe incorrect language – around 95% of the time for many low resource languages – \\nbecause automatic language identification works much more poorly with insufficient \\ndata, thus creating a circular problem (Caswell et al., 2020). Languages with the worst \\nquality web data are disproportionately those written in non-Latin scripts (e.g. Urdu, \\nJapanese, Arabic) and those spoken in the Global South (e.g. African languages, \\nminority languages in the Middle East, non-Mandarin Chinese languages) (Kreutzer et \\nal., 2022).\\n17\\n0\\n200\\n300\\n100\\nEnglish\\nKorean\\nIndonesian\\nThai\\nFrench\\nGreek\\nTurkish\\nFinnish\\nGerman\\nSpanish\\nSwahili\\nClassical Chinese\\nHindi\\nHebrew\\nPolish\\nItalian\\nKinyarwanda\\nArabic\\nRussian\\nTalugu\\nDutch\\nJapanese\\nVietnamese\\nPortuguese\\nLatin\\nMarathi\\n311\\n27\\n18\\n16\\n16\\n16\\n16\\n13\\n10\\n7\\n7\\n7\\n6\\n5\\n5\\n5\\n4\\n4\\n4\\n4\\n4\\n3\\n3\\n3\\n3\\n3\\nI. Background\\nLost in Translation\\nCDT Research\\n18\\nLow resource languages also tend to have data that comes from a less diverse set of \\nsources. The clean data that does exist often comes from places such as Wikipedia, the \\nBible, and parliamentary proceedings, particularly in large language models that depend \\non drawing parallels between low and high resource languages (see III.B and III.C) \\n(Nekoto et al., 2020). None of these data sources is representative of a language as a \\nwhole. For example, there is a significant gender gap when it comes to who contributes \\nto Wikipedia, with studies finding that the percentage of women who edit Wikipedia \\narticles remains “dismally low” (Callahan & Herring, 2011; Vitulli, 2018), and it \\ndoesn’t reflect a more casual style of speech. Some text on Wikipedia is also machine-\\ntranslated — Cebuano, Swedish, and Waray for instance are some of the Wikipedia \\nlanguages with the most articles, but most are translated by the same bot (Lokhov, \\n2021). The Bible is similarly its own unique domain, unrepresentative of language at \\nlarge, but is overrepresented in the training data for non-English large language models. \\nThis can lead to errors in the tone and substance of language. For example, for a period \\nof time, running a word repeated enough times through Google translate produced a \\nreligious-sounding text: the word “dog” pasted two dozen times and translated from \\nMaori to English produced text about Jesus’ return at the end of days (Christian, 2018).\\nThe resourcedness of a language is often out of sync with the number of speakers or \\ninternet users that language has. Hindi, Bengali, and Indonesian are medium-resource \\nlanguages yet each has hundreds of millions of speakers (Joshi et al., 2020). Guaraní, \\nan Indigenous language spoken by most of the ~7 million-person population of \\nParaguay, hardly has any data resources at all (Góngora et al., 2021). Fula, a language \\nspoken by tens of millions of West Africans, also has few data sets (Nguer et al., 2020). \\nDespite over 600 million internet users across the African continent, nearly all African \\nlanguages remain low-resourced.\\nTable 1. Categories of language \\nresourcedness. Languages divided into \\ndifferent levels of resourcedness, according \\nto labeled and unlabeled datasets available \\nas of 2020.\\nSource: (Joshi et al., 2020)\\nResourcedness\\nLanguages\\nNumber of Languages\\nNumber of Speakers\\nExtremely High Resource\\nEnglish\\n1\\n1.1B\\nHigh Resource\\nArabic, French, Japanese, German, \\nSpanish, Mandarin\\n6\\n2.7B\\nMedium Resource\\nDutch, Vietnamese, Korean, \\nPortuguese, Hindi, Slovak, Hebrew, \\nIndonesian, Afrikaans, Bengali, etc.\\nDozens\\n2.7B\\nLow Resource\\nHaitian Creole, Tigrinya, Swahili, \\nBavarian, Cherokee, Zulu, Burmese, \\nTelugu, Maltese, Amharic, etc.\\nHundreds\\n0.5B\\nExtremely Low Resource\\nDahalo, Warlpiri, Popoloca, \\nWallisian, Bora, etc.\\nThousands\\n1.1B\\nLost in Translation\\n18\\nLarge Language Models in Non-English Content Analysis\\n19\\nMany scholars have worked to try to close this resourcedness gap between high and low \\nresource languages. Individual NLP communities have formed around many languages in \\norder to kickstart and perpetuate the virtuous cycle of research attention and benchmark \\ndevelopment, including collectives such as IndoNLP for languages spoken in Indonesia \\nand Masakhane for African languages (Cahyawijaya et al., 2022; Nekoto et al., 2020; Orife \\net al., 2020), and conferences such as the Association for Computational Linguistics’ \\nlow resource language track, and AmericasNLP for indigenous languages (ACL, 2021; \\nAmericasNLP, 2022; Masakhane, n.d.). Tech companies have also sought to expand the \\nnumber of language models their models work in, in part by creating more data sets, \\nincluding with projects like Facebook’s No Language Left Behind project and Google’s \\n1000 Languages Initiative (NLLB Team et al., 2022; Vincent, 2022). DARPA even \\nfunded the Low Resource Languages for Emergent Incidents (LORELEI) program in \\n2014 to improve translation about emergency incidents into low resource languages \\n(Corvey, 2014). But the gaps between English, other high resource languages, and low \\nresource languages remain large and are growing exponentially greater by the day, at least \\nin terms of available, raw digitized data.\\nThe response by the NLP community has not just been to collect more language \\ndata but also to employ technical tricks to help language models squeeze the most \\nperformance out of the little data they have. In the next section, we discuss the primary \\ntechnical architecture developers use to do this: multilingual language models.\\nC. Multilingual Language Models: Efforts to \\nBridge the Resourcedness Gap\\nIn English, most large language models are monolingual, meaning that they train mostly \\non data from one language. Researchers have also built monolingual models in non-\\nEnglish languages: for instance, the architecture for Google’s BERT model — one of \\nthe most popular and cheapest to train — has been utilized for French (CamemBERT), \\nItalian (AlBERTo), Arabic (AraBERT), Dutch (BERTje), Basque (BERTeus), Maltese \\n(BERTu), and Swahili (SwahBERT), to name a few (Agerri et al., 2020; Antoun et al., \\n2020; de Vries et al., 2019; G. Martin et al., 2022; L. Martin et al., 2020; Micallef et al., \\n2022; Polignano et al., 2019). However, in general, these monolingual models perform \\nworse in their respective languages than the best English models do in English because \\nthey don’t have as much data to train on.\\nThis lack of data manifests in different ways depending on the specific task a model is \\nfine-tuned to perform. Some language model capabilities — usually ones that depend \\non fact retrieval — improve linearly with size. For instance, the more data a language \\nmodel is exposed to, the better it is at answering trivia questions or reformatting \\ndata (Srivastava et al., 2022). Other capabilities — usually ones with multiple steps \\nI. Background\\nLost in Translation\\nCDT Research\\n20\\nor components — exhibit a “breakthrough” behavior, where once a model reaches a \\ncertain size, it improves sharply at the task. For instance, language models typically are \\nunable to write code or add three digit numbers until they train on a certain amount \\nof data, at which point their performance improves dramatically (Ganguli et al., 2022). \\nLow and extremely low resource languages often do not have enough data to train a \\nlarge language model at all, but medium and even high resource languages may not \\nhave the hundreds of millions, or billions of words of text data necessary to achieve the \\nbreakthroughs that English can (Y. Zhang et al., 2021).\\nBesides technical limitations, companies may not be interested in deploying a different \\nmonolingual model for every language their product is available in for business reasons \\nas well. Maintaining and debugging one large language model for each language \\nintroduces costs that scale per language introduced, introducing complexity and \\nadditional overhead costs. Companies that seek to expand into new global markets \\nwill likely try to keep their costs fixed by reusing as much infrastructure as possible, \\nincluding language models.\\nTherefore, instead of using monolingual models to do NLP tasks in non-English \\nlanguages, researchers and developers most often use multilingual language models, \\nsuch as Google’s mBERT and Meta’s XLM-R, which are trained on texts from \\nmany different languages at once. Like their monolingual counterparts, multilingual \\nlanguage models are trained on a fill-in-the-blank task. However, by training on text \\nfrom several different languages, multilingual language models can, at least in theory, \\ninfer connections between languages, acting as a sort of bridge between high and low \\nresource languages, allowing the former to bootstrap the latter.\\nFor instance, imagine that an Indian climate change researcher wants to use a language \\nmodel to collect all Hindi-language tweets about the weather. A monolingual language \\nmodel trained on just Hindi text may not have enough data to have seen the words \\n“thaand” (“cold” in Hindi) and “shaal” or (“shawl” in Hindi) appear near one another \\nin text, so it may miss that tweets to the effect of “Main Agast mein shaal pahanta \\nhoon” (“I put a shawl on in August”) is a sentence about cold weather.1 A multilingual \\nmodel, trained on data from English, Hindi, and many other languages may have seen \\ntext where “thaand” appears near “cold,” “shaal” appears near “shawl,” and “cold” \\nappears near “shawl,” thereby allowing the model to infer that “thaand” and “shaal” are \\ninterrelated terms.\\nMultilingual language models are usually not trained on equal volumes of data from \\neach language: mBERT for instance is trained on 15.5 GB of English text but as little \\nas 10 MB of Yoruba text (Wu & Dredze, 2020). Even BLOOM, a popular multilingual \\nmodel by BigScience with a particular focus on language representation, has 30% of its \\n1\\t  Transliterated into Roman script for ease of reading for an English-language reader.\\nLarge Language Models in Non-English Content Analysis\\n21\\nFigure 3. Monolingual vs multilingual \\nlanguage model representation \\nspace. A visualization of a monolingual \\nand a multilingual langauge model’s \\nrepresentation space, collapsed into three \\ndimensions.\\nSource: (Schwenk, 2019) \\ntraining text in English (BigScience Workshop et al., 2023). In large part, this is because \\nof the lack of available data in these languages, which come disproportionately from \\nWikipedia and religious texts, as discussed earlier (see Part I.C).\\nJust as a monolingual language model can be fine-tuned to work better on an individual \\ntask, a multilingual language model can be fine-tuned to work better in an individual \\nlanguage. Imagine for instance a developer who wants to use a multilingual language \\nmodel to detect Indonesian election disinformation on social media. One way they \\ncould do it is by using an out-of-the-box multilingual model, such as BLOOM, and \\nfine-tuning it by showing examples of false narratives circulated in Indonesian related \\nto the local election. This likely would not work very well though, since BLOOM has \\nonly been exposed to a limited amount of data on Indonesian text —\\xa0only 1.2% of its \\ntraining data is in Indonesian (BigScience Workshop et al., 2023). Another better way \\nto do it, if the developer has access to more Indonesian language data, would be first to \\nfine-tune the model on additional Indonesian text (essentially, continuing to learn the \\nfill-in-the-missing-word task, but this time just in Indonesian) and then further fine-\\ntuning it on the task election disinfo detection using that dataset.\\nModel developers though do not always have enough text data to sufficiently fine-\\ntune a multilingual model to work in a specific language. To make up for this, they \\noften use imperfect machine-translated text. The two main methods of incorporating \\ntranslated text are called translate-train or translate-test methods. With translate-train, \\na multilingual language model is fine-tuned on data that has been translated from \\n(usually) English into a desired lower resource language (Conneau & Lample, 2019). \\nWith translate-test, a (usually) English monolingual language model is fine-tuned \\non data translated from the desired language into English, and all testing data gets \\ntranslated into English as well (Artetxe, Labaka, et al., 2020).\\nI. Background\\n21\\nThe tree is green.\\nThe tree is green.\\nEl árbol es verde.\\nMonolingual model\\nIt is cold today.\\nMultlingual model\\nI put on a shawl.\\nI like to sing. \\nI like to sing. \\nJ’aime chanter. \\nAaj bohut thaand hai.\\nMain ek shaal pahanta hoon.\\nI put on a shawl.\\nIt is cold today.\\nLost in Translation\\nCDT Research\\n22\\nImagine, for example, a developer building a language model to detect terrorist content \\nin the Basque language with a handful of examples of terrorist content in Basque \\nbut not enough Basque text data to properly fine-tune a language model. With the \\ntranslate-train approach, a developer would take a large volume of English text data, \\nmachine translate it into Basque, use that data to fine-tune a pretrained multilingual \\nlanguage model, and then further fine-tune it to the task of terrorist content detection \\nusing the native Basque data. With translate-test, a developer would fine-tune a \\npretrained English language model on data translated from Basque to English, and \\nthen further fine-tune it by translating the terrorist content data they have into English. \\nSubsequently, to analyze Basque text, it would first have to be translated into English \\nbefore being evaluated by the model. Reliance on translated data raises many concerns, \\nas discussed in Part II.C.1.\\nHowever, translated texts can help multilingual language models learn connections \\nbetween languages. By feeding a model parallel texts — for instance, explicitly \\ninforming it that “baahar bohut thand hai” and “It’s so cold outside” have the same \\nmeaning — it can better extrapolate other language parallels as well (e.g. NLLB Team et \\nal., 2022; Reid & Artetxe, 2022). Multilingual language models can learn connections \\nbetween languages without explicit labeling, instead inferring relationships between \\nlanguages on its own through borrowed words, numbers, and URLs (Pires et al., 2019). \\nIn general, NLP researchers understand little about why it is that multilingual language \\nmodels can be effectively fine-tuned to work in languages that they have relatively little \\ndata for (Conneau, Khandelwal, et al., 2020; Pires et al., 2019; Wu & Dredze, 2019). \\nSome argue that it is because multilingual language models have inferred language-\\nagnostic concepts and universal rules that can be applied to any language (Artetxe, Ruder, \\net al., 2020; Chi et al., 2020; Conneau, Wu, et al., 2020; Tsvetkov et al., 2016). Others \\nsay that multilingual language models are just effective imitators (Bender et al., 2021; \\nLauscher et al., 2020). The debate is impossible to fully resolve because of the overall \\ncomplexity and opacity of large language models, but so far evidence suggests that at \\nbest, the linguistic universals they learn are limited to narrow semantic and syntactic \\ndomains (Libovický et al., 2019; Wu & Dredze, 2019), such as learning plural/singular \\nverb agreement across multiple languages (de Varda & Marelli, 2023). But even if a model \\ncan infer syntactic or semantic commonalities between languages, such inferences will \\nnot necessarily help it manage more complex, context-dependent tasks (Choi et al., 2021). \\nFor instance, in some languages, multilingual language models do no better than random \\nguessing at detecting hate speech (Lin et al., 2022). As will be discussed in the next section, \\nthese are hardly the only limits of multilingual language models.\\n23\\nLost in Translation\\nII. Limitations of \\nLanguage Models \\nin English and Non-\\nEnglish Contexts T\\nhe press, technology companies, and social media are abuzz \\nabout the potential of large language models. In this section, \\nhowever, we discuss the shortcomings of these models, \\nparticularly as they operate in non-English language contexts. \\nIn the first section, we discuss general concerns with building and \\ndeploying large language models. These concerns apply both to the \\nEnglish and non-English contexts. In the second section, we look at the \\nproblems more specifically raised by multilingual language models.\\nA. Concerns with Building and \\nDeploying Large Language Models\\n1. LARGE LANGUAGE MODELS ARE BOUND BY \\nLANGUAGE THEY HAVE SEEN BEFORE AND STRUGGLE \\nTO PERFORM IN NEW CONTEXTS.\\nA large language model does not understand language; instead, it makes \\nprobabilistic inferences about text based on the distribution of language \\nwithin the data it is trained on. Bender and Koller argue that this means \\nlanguage models are limited to contexts they have encountered before \\nand struggle greatly in those they have not (2020). NLP researchers have \\nalready proven this is the case in generative AI by demonstrating several \\nunintuitive outcomes: for instance, language models are better able to \\nperform mathematical operations with numbers that appear frequently \\nin written language (e.g., multiplying numbers by 24), than numbers \\nthat appear infrequently (e.g. multiplying numbers by 23) (Razeghi \\net al., 2022). Large language models may exhibit similar limitations in \\ncontent analysis as well. For instance, if a large language model were \\nused to analyze a candidate’s resume, it may struggle to account for \\nlesser-known companies or newer skill sets without up-to-date, domain-\\nspecific data to fine-tune on. These tasks are reliant on in-context \\nknowledge and without domain-specific training, i.e. training an off-\\nthe-shelf large language model with text relevant to the task at hand, \\nthese models are likely to perform poorly and their purported domain-\\nagnostic abilities should garner skepticism (Duarte et al., 2017). \\nLost in Translation\\nCDT Research\\n24\\n2. LARGE LANGUAGE MODELS REPRODUCE THE BIASES, VALUES, \\nAND HARMS OF THE DATA THEY TRAIN ON.\\nLarge language models are built using vast quantities of text scraped from the internet \\nand exhibit all the biases and limitations of their data source (Okerlund et al., 2022). \\nSome commonly used datasets, such as Common Crawl, include large volumes of \\nhate speech and sexually explicit content (Luccioni & Viviano, 2021). Other problems \\nare more nefarious. For example, researchers found that when GPT-3 generated \\ncompletions for the prompt “Two Muslims walked into a___,” 66% of completions \\nincluded violent language, three times more than for other religious groups (Abid et \\nal., 2021). Others have found similar entrenched biases against people with disabilities, \\nfor example inferring negative sentiment from sentences that include disability-related \\nterms (Hutchinson et al., 2020).\\nThough technologists often try to pull out explicitly harmful data from training \\nsets, models can still reify harms, such as referring to “women doctors” or calling \\nundocumented immigrants “illegals” (Bender et al., 2021). Removing these instances \\nof harmful data from training datasets, which are disproportionately outsourced \\nto underpaid staff around the world, also imposes labor and psychological burdens \\n(Williams et al., 2022). \\nEven if datasets are rid of specific examples of harmful text, they will nonetheless \\ncontain values and assumptions that are encoded into the language we speak and the \\ndominant perspectives that exist in many pieces of written text, particularly government \\ndocuments or state-run media pieces that may make up the bulk of text available for \\nlow resource languages (Bender et al., 2021). Many machine learning researchers fail to \\nconsider these problems in their work — one study found that 98% of machine learning \\npapers mention no negative potential of the technologies they are describing (Birhane \\net al., 2022). Yet the risks are very real: as Birhane & Prabhu put it, “Feeding AI systems \\non the world’s beauty, ugliness, and cruelty, but expecting it to reflect only the beauty \\nis a fantasy” (2021). When these problems exist in any particularly popular foundation \\nmodel, they proliferate across many different applications built on top of that model.\\n3. THE DATA LARGE LANGUAGE MODELS TRAIN ON RAISE \\nCOPYRIGHT AND PRIVACY CONCERNS.\\nLegal experts also raise concerns about copyright and ownership of text that make up \\nthe vast quantities of data that train and distinguish large models (Ebers et al., 2022; \\nOkerlund et al., 2022). Getty Images has sued the creators of Stable Diffusion, an AI \\ntool that creates images based on written prompts, claiming that the toolscraped Getty’s \\ndatabases of proprietary images and photos without permission (Vincent, 2023a). Legal \\nquestions about ownership of text and whether scraping proprietary text is lawful (e.g., \\nbecause it constitutes fair use) or not remain unanswered (Kublik, n.d.).\\nLarge Language Models in Non-English Content Analysis\\n25\\nII. Limitations of Language Models in English and Non-English Contexts\\nSome datasets that large language models train on are likely to capture examples of \\nlanguage from sites such as social media, raising personal data privacy concerns. There \\nis a high possibility that in gathering exchanges from social media networks, training \\ndatasets inadvertently contain private and even sensitive information, which increases \\nthe risk of models leaking details like names, phone numbers, or addresses from the data \\non which they’re trained (Carlini et al., 2021, 2023).\\n4. TRAINING LARGE LANGUAGE MODELS COULD HAVE A \\nSIGNIFICANT ENVIRONMENTAL IMPACT.\\nFinally, there are increasing concerns about the environmental cost of producing large \\nlanguage models. Scholars and advocates have raised concerns about the environmental \\nimpact of training these models, particularly the largest ones with billions of \\nparameters, due to their intense computation requirements (Crawford, 2021; Okerlund \\net al., 2022). There is preliminary research attempting to quantify the energy impacts \\nof computation at this scale (Kaack et al., 2022), but some early estimates suggest that \\ntraining a single BERT model, one that serves as the foundation for some multilingual \\nlanguage models, requires as much energy as a trans-American flight (Strubell et al., \\n2019). Large language models, like GPT-3, require thousands of times more (Heikkilä, \\n2022). Png writes that these costs may be concentrated in poorer countries, where \\nserver farms and raw materials required to build necessary infrastructure are often \\nlocated (2022).\\nB. Limitations of Multilingual Language Models\\n1. MULTILINGUAL LANGUAGE MODELS OFTEN RELY ON MACHINE-\\nTRANSLATED TEXT THAT CAN CONTAIN ERRORS OR TERMS NATIVE \\nLANGUAGE SPEAKERS DON’T ACTUALLY USE.\\nIncorporating machine-translated data into the training and fine-tuning of multilingual \\nlanguage models creates various opportunities for the model to malfunction. \\nMultilingual language models that depend on translation may struggle to build \\naccurate representations of words or concepts which have different connotations in \\ndifferent languages. For instance, in English, “dove” is a term associated with peace, but \\nits equivalent in Basque, “uso,” is an emasculating insult. A translation-based cross-\\nlingual model that does not train on the word “uso” used in its native context could \\npotentially fail to see it used in a call for violence since the English mapping is so closely \\nassociated with “peace.”\\nLost in Translation\\nCDT Research\\n26\\nAnother issue is what NLP practitioners call the “translationese” problem (Yu et al., \\n2022) — that is, machine-translated language materially differs from how human \\nnative speakers naturally use language (Bizzoni et al., 2020; Teich, 2003). In generative \\nAI, translationese can result in mono- or multilingual language models simplifying \\nor overcomplicating sentences, producing repeated words, using too common or too \\nuncommon words, borrowing too much or too little from the original language, and \\nother patterns of speech native speakers would not use (Volansky et al., 2015). These \\nmistakes are not consistent between languages or systems, so it would be difficult for \\nmodels to be able to systematically root them out, though some argue that it is possible \\n(Yu et al., 2022).\\nThe problems of machine translation spread beyond models that intentionally train on \\nit. The web is filled with machine-translated text, and models that train on web-scraped \\ndata will inadvertently encounter a lot of it, particularly in low resource languages \\n(Kreutzer et al., 2022). For instance, a lot of the Catalan data that exists on the web, \\nparticularly on websites using the .cat top-level domain, is translated using Google \\nTranslate, even on official government websites (Pym et al., 2022). Even benchmarks to \\ntest how well multilingual language models work in high and low resource languages are \\noften translated from another language, leaving researchers with less of a sense of how \\nwell these models work on language as spoken by native speakers. For instance, OpenAI \\ntested GPT-4’s capabilities in 26 languages, but using only benchmarks translated from \\nEnglish (OpenAI, 2023).\\n2. MULTILINGUAL LANGUAGE MODELS FAIL TO ACCOUNT FOR THE \\nCONTEXTS OF LOCAL LANGUAGE SPEAKERS.\\nAs discussed earlier, large language models only work well in contexts similar to \\ncontexts of the data they are trained on. A language model trained on legal texts, \\nfor instance, will perform much better on law-related tasks than medical tasks \\nor interpreting the Quran (Koehn & Knowles, 2017). This poses a problem for \\nmultilingual language models, which, particularly in low resource languages, are trained \\non text that is translated from other language contexts or comes from a few distinctive \\ncontexts, such as Wikipedia and the Bible. Multilingual language models that are not \\ntrained on large volumes of text from native speakers of a given language will more \\noften fail at tasks that require knowledge of an individual speaker’s local context, such \\nas hate speech detection and resume scanning (Lin et al., 2022).\\nImagine, for example, a multilingual language model fine-tuned to detect anti-\\nMuslim content in Assamese, a low-resource language with fifteen million speakers, \\npredominantly in northeast India (Ethnologue, 2023a). Assamese and Bengali are both \\nmedium resource languages, so a multilingual model may draw connections between \\nthe two. However, anti-Muslim hate speech is very closely tied to historical events and \\nthe specific political conditions of Assam. For instance, the term “Bangladeshi Muslim,” \\nLarge Language Models in Non-English Content Analysis\\n27\\nII. Limitations of Language Models in English and Non-English Contexts\\nneutral in many other languages and contexts, is a hate speech dog whistle in Assamese \\nbecause it casts Assamese Muslims as foreigners (a concept that is itself closely tied to the \\nIndian government’s repatriation efforts) (Avaaz, 2019). A multilingual model neither \\ntrained on extensive native Assamese text nor explicitly trained by a language expert would \\nlikely not be able to capture this hyperlocal distinction.\\nMultilingual language models work by transferring between language contexts, but that \\ntransfer often means simply that the context of higher resource languages overwrites \\nlower resource ones. Spanish, for instance, tends to use more adjectives and analogies \\ndescribing extreme situations than English, so a sentiment detection algorithm that \\ntransfers linguistic properties over from English may mischaracterize Spanish text as \\nhaving a stronger emotional valence than it would to a native speaker (Stadthagen-\\nGonzalez et al., 2017). This structure transfer can also bring the biases of a source \\nlanguage into a target language (Savoldi et al., 2021). For instance, if a language without \\ngender pronouns, such as Hungarian or Yoruba, is mapped onto a language with \\ngendered third-person pronouns, such as English or French, the language model could \\nforce gender associations and biases of the gendered language onto the non-gendered \\none, as often occurs in translation (Prates et al., 2020) (see Figure 4).\\nFigure 4. Google Translate from \\nHungarian to English. A screenshot of \\nGoogle Translate, circa 2020, showing how \\nthe multilingual language models project \\ngender onto genderless languages.\\nSource: (Prates et al., 2020)\\n27\\nLost in Translation\\nCDT Research\\n28\\n3. MULTILINGUAL LANGUAGE MODELS DO NOT AND CANNOT WORK \\nEQUALLY WELL IN ALL LANGUAGES.\\nMultilingual language models not only do not work equally well in all languages but \\nthey cannot, since the more languages a multilingual model is trained on, the less it \\ncan capture unique traits of any specific languages. This problem is called the curse \\nof multilinguality (Lauscher et al., 2020). Large language model developers are thus \\nforced to trade off performance between disparate languages; making a model work \\nbetter in Hindi for example, may come at a cost to its performance in English. In \\npractice, when technology companies must choose which languages to deprioritize \\nwithin their multilingual language models, they may be incentivized to have them \\nbe languages where speakers tend to be less wealthy, have less political power, or live \\noutside of the company’s priority markets, thus exacerbating the resourcedness gap they \\nare designed to address.\\nIn general, semantic and syntactic similarity to a high resource language protects \\nfrom the curse of multilinguality (Eronen et al., 2023). For instance, Muller et al. \\ntested mBERT on languages it had not explicitly trained on before and found that it \\nworked better in Swiss German (related to German, a high resource language), than \\nit did in Estonian (a Uralic language, like medium resource languages Hungarian and \\nFinnish), than it does Uyghur (a Turkic language, distant from any high or medium \\nresource language, with four alphabets) (2021). In general, multilingual language \\nmodels struggle with languages written in non-Latin scripts (Pires et al., 2019; Ruder \\net al., 2021), language isolates (languages etymologically distinct from all other \\nlanguages, such as Basque), and families of languages less connected to those of high \\nresource languages. This threatens to create a poor-get-poorer dynamic for languages \\nthat are only similar to other low resource languages, as is the case with many widely \\nspoken African languages including Swahili, Amharic, and Kabyle (Joshi et al., 2020). \\nThis dynamic further strengthens the post-colonial structural inequality discussed \\nthroughout this report.\\nMultilingual language models are also forced to trade off between languages in the \\nvocabulary they use. Large language models train on the problem of predicting the next \\nword in a sentence. If a model is trying to guess the word to fill in “Today I feel ___,” it \\nwill have a harder time doing so if it has to choose between ten million possible words \\nfrom any language instead of just a few hundred thousand English words. The total \\nnumber of words a language model has to choose from is called its vocabulary size. The \\nlarger a model’s vocabulary size, the more different possible words it can generate and \\nrecognize, but also the more computational resources it takes to train. Multilingual \\nlanguage models use all kinds of shortcuts to get their vocabulary size down. For instance, \\nLarge Language Models in Non-English Content Analysis\\n29\\nthey will often transliterate languages into Latin scripts or train the model to guess the \\nnext subword (e.g. breaking “tasks” into “ta” and “##sks”) or letter instead of the full \\nword, thus collapsing the barrier between languages (Tay et al., 2022; C. Wang et al., \\n2020). These shortcuts cut down on costs, but they also reduce a model’s ability to \\ncapture semantic relationships between words, thus degrading its performance overall.\\nVocabulary is often decided by how frequently different words, subwords, and \\nletters appear in a model’s training text, and since multilingual language models are \\ntrained mostly on English data, their vocabularies will skew towards English as well. \\nA multilingual model may have a relatively obscure word like “riposte” in its English \\nvocabulary, but be may missing common words in other high resource languages (e.g., \\n“escritorio” in Spanish), common subwords in medium resource languages, (e.g., “tzv” \\nin Hebrew), and entire letters in low resource languages (e.g., a character that appears in \\nTigrinya but not other Ge’ez-based scripts). This inferior representation makes models \\nperform worse in a variety of tasks, and makes content analysis systems far easier to trick \\nby doing things like changing white space, using typos, or in the case of toxic content \\ndetection, adding common, positive words like “love” (Gröndahl et al., 2018; Lees et al., \\n2022).\\n4. WHEN MULTILINGUAL LANGUAGE MODELS FAIL, THEIR \\nPROBLEMS ARE HARD TO IDENTIFY, DIAGNOSE, AND FIX.\\nNLP practitioners depend on benchmarks to determine both how well a language \\nmodel performs at specific tasks and how close it is in general to achieving “natural \\nlanguage understanding” (Bender & Koller, 2020). This latter type of benchmarking \\nis very difficult in all languages, since it is hard to generalize about a language model’s \\ncapabilities from only a handful of disparate tests (Raji et al., 2021). However, the \\nchallenges of both types of benchmarks are exacerbated in the multilingual context. \\nThe disparities in NLP research attention and labeled data between languages mean \\nthat there are far more benchmarks and tasks that can be used to test models in English \\nthan in other languages, particularly low resource ones. Models developed to operate in \\nnon-English contexts are still usually tested with benchmarks translated from English \\nwhich, as discussed earlier, is often markedly different from the target language.\\nThe alternative to translation is hiring people local to the contexts a model is being \\napplied to and paying them to create data sets and develop benchmarks. This works \\nparticularly well for models built to do a specific task in a specific language (Nguyen, \\n2020; Tattle, n.d.), but is very expensive and resource intensive to scale up for models \\nmeant to work in many languages and contexts. It also raises challenging questions \\nfor detecting bias in language models (Talat et al., 2022) and performing inherently \\nII. Limitations of Language Models in English and Non-English Contexts\\nLost in Translation\\nCDT Research\\n30\\npolitical tasks, such as content moderation. For instance, a social media company trying \\nto create a dataset of inflammatory content posted in Bosnia and Herzegovina needs \\npeople who are experts in multiple ethnic conflicts and languages (Bosnian, Serbian, \\nMontenegrin, and Macedonian) but also unbiased in those conflicts, all in a country \\nthat lacks media pluralism or a strong civil society sector (Article 19, 2022). Scaling this \\nto every geopolitical problem discussed in all languages on a given online service is a \\ndaunting, if not impossible, task.\\nWhen problems with multilingual language models can be found, it is often difficult \\nto determine why they are occurring. Large language models are already opaque, even \\nto those who develop them — neural networks, the core technology underlying large \\nlanguage models, are known for being particularly obtuse and for representing language \\nin a way that doesn’t map cleanly onto human-understandable concepts (Nicholas, \\n2020). However, multilingual language models are particularly opaque because they \\nmake unintuitive, hard-to-trace connections between languages. Take for instance, \\nthis case from an NLP paper: the Google researchers behind the Perspective API, a \\nmodel for detecting “toxic” content, found that their model flagged tweets that used \\nthe Italian word “sfiga” (which roughly translates to “bad luck”) as hate speech because \\ntwo of the three examples included in the training dataset that contained the subword \\n“sfiga” were labeled as hate speech (“sfigati” is an insult meaning “loser”) (Lees et \\nal., 2020). If this were a multilingual model that had mapped Italian learnings onto \\nTurkish analysis, perhaps sentences with the equivalent Turkish word for “unlucky” \\n(“şanssız”) would also be flagged as hate speech. Even if researchers had access to all the \\ndata used to train that multilingual model, it would be extremely difficult to locate and \\nfix this bug without knowing Italian or understanding how the model had mapped \\nthese relationships.\\n31\\nLost in Translation\\nIII. Recommendations E\\nfforts to improve language models’ performance in various \\nlanguages and contexts are exciting, as they may boost \\nconnectivity and information exchange for billions of users \\naround the world. However, language models are limited in their \\ncapabilities, and employing them too widely, without safeguards, or for \\nthe wrong kinds of tasks has the potential to raise civil liberties concerns \\nand erect new barriers (Maundu, 2023). Unthinking deployment \\nof large language models may impede peoples’ ability to access \\ninformation, employment, and public benefits, with disparate impacts \\nfor individuals in the Global South where many of the low resource \\nlanguages are spoken. We should be cautious about the rapid adoption \\nof these technologies, especially as building blocks for other types of \\nautomation in high-stakes arenas like content moderation, employment \\nsoftware, and resource allocation.\\nIn this section, we offer recommendations for companies, researchers, and \\ngovernments to take into consideration as they build, study, and regulate \\nlarge language models, particularly in non-English language contexts.\\nA. Companies\\nTECHNOLOGY COMPANIES SHOULD DISCLOSE WHEN, \\nHOW, AND IN WHAT LANGUAGES THEY USE LARGE \\nLANGUAGE MODELS.\\nTo better understand the problems and challenges with deploying large \\nlanguage models in different languages, researchers and the public need \\nto know where to look. Companies that incorporate language models \\ninto their technical systems should always disclose how they are using \\nthem, which languages they are using them in, and what languages they \\nhave been trained on. Currently, the approach of many companies to AI \\ntransparency consists of trumpeting the capabilities of their AI systems \\nin blog posts and press releases, and, for a few larger firms, releasing \\nresearch versions of their language models that still differ from the ones \\nthey use in production. Despite publishing on AI and pushing the field \\nforward, technology companies tend to hold information about their \\nproduction AI systems, even basic information about what languages \\nthey are used in, close to the chest.\\nLost in Translation\\nCDT Research\\n32\\nAcademics and civil society have written extensively about how technology companies, \\nparticularly online service providers, could offer better transparency and accountability \\nfor their AI systems, including language models. The Santa Clara Principles, a set of \\nprinciples developed and revised by global civil society groups, provides examples of \\nthe types of disclosures companies can make about their content moderation policies \\nand processes (2021). Groups like BigScience also pave the way, exemplifying the type \\nof documentation other model-developers can publish about their content analysis \\nsystems, including model cards, transparency reports, and other avenues to disclose \\nmore information about the linguistic makeup of a model’s training data (e.g. what \\nlanguages it has trained on, how much data from each language, where those datasets \\ncome from). Better transparency creates opportunities for external actors to more \\nimmediately identify potential risks and impacts on users and for technology companies \\nto mitigate the potential dangers of deploying large language models in English and \\nnon-English contexts.\\nWHEN DEPLOYED, LARGE LANGUAGE MODELS SHOULD BE \\nACCOMPANIED BY ADEQUATE REMEDIAL CHANNELS AND \\nMECHANISMS THAT ENSURE INDIVIDUALS CAN APPEAL OUTCOMES \\nAND DECISIONS MADE BY THESE SYSTEMS.\\nBecause of the complexities of human speech and the error-prone nature of automated \\ntools, decision-making systems built on top of large language models should be used \\nwithin narrow remits and with adequate remedial channels for users encountering \\nthem. Those remedial channels and processes should have human reviewers with \\nthe same language proficiencies that their systems are deployed in. Language- and \\ncontext-specific remedial channels are particularly important for allowing users to \\nappeal decisions made by online services, especially when those decisions either restrict \\ntheir expression or access to information or fundamentally determine their access \\nto economic or social rights like the right to housing, education, and social security \\n(United Nations Human Rights Office of the High Commissioner, n.d.).\\nTechnology companies can also offer accountability at a system level, not just the \\nlevel of individual decisions. One way to do this is to conduct and publish human \\nrights impact assessments at the different phases of the language model’s life cycle \\n— development, testing, deployment, and evaluation (Prabhakaran et al., 2022). \\nPublishing human rights impact assessments will also aid in other actors’ decisions \\nwhen procuring these systems to conduct tasks in different domains and contexts. In \\nparticular, these human rights impact assessments should consider the disparate risks \\nto different language speakers in advance of a model being deployed in those languages. \\nOnline service providers can provide transparency by disclosing the systems and \\nlanguages they use large language models in. \\nLarge Language Models in Non-English Content Analysis\\n33\\nIII. Recommendations\\nCOMPANIES SHOULD INVEST IN IMPROVING LANGUAGE MODEL \\nPERFORMANCE IN INDIVIDUAL LANGUAGES BY BRINGING IN \\nLANGUAGE AND CONTEXT EXPERTS.\\nRecently, an arms race has begun between Google and Meta to see who can include \\nmore languages in their multilingual language model. Meta’s “No Language Left \\nBehind” initiative trained a model on over 200 languages (NLLB Team et al., 2022); \\nmonths later, Google one upped Meta with its “1,000 Languages Initiative” (Vincent, \\n2022). This race puts a premium on the number of languages the model trains on, \\nrather than how well it works in each language. In particular, it is unclear how these \\nmodels will handle the “curse of multilinguality,” where, as explained in II.B.3, the \\nmore languages a model trains on, the less it can capture the idiosyncrasies of each \\nlanguage. It is also unclear how these companies define a model “working” in any of \\nthese languages.\\nCompanies building large language models should not just focus on the number of \\nlanguages their model is trained on but the quality of its performance in each language. \\nIn part, that means better benchmarks, but benchmarks can only go so far. To evaluate \\nthe full range of potential applications and pitfalls that could come with applying a \\nlanguage model in a specific language context, it is necessary to involve language experts, \\ncivil society, local experts, heritage and language preservation advocates, linguists, and \\nhuman rights experts. These actors are crucial to ensuring that labeled training datasets \\nadequately capture the nuances and variations of a given language. Many organizations \\nare already doing this type of work. Uli is an example of this, where two India-based \\nnonprofit organizations — Tattle and Centre for Internet & Society — convened a \\nrange of gender, gender-based violence, communal violence, and other language experts \\nto annotate training datasets in Indian English, Tamil, and Hindi to build a tool capable \\nof parsing sentiment and toxicity on Twitter. Other researchers have also pointed to \\nusing annotators to label training datasets as a way to equip models with the ability to \\nparse variations in the speech of a certain language (Bergman & Diab, 2022; Nkemelu \\net al., 2022). \\nB. Researchers and Funders\\nRESEARCH FUNDERS SHOULD INVEST IN SPECIFIC NLP LANGUAGE \\nCOMMUNITIES TO KICKSTART THE VIRTUOUS CYCLE OF \\nDEVELOPMENT.\\nDeveloping NLP capabilities in any language is a cyclical process, and for high resource \\nlanguages — particularly English —\\xa0that cycle is virtuous. When a language has lots \\nof clean, human-annotated datasets, researchers and developers are better equipped \\nto build models and benchmarks to test models in that language. More models and \\nLost in Translation\\nCDT Research\\n34\\nbenchmarks lead to more publications, conferences, and real-world use cases. And \\nfinally, increased demand for research and software in a language drives demand for \\nmore datasets. For low resource languages, however, the virtuous cycle is hard to \\nkickstart. Without tools, annotators, and financial investment earmarked for different \\nlanguage communities, NLP researchers cannot create the datasets needed to build \\nmodels or benchmarks, and even if they could, they face difficulties publishing or \\ngetting attention for their work in popular journals and conferences. The most \\nprestigious NLP publications focus disproportionately on English; languages without \\ntheir own self-sustaining NLP communities end up to a handful of specialized outlets.\\nInvestments into non-English NLP should particularly focus on creating self-sustaining \\nscholarly NLP communities, and doing this requires investing in all levels at once. The \\ngroups that are best set up to properly allocate these investments are the language- and \\ngeography-specific NLP research communities that have cropped up over the years, \\nsuch as such as Masakhane, AmericasNLP, ARBML, and others who can convene \\npractitioners around common goals to advance the field (Alyafeai & Al-Shaibani, 2020; \\nAmericasNLP, 2022; Orife et al., 2020). These communities know what kind of data \\nsets should be built, which community actors are needed to properly vet them, and \\nwhat kind of competitions and conferences should be run to keep the virtuous cycles \\ngoing. One model for how this can work is exemplified by EVALITA, an event hosted \\nby the Italian Association for Computational Linguistics. In it, researchers first submit \\ndata sets for new language tasks, such as identifying misogyny or dating documents. \\nThen, researchers compete to train models to perform those tasks the best. Finally, \\nthose results get published, thus generating interest and attention toward Italian NLP \\nand ensuring researchers continue to build tools for the language (Basile et al., 2020).\\nPrivate companies can contribute not only by financially supporting these efforts \\nbut by sharing more of the non-English datasets they use to train their large language \\nmodels, both for transparency and to support research. Large tech companies have \\nalready shared the code for training many of their multilingual language models \\n— Meta’s XLM-R and Google’s mBERT are the subjects of most multilingual \\nmodel research in publication — and disclosed the data they train them on — \\nCommonCrawl, and\\xa0Wikipedia and BooksCorpus, respectively. However, the models \\nthat Google, Meta, OpenAI, and other large companies use in their products train on \\nother, proprietary, language data. Companies should share more of their training data, \\nboth for public accountability and to bolster research.\\nLarge language models have by and large been built by private companies, but private \\nincentives may be at odds with developing these models in safe and equitable ways. \\nGovernment investment into non-English large language model research could lead \\nto improvements in areas private companies may be underinvesting in (Mazzucato, \\n2014). DARPA’s late 2010’s LORELEI project, aimed at spurring research into low \\nLarge Language Models in Non-English Content Analysis\\n35\\nIII. Recommendations\\nresource languages to improve translation for humanitarian efforts, is a good first step, \\nbut further government incentives could help assure that NLP researchers invest in \\na broad range of approaches and languages, rather than focus disproportionately on \\nEnglish. BigScience’s BLOOM is a good example of how large language models can \\nbe developed in the open and with public support. The French government is one of \\nmany funders which has allowed BLOOM to remain open to inquiry by other NLP \\npractitioners. The multilingual language model was trained using ROOTs, a 1.6TB \\nmultilingual dataset that is clearly documented and available for NLP practitioners to \\nanalyze (Laurençon et al., 2022).\\nRESEARCHERS SHOULD FOCUS ON MEASURING AND ADDRESSING \\nTHE IMPACTS OF LARGE LANGUAGE MODELS.\\nTechnologists understand little about the internal logic of how large language models \\noperate and therefore have a difficult time predicting when they make mistakes, \\nwhat the effects of these mistakes will be, and how to fix them. Multilinguality only \\nexacerbates this problem. Better tools are needed to interrogate large language models, \\nparticularly multilingual language models, about why they make the decisions and \\nmistakes they do, and how to fix them.\\nIn particular, the increased use of multilingual language models has the potential to \\nhelp and harm language communities. Enabling greater digital participation amongst \\na language community raises something that researchers call the “Janus-face nature \\nof digital participation” (NLLB Team et al., 2022): it allows more to participate and \\nbenefit from the digital economy, however, it may also expose more people to the harms \\npresent online, often without their consultation and consent (Hao, 2022; Toyama, \\n2015). More research on the effects and externalities of the increased use of language \\nmodels and specifically multilingual language models must grapple with the impacts \\nthese tools have on different linguistic communities, linguistic preservation and \\ndiversity efforts, and access to opportunity for all. \\nDifferent actors have different roles to play here. Civil society has a role in documenting \\nthe impacts of these models and imagining what these “better” models should look like. \\nThere are many open questions around the types of problems that need automated \\nsolutions, what more representative datasets might look like, how to manage the tradeoffs \\nbetween languages, how large language models affect linguistic preservation efforts, and \\nwhat the rights implications are of using large language models, among other things. \\nAcademics and corporate researchers have a role in better defining the contexts and tasks \\nthese models hope to address, and developing quantitative and qualitative methods to \\nevaluate these desired normative values. And companies that deploy language models \\ncan provide researchers more transparency into how their models work, what data they \\nare trained on, and in what situations they use them so researchers can better tailor their \\nresearch to reflect what is happening in real-world systems.\\nLost in Translation\\nCDT Research\\n36\\nC. Governments\\nGOVERNMENTS SHOULD CAUTION AGAINST USING AUTOMATED \\nDECISION-MAKING SYSTEMS THAT RELY ON LARGE LANGUAGE \\nMODELS TO MAKE HIGH-STAKES DECISIONS.\\nMany governments have deployed or are considering deploying systems that use natural \\nlanguage processing technology as part of AI systems to make high-impact decisions, \\nsuch as determining immigration status or selecting judicial cases to try (Patel et al., \\n2020; Rionda & Mejia, 2021). Vendors who build these systems may soon follow the \\nlarger industry trend of incorporating large language models since they are relatively \\ncheap to build and easy to adapt as requirements change. However, as discussed \\nthroughout this paper, large language models are a relatively novel technology that has \\ntechnical limitations. These tools pose serious civil liberty concerns that are magnified \\nin non-English contexts and when used to make decisions that may affect a person’s \\nlivelihood. For instance, if a large language model is used as the basis of an algorithm \\nto evaluate affordable housing applications and the text that large language model was \\ntrained on exhibits anti-Muslim bias, the resulting affordable housing algorithm may \\ndisproportionately deny Muslims’ applications. Relying on large language models to \\nmake high-stakes decisions can have outsized, negative impacts on individuals’ lives, \\nimpeding safety and access to economic opportunities.\\nGovernments should therefore never rely solely on automated systems that incorporate \\nlarge language models to make high-risk decision-making areas, such as pretrial risk \\nassessment, allocation of social services, and immigration status. Policymakers should \\nconsider the impact on rights and access to services when procuring new tools and \\nvendors to build these systems and conduct and disclose any assessments conducted \\non these systems. They should also be cautious when adopting these systems for \\ninformation sharing services, such as chatbots about social services or that provide \\nhealthcare information, and test them extensively in every language in which they are \\ndeployed, and never use them to entirely replace human intermediaries.\\nGOVERNMENTS SHOULD NOT MANDATE OR INADVERTENTLY \\nREQUIRE BY LAW THE USE OF AUTOMATED CONTENT ANALYSIS \\nSYSTEMS TO DETECT OR REMOVE CONTENT IN ANY LANGUAGE.\\nGovernments around the world are increasingly pressuring online service providers to \\nlimit content they find to be inaccurate or harmful, such as misinformation related to \\nhealth care, or preemptively monitor online speech which may incite violence. Given \\nthe scale of content available on social media and other services, this has driven an \\ninterest amongst governments to mandate that online service providers use automated \\ncontent analysis systems to detect or remove content they deem as “illegal” or harmful \\nto their constituents.\\nLarge Language Models in Non-English Content Analysis\\n37\\nIII. Recommendations\\nThis is ill-advised. Mandating the use of automated content moderation technologies \\nor requiring companies to take down content in a limited time period (effectively \\nrequiring the use of automated technologies) opens the door for the overbroad removal \\nof speech. Large language models, especially in non-English language contexts, are not \\na magical technology that can perfectly distinguish between “good” and “bad” speech. \\nAt best, they are an imprecise technology that fails to understand the context of speech \\n— for instance, when an individual uses a slur versus when a journalist documents \\nthe use of a slur by that individual. At worst, they are tools that can be appropriated \\nby governments to squash dissent and freedom of expression. Efforts to persuade tech \\ncompanies to improve their automated systems, clarify their policies, introduce more \\naccountability, and promote parity between languages are all welcome, but requiring \\ncompanies to adopt certain technologies is not an effective way to achieve those ends.\\nINTERNATIONAL AND MULTILATERAL STANDARDS BODIES, \\nREGULATORY AGENCIES, AND OTHERS SHOULD CONVENE \\nMULTI-STAKEHOLDER DISCUSSIONS ABOUT STANDARDS AND \\nGUARDRAILS FOR THE DEVELOPMENT AND USE OF LARGE \\nLANGUAGE MODELS.\\nThe norms around when and how multilingual language models should be deployed \\nare very much in flux. Those norms so far have mostly been established implicitly by \\ntechnology companies in the ways they build and deploy these models, but trends in \\nthese norms may be at odds with the public interest. For instance, OpenAI revealed \\nsome information about the training data they used for GPT-3 but almost nothing \\nabout GPT-4; Open AI co-founder Ilya Sutskever described having shared information \\nabout GPT-3’s training data as “just not wise” and something the company would \\nunlikely do again (Vincent, 2023b).\\nCompanies should not have a monopoly on the norms around language models. \\nGovernmental and nongovernmental\\xa0convening bodies need to organize and push back \\nto establish counter-norms that better serve the public’s interests. This field is early on \\nenough that these bodies should discuss what positive outcomes even look like. Users \\naffected by the deployment of large language models need to be at the table for those \\nconversations. Government agencies and multilateral organizations (e.g. the Internet \\nEngineering Task Force, United Nations) can play a coordinating role to get together \\nthe relevant stakeholders to come up with such standards.\\nThere are also larger questions to reckon with when it comes to the use of large \\nlanguage models in non-English contexts. At once, companies are increasingly \\ndeploying multilingual language models to bridge the gap between the functionality in \\nEnglish and other languages across a myriad of tasks, such as harmful content detection, \\nsentiment analysis, and content scanning. However, as we show in this paper, these \\nmultilingual systems are relatively new and perform inconsistently across languages. \\nLost in Translation\\nCDT Research\\n38\\nIf deployed prematurely and without guardrails, these models pose real risks to \\nindividuals around the world and in particular their ability to express themselves freely. \\nThese risks have the potential to compound existing challenges in the information \\nenvironment for individuals in Western democracies where there are real vacuums of \\navailable information in languages other than English and in countries in the Global \\nSouth where there are already real threats to the free expression and exchange of \\ninformation posed by majoritarian and institutional powers (Golebiewski & boyd, \\n2018). Alternatively, companies may decide to only roll out systems that have been \\nfine-tuned for English and wait until there is enough data and tooling available for non-\\nEnglish language tools — something that will take an enormous amount of financial \\ninvestment, time, effort, and rare consensus — further entrenching the digital divide \\nand Anglocentrism present online. Both scenarios are lose-lose for all speakers on the \\nweb. This is a wicked problem and the current incentives are at play to build bigger \\nmodels, and with more languages. Multi-stakeholder bodies are much better positioned \\nthan companies to determine when the risks associated with building larger, more \\nmultilingual language models are worth taking.\\nLarge Language Models in Non-English Content Analysis\\n39\\nWorks Cited\\nAbid, A., Farooqi, M., & Zou, J. (2021). Large language models associate Muslims with violence. Nature Machine \\nIntelligence, 3(6), Article 6. [perma.cc/HK4B-3AAQ]\\nACL. (2021, August 3). ACL 2022 Theme Track: “Language Diversity: from Low-Resource to Endangered \\nLanguages.” ACL. [perma.cc/F2YW-QZBP]\\nACL Rolling Review Dashboard. (2022). Papers Mentioning >0 Languages. [perma.cc/EQU9-5CWQ]\\nAgerri, R., Vicente, I. S., Campos, J. A., Barrena, A., Saralegi, X., Soroa, A., & Agirre, E. (2020). Give your Text \\nRepresentation Models some Love: The Case for Basque. Proceedings of the 12th Conference on Language \\nResources and Evaluation, 4781–4788. [perma.cc/R2DA-GGQZ]\\nAlyafeai, Z., & Al-Shaibani, M. (2020). ARBML: Democratizing Arabic Natural Language Processing Tools. \\nProceedings of Second Workshop for NLP Open Source Software (NLP-OSS), 8–13. [perma.cc/4TFY-E9EJ]\\nAmer, M. (2022, July 13). Large Language Models and Where to Use Them: Part 2. Cohere. [perma.cc/CRT5-\\nHDX8]\\nAmericasNLP. (2022, December 7). Second Workshop on NLP for Indigenous Languages of the Americas \\n(AmericasNLP). [perma.cc/SC88-9WGF]\\nAmrute, S., Singh, R., & Guzmán, R. L. (2022). A Primer on AI in/from the Majority World. Data & Society. \\n[perma.cc/SR8B-J2L9]\\nAntoun, W., Baly, F., & Hajj, H. (2020). AraBERT: Transformer-based Model for Arabic Language \\nUnderstanding. Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a \\nShared Task on Offensive Language Detection, 9–15. [perma.cc/X5VJ-JKXQ]\\nArtetxe, M., Labaka, G., & Agirre, E. (2020). Translation Artifacts in Cross-lingual Transfer Learning. Proceedings \\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 7674–7684. \\n[perma.cc/MZY5-DL83]\\nArtetxe, M., Ruder, S., & Yogatama, D. (2020). On the Cross-lingual Transferability of Monolingual \\nRepresentations. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, \\n4623–4637. [perma.cc/7WMN-5QPR]\\nArtetxe, M., & Schwenk, H. (2019). Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual \\nTransfer and Beyond. Transactions of the Association for Computational Linguistics, 7, 597–610. [perma.cc/\\nLB6R-GH9K]\\nArticle 19. (2022). Bridging the Gap: Local voices in content moderation. Bosnia and Herzegovina. [perma.cc/ASU5-\\nST4N]\\nAvaaz. (2019). Megaphone for Hate: Disinformation and Hate Speech on Facebook During Assam’s Citizenship \\nCount. Avaaz. [perma.cc/5MXS-7P7N]\\nLost in Translation\\n40\\nLost in Translation\\nCDT Research\\nBasile, V., Maro, M. D., Croce, D., & Passaro, L. (2020, December 17). EVALITA 2020: Overview of the 7th \\nEvaluation Campaign of Natural Language Processing and Speech Tools for Italian. Seventh Evaluation \\nCampaign of Natural Language Processing and Speech Tools for Italian, Online. [perma.cc/76EK-EJQ8]\\nBelloni, M. (2021, December 8). Multilingual message content moderation at scale. Bumble Tech. [perma.cc/\\nRL2A-L2BD]\\nBender, E. (2019, September 15). The #BenderRule: On Naming the Languages We Study and Why It Matters. \\nThe Gradient. [perma.cc/J3ZM-A5UP]\\nBender, E., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can \\nLanguage Models Be Too Big? 🦜. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and \\nTransparency, 610–623. [perma.cc/3KLC-TBUY]\\nBender, E., & Koller, A. (2020). Climbing towards NLU: On Meaning, Form, and Understanding in the Age of \\nData. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5185–5198. \\n[perma.cc/TN3W-5NTC]\\nBergman, A., & Diab, M. (2022). Towards Responsible Natural Language Annotation for the Varieties of Arabic. \\nFindings of the Association for Computational Linguistics: ACL 2022, 364–371. [perma.cc/Q37M-8F2Y]\\nBigScience Workshop, Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. \\nS., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., \\nSagot, B., Muennighoff, N., … Wolf, T. (2023). BLOOM: A 176B-Parameter Open-Access Multilingual \\nLanguage Model (arXiv:2211.05100). arXiv. [perma.cc/2K4Z-F5U7]\\nBirhane, A., Kalluri, P., Card, D., Agnew, W., Dotan, R., & Bao, M. (2022). The Values Encoded in Machine \\nLearning Research. 2022 ACM Conference on Fairness, Accountability, and Transparency, 173–184. \\n[perma.cc/9GNB-JHQ5]\\nBirhane, A., & Prabhu, V. U. (2021). Large image datasets: A pyrrhic win for computer vision? 2021 IEEE Winter \\nConference on Applications of Computer Vision, 1536–1546. [perma.cc/Q8LP-THYK]\\nBizzoni, Y., Juzek, T. S., España-Bonet, C., Dutta Chowdhury, K., van Genabith, J., & Teich, E. (2020). How \\nHuman is Machine Translationese? Comparing Human and Machine Translations of Text and Speech. \\nProceedings of the 17th International Conference on Spoken Language Translation, 280–290. [perma.\\ncc/4DTZ-DVKC]\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, \\nA., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., \\nDavis, J. Q., Demszky, D., … Liang, P. (2021). On the Opportunities and Risks of Foundation Models. \\nStanford Center for Research on Foundation Models. [perma.cc/3TKJ-UM2F]\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., \\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., \\nWu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. Advances in Neural \\nInformation Processing Systems, 33, 1877–1901. [perma.cc/7EES-WDAB]\\nCahyawijaya, S., Lovenia, H., Aji, A. F., Winata, G. I., Wilie, B., Mahendra, R., Wibisono, C., Romadhony, A., \\nVincentio, K., Koto, F., Santoso, J., Moeljadi, D., Wirawan, C., Hudi, F., Parmonangan, I. H., Alfina, \\nI., Wicaksono, M. S., Putra, I. F., Rahmadani, S., … Purwarianti, A. (2022). NusaCrowd: Open Source \\nInitiative for Indonesian NLP Resources (arXiv:2212.09648). arXiv. [perma.cc/UQ3Y-4LKW]\\n41\\nLarge Language Models in Non-English Content Analysis\\nWorks Cited\\nCallahan, E. S., & Herring, S. C. (2011). Cultural bias in Wikipedia content on famous persons. Journal of the \\nAmerican Society for Information Science and Technology, 62(10), 1899–1915. [perma.cc/2S8K-YEJK]\\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., & Zhang, C. (2023, February 1). Quantifying \\nMemorization Across Neural Language Models. The Eleventh International Conference on Learning \\nRepresentations. [perma.cc/678U-9PAQ]\\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., \\nErlingsson, U., Oprea, A., & Raffel, C. (2021). Extracting Training Data from Large Language Models \\n(arXiv:2012.07805). arXiv. [perma.cc/58MA-VWRZ]\\nCaswell, I., Breiner, T., van Esch, D., & Bapna, A. (2020). Language ID in the Wild: Unexpected Challenges on \\nthe Path to a Thousand-Language Web Text Corpus. Proceedings of the 28th International Conference on \\nComputational Linguistics, 6588–6608. [perma.cc/8RFD-DTUK]\\nChi, E. A., Hewitt, J., & Manning, C. D. (2020). Finding Universal Grammatical Relations in Multilingual BERT. \\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5564–5577. \\n[perma.cc/8LNR-VNY9]\\nChoi, H., Kim, J., Joe, S., Min, S., & Gwon, Y. (2021). Analyzing Zero-shot Cross-lingual Transfer in Supervised \\nNLP Tasks (arXiv:2101.10649). arXiv. [perma.cc/NEB9-8THZ]\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., \\nGehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., \\nPrabhakaran, V., … Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways. Google Research. \\n[perma.cc/NZ7N-6GPB]\\nChristian, J. (2018, July 20). Why Is Google Translate Spitting Out Sinister Religious Prophecies? Vice. [perma.\\ncc/8YQU-NUFM]\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., \\nZettlemoyer, L., & Stoyanov, V. (2020). Unsupervised Cross-lingual Representation Learning at Scale. \\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 8440–8451. \\n[perma.cc/2MP6-9W3J]\\nConneau, A., & Lample, G. (2019). Cross-lingual Language Model Pretraining. Advances in Neural Information \\nProcessing Systems, 32. [perma.cc/N7EE-JM83]\\nConneau, A., Wu, S., Li, H., Zettlemoyer, L., & Stoyanov, V. (2020). Emerging Cross-lingual Structure in \\nPretrained Language Models. Proceedings of the 58th Annual Meeting of the Association for Computational \\nLinguistics, 6022–6034. [perma.cc/3NHR-G7Y4]\\nCorradi, A. (2017, April 25). The Linguistic Colonialism of English. Brown Political Review. [perma.cc/5M3M-\\n9EMN]\\nCorvey, W. (2014). Low Resource Languages for Emergent Incidents. Defense Advanced Research Projects Agency. \\n[perma.cc/4FDR-M3YC]\\nCrawford, K. (2021). Atlas of AI: Power, politics, and the planetary costs of artificial intelligence. Yale University \\nPress.\\n42\\nLost in Translation\\nCDT Research\\nde Varda, A. G., & Marelli, M. (2023). Data-driven Cross-lingual Syntax: An Agreement Study with Massively \\nMultilingual Models. Computational Linguistics, 1–39. [perma.cc/7LQP-EEBQ]\\nde Vries, W., van Cranenburgh, A., Bisazza, A., Caselli, T., van Noord, G., & Nissim, M. (2019). BERTje: A Dutch \\nBERT Model (arXiv:1912.09582). arXiv. [perma.cc/MGU3-WPXR]\\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional \\nTransformers for Language Understanding. Proceedings of the 2019 Conference of the North American \\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long \\nand Short Papers), 4171–4186. [perma.cc/E46R-UYDE]\\nDodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., & Gardner, M. (2021). \\nDocumenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. Proceedings \\nof the 2021 Conference on Empirical Methods in Natural Language Processing, 1286–1305. [perma.\\ncc/3GC6-UEWJ]\\nDuarte, N., Llansó, E., & Loup, A. C. (2017). Mixed Messages? The Limits of Automated Social Media Content \\nAnalysis. Center for Democracy & Technology. [perma.cc/9BRH-5ZZN]\\nDulhanty, C., Deglint, J. L., Daya, I. B., & Wong, A. (2019, November 26). Taking a Stance on Fake News: Towards \\nAutomatic Disinformation Assessment via Deep Bidirectional Transformer Language Models for Stance \\nDetection. NeurIPS 2019, Vancouver. [perma.cc/P5JD-5AD9]\\nEbers, M., Poncibò, C., & Zou, M. (Eds.). (2022). Contracting and Contract Law in the Age of Artificial \\nIntelligence. Hart Publishing. [perma.cc/G4XR-VYNL]\\nEronen, J., Ptaszynski, M., & Masui, F. (2023). Zero-shot cross-lingual transfer language selection using linguistic \\nsimilarity. Information Processing & Management, 60(3), 103250. [perma.cc/S78N-C9MR]\\nEthnologue. (2023a). Assamese. Ethnologue, Languages of the World. [perma.cc/BE78-H3PN]\\nEthnologue. (2023b). Statistics. Ethnologue, Languages of the World. [perma.cc/H27U-44TK]\\nGanguli, D., Hernandez, D., Lovitt, L., DasSarma, N., Henighan, T., Jones, A., Joseph, N., Kernion, J., Mann, \\nB., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Elhage, N., Showk, S. E., Fort, S., Hatfield-Dodds, \\nZ., Johnston, S., … Clark, J. (2022). Predictability and Surprise in Large Generative Models. 2022 ACM \\nConference on Fairness, Accountability, and Transparency, 1747–1764. [perma.cc/C8YH-6LMA]\\nGolebiewski, M., & boyd, danah. (2018). Data Voids: Where Missing Data Can Easily Be Exploited. Data & \\nSociety. [perma.cc/HE5A-7QTJ]\\nGóngora, S., Giossa, N., & Chiruzzo, L. (2021). Experiments on a Guarani Corpus of News and Social Media. \\nProceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the \\nAmericas, 153–158. [perma.cc/N6S5-4PGN]\\nGrant-Chapman, H., Laird, E., & Venzke, C. (2021). Student Activity Monitoring Software Research Insights and \\nRecommendations. Center for Democracy & Technology. [perma.cc/FY8G-WC2P]\\nGröndahl, T., Pajola, L., Juuti, M., Conti, M., & Asokan, N. (2018). All You Need is “Love”: Evading Hate Speech \\nDetection. Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security, 2–12. [perma.cc/\\nT6P5-FRX5]\\n43\\nLarge Language Models in Non-English Content Analysis\\nWorks Cited\\nHao, K. (2022, April 22). A new vision of artificial intelligence for the people. MIT Technology Review. [perma.\\ncc/54U3-KU5C]\\nHeikkilä, M. (2022, November 14). We’re getting a better idea of AI’s true carbon footprint. MIT Technology \\nReview. [perma.cc/8PWZ-ESJK]\\nHutchinson, B., Prabhakaran, V., Denton, E., Webster, K., Zhong, Y., & Denuyl, S. (2020). Social Biases in NLP \\nModels as Barriers for Persons with Disabilities. Proceedings of the 58th Annual Meeting of the Association \\nfor Computational Linguistics, 5491–5501. [perma.cc/8FGR-P3FA]\\nIzsak, P., Berchansky, M., & Levy, O. (2021). How to Train BERT with an Academic Budget. Proceedings of the \\n2021 Conference on Empirical Methods in Natural Language Processing, 10644–10652. [perma.cc/8MPG-\\nW2QE]\\nJoshi, P., Santy, S., Budhiraja, A., Bali, K., & Choudhury, M. (2020). The State and Fate of Linguistic Diversity and \\nInclusion in the NLP World. Proceedings of the 58th Annual Meeting of the Association for Computational \\nLinguistics, 6282–6293. [perma.cc/82HQ-EH65]\\nKaack, L. H., Donti, P. L., Strubell, E., Kamiya, G., Creutzig, F., & Rolnick, D. (2022). Aligning artificial \\nintelligence with climate change mitigation. Nature Climate Change, 12(6), Article 6. [perma.cc/7C4S-\\nX2LH]\\nKhan, M., & Hanna, A. (2023). The Subjects and Stages of AI Dataset Development: A Framework for Dataset \\nAccountability. Ohio State Technology Law Journal, 19. [perma.cc/XLG3-AP2J]\\nKinchin, N., & Mougouei, D. (2022). What Can Artificial Intelligence Do for Refugee Status Determination? A \\nProposal for Removing Subjective Fear. International Journal of Refugee Law. [perma.cc/3KER-DZ5R]\\nKoehn, P., & Knowles, R. (2017). Six Challenges for Neural Machine Translation. Proceedings of the First \\nWorkshop on Neural Machine Translation, 28–39. [perma.cc/9WSQ-HQJY]\\nKornai, A. (2013). Digital Language Death. PLOS ONE, 8(10), e77056. [perma.cc/MMZ8-C9VH]\\nKreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh, N., Tapo, A., Subramani, N., Sokolov, \\nA., Sikasote, C., Setyawan, M., Sarin, S., Samb, S., Sagot, B., Rivera, C., Rios, A., Papadimitriou, I., Osei, \\nS., Suarez, P. O., … Adeyemi, M. (2022). Quality at a Glance: An Audit of Web-Crawled Multilingual \\nDatasets. Transactions of the Association for Computational Linguistics, 10, 50–72. [perma.cc/YZ7B-\\nQ7PN]\\nKublik, V. (n.d.). EU/US Copyright Law and Implications on ML Training Data. Valohai. [perma.cc/LD3Z-\\nRVW7]\\nKupfer, M., & Muyumba, J. (2022). Language & Coloniality: Non-Dominant Languages in the Digital Landscape. \\nPollicy. [perma.cc/PM8N-Y9YW]\\nLaurençon, H., Saulnier, L., Wang, T., Akiki, C., Moral, A. V. del, Scao, T. L., Werra, L. V., Mou, C., Ponferrada, \\nE. G., Nguyen, H., Frohberg, J., Šaško, M., Lhoest, Q., McMillan-Major, A., Dupont, G., Biderman, \\nS., Rogers, A., Allal, L. B., Toni, F. D., … Jernite, Y. (2022, October 31). The BigScience ROOTS Corpus: \\nA 1.6TB Composite Multilingual Dataset. Thirty-sixth Conference on Neural Information Processing \\nSystems Datasets and Benchmarks Track. [perma.cc/QS7B-YNYU]\\n44\\nLost in Translation\\nCDT Research\\nLauscher, A., Ravishankar, V., Vulić, I., & Glavaš, G. (2020). From Zero to Hero: On the Limitations of Zero-\\nShot Language Transfer with Multilingual Transformers. Proceedings of the 2020 Conference on Empirical \\nMethods in Natural Language Processing (EMNLP), 4483–4499. [perma.cc/ZJ3R-95JM]\\nLees, A., Sorensen, J., & Kivlichan, I. (2020). Jigsaw @ AMI and HaSpeeDe2: Fine-Tuning a Pre-Trained \\nComment-Domain BERT Model. In V. Basile, D. Croce, M. Maro, & L. C. Passaro (Eds.), EVALITA \\nEvaluation of NLP and Speech Tools for Italian—December 17th, 2020 (pp. 40–47). Accademia University \\nPress. [perma.cc/9D4M-RSCL]\\nLees, A., Tran, V. Q., Tay, Y., Sorensen, J., Gupta, J., Metzler, D., & Vasserman, L. (2022). A New Generation \\nof Perspective API: Efficient Multilingual Character-level Transformers. Proceedings of the 28th ACM \\nSIGKDD Conference on Knowledge Discovery and Data Mining, 3197–3207. [perma.cc/5K82-WG8J]\\nLibovický, J., Rosa, R., & Fraser, A. (2019). How Language-Neutral is Multilingual BERT? (arXiv:1911.03310). \\narXiv. [perma.cc/96RW-WXBL]\\nLin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., \\nPasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V., O’Horo, B., Wang, J., Zettlemoyer, L., Kozareva, Z., \\nDiab, M., … Li, X. (2022). Few-shot Learning with Multilingual Generative Language Models. Proceedings \\nof the 2022 Conference on Empirical Methods in Natural Language Processing, 9019–9052. [perma.\\ncc/5QY9-97G5]\\nLokhov, I. (2021, January 28). Why are there so many Wikipedia articles in Swedish and Cebuano? Datawrapper \\nBlog. [perma.cc/WDL2-TF53]\\nLuccioni, A., & Viviano, J. (2021). What’s in the Box? An Analysis of Undesirable Content in the Common Crawl \\nCorpus. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the \\n11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), 182–189. \\n[perma.cc/2QQU-NRPB]\\nLunden, I. (2023, March 14). Nabla, a digital health startup, launches Copilot, using GPT-3 to turn patient \\nconversations into action. TechCrunch. [perma.cc/MK55-SV54]\\nMartin, G., Mswahili, M. E., Jeong, Y.-S., & Woo, J. (2022). SwahBERT: Language Model of Swahili. Proceedings \\nof the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: \\nHuman Language Technologies, 303–313. [perma.cc/3ZP6-V6AJ]\\nMartin, L., Muller, B., Suárez, P. J. O., Dupont, Y., Romary, L., de la Clergerie, É. V., Seddah, D., & Sagot, B. \\n(2020). CamemBERT: A Tasty French Language Model. Proceedings of the 58th Annual Meeting of the \\nAssociation for Computational Linguistics, 7203–7219. [perma.cc/76EU-4LTM]\\nMasakhane. (n.d.). Masakhane. Retrieved December 21, 2022. [perma.cc/A7SA-ALPM]\\nMaundu, C. (2023, February 21). How language denies people access to public information. Nation. [perma.\\ncc/8C4B-JS3Y]\\nMazzucato, M. (2014). The entrepreneurial state: Debunking public vs. private sector myths (Revised edition). \\nAnthem Press.\\nMeta AI. (2019, November 7). XLM-R: State-of-the-art cross-lingual understanding through self-supervision. \\nMeta AI. [perma.cc/J55N-4MV5]\\n45\\nLarge Language Models in Non-English Content Analysis\\nWorks Cited\\nMicallef, K., Gatt, A., Tanti, M., van der Plas, L., & Borg, C. (2022). Pre-training Data Quality and Quantity for a \\nLow-Resource Language: New Corpus and BERT Models for Maltese. Proceedings of the Third Workshop \\non Deep Learning for Low-Resource Natural Language Processing, 90–101. [perma.cc/QY8V-9Q6H]\\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013, September 6). Efficient Estimation of Word Representations \\nin Vector Space. International Conference on Learning Representations. [perma.cc/T869-PDX4]\\nMuller, B., Anastasopoulos, A., Sagot, B., & Seddah, D. (2021). When Being Unseen from mBERT is just the \\nBeginning: Handling New Languages With Multilingual Language Models. Proceedings of the 2021 \\nConference of the North American Chapter of the Association for Computational Linguistics: Human \\nLanguage Technologies, 448–462. [perma.cc/J5MH-QDW3]\\nNadkarni, P. M., Ohno-Machado, L., & Chapman, W. W. (2011). Natural language processing: An introduction. \\nJournal of the American Medical Informatics Association\\u202f: JAMIA, 18(5), 544–551. [perma.cc/72PK-\\nUGK9]\\nNekoto, W., Marivate, V., Matsila, T., Fasubaa, T., Fagbohungbe, T., Akinola, S. O., Muhammad, S., Kabongo \\nKabenamualu, S., Osei, S., Sackey, F., Niyongabo, R. A., Macharm, R., Ogayo, P., Ahia, O., Berhe, M. \\nM., Adeyemi, M., Mokgesi-Selinga, M., Okegbemi, L., Martinus, L., … Bashir, A. (2020). Participatory \\nResearch for Low-resourced Machine Translation: A Case Study in African Languages. Findings of the \\nAssociation for Computational Linguistics: EMNLP 2020, 2144–2160. [perma.cc/5BVM-LUMM]\\nNguer, E. M., Lo, A., Dione, C. M. B., Ba, S. O., & Lo, M. (2020). SENCORPUS: A French-Wolof Parallel \\nCorpus. Proceedings of the Twelfth Language Resources and Evaluation Conference, 2803–2811. [perma.cc/\\nNBE7-QCZW]\\nNguyen, T. (2020, November 27). Why fake news is so hard to combat in Asian American communities. Vox. \\n[perma.cc/45GF-UUEC]\\nNicholas, G. (2020). Explaining Algorithmic Decisions. Georgetown Law Technology Review, 4(711), 20. [perma.\\ncc/UD7D-HF6F]\\nNicholas, G. (2022). Shedding Light on Shadowbanning. Center for Democracy & Technology. [perma.cc/D2TS-\\nY92D]\\nNkemelu, D., Shah, H., Essa, I., & Best, M. L. (2023). Tackling Hate Speech in Low-resource Languages with \\nContext Experts. International Conference on Information & Communication Technologies and \\nDevelopment, Washington, USA. [perma.cc/5QK7-GTMR]\\nNLLB Team, Costa-jussà, M. R., Cross, J., Çelebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, \\nJ., Licht, D., Maillard, J., Sun, A., Wang, S., Wenzek, G., Youngblood, A., Akula, B., Barrault, L., Gonzalez, \\nG. M., Hansanti, P., … Wang, J. (2022). No Language Left Behind: Scaling Human-Centered Machine \\nTranslation (arXiv:2207.04672). arXiv. [perma.cc/LZH5-DMUA]\\nOkerlund, J., Klasky, E., Middha, A., Kim, S., Rosenfeld, H., Kleinman, M., & Parthasarathy, S. (2022). What’s \\nin the Chatterbox? Large Language Models, Why They Matter, and What We Should Do About Them. \\nUniversity of Michigan. [perma.cc/8SXE-RSYE]\\nOpenAI. (2023). GPT-4 Technical Report (arXiv:2303.08774). arXiv. [perma.cc/6ACB-LZYC]\\n46\\nLost in Translation\\nCDT Research\\nOrife, I., Kreutzer, J., Sibanda, B., Whitenack, D., Siminyu, K., Martinus, L., Ali, J. T., Abbott, J., Marivate, V., \\nKabongo, S., Meressa, M., Murhabazi, E., Ahia, O., van Biljon, E., Ramkilowan, A., Akinfaderin, A., \\nÖktem, A., Akin, W., Kioko, G., … Bashir, A. (2020). Masakhane—Machine Translation For Africa \\n(arXiv:2003.11529). arXiv. [perma.cc/84Z4-S7AZ]\\nPatel, F., Levinson-Waldman, R., Koreh, R., & DenUyl, S. (2020). Social Media Monitoring. Brennan Center for \\nJustice. [perma.cc/N5LF-ZKP2]\\nPhillipson, R. (1992). Linguistic Imperialism. Oxford University Press.\\nPires, T., Schlinger, E., & Garrette, D. (2019). How Multilingual is Multilingual BERT? Proceedings of the 57th \\nAnnual Meeting of the Association for Computational Linguistics, 4996–5001. [perma.cc/4DPF-LWWX]\\nPng, M.-T. (2022). At the Tensions of South and North: Critical Roles of Global South Stakeholders in AI \\nGovernance. 2022 ACM Conference on Fairness, Accountability, and Transparency, 1434–1445. [perma.cc/\\nZ7HD-3T4A]\\nPolignano, M., Basile, P., Degemmis, M., Semeraro, G., & Basile, V. (2019). AlBERTo: Italian BERT Language \\nUnderstanding Model for NLP Challenging Tasks Based on Tweets. Sixth Italian Conference on \\nComputational Linguistics, Bari, Italy. [perma.cc/RBY9-4JHJ]\\nPrabhakaran, V., Mitchell, M., Gebru, T., & Gabriel, I. (2022). A Human Rights-Based Approach to Responsible AI \\n(arXiv:2210.02667). arXiv. [perma.cc/R97H-WQSK]\\nPrates, M., Avelar, P., & Lamb, L. (2020). Assessing gender bias in machine translation: A case study with Google \\nTranslate. Neural Computing and Applications, 32. [perma.cc/CGK2-NMU2]\\nPym, A., Ayvazyan, N., & Prioleau, J. M. (2022). Should raw machine translation be used for public-health \\ninformation? Suggestions for a multilingual communication policy in Catalonia. Just. Journal of Language \\nRights & Minorities, Revista de Drets Lingüístics i Minories, 1(1–2), 71–99. [perma.cc/HSA8-TB3F]\\nRaji, D., Denton, E., Bender, E. M., Hanna, A., & Paullada, A. (2021). AI and the Everything in the Whole \\nWide World Benchmark. Proceedings of the Neural Information Processing Systems Track on Datasets and \\nBenchmarks, 1. [perma.cc/EX84-X9BQ]\\nRazeghi, Y., Logan IV, R. L., Gardner, M., & Singh, S. (2022). Impact of Pretraining Term Frequencies on Few-\\nShot Numerical Reasoning. Findings of the Association for Computational Linguistics: EMNLP 2022, \\n840–854. [perma.cc/SMG9-BSKV]\\nReid, M., & Artetxe, M. (2022). On the Role of Parallel Data in Cross-lingual Transfer Learning \\n(arXiv:2212.10173). arXiv. [perma.cc/83GW-CVXX]\\nRichter, F. (n.d.). English Is the Internet’s Universal Language. Statista Infographics. Retrieved December 14, \\n2022, from [perma.cc/WW7B-7X37]\\nRionda, V. P. S., & Mejia, J. C. U. (2021). PretorIA y la automatización del procesamiento de causas de derechos \\nhumanos. Derechos Digitales and Dejusticia. [perma.cc/65MQ-X484]\\nRowe, J. (2022, March 2). Marginalised languages and the content moderation challenge. Global Partners Digital. \\n[perma.cc/GU4K-5HBE]\\n47\\nLarge Language Models in Non-English Content Analysis\\nWorks Cited\\nRuder, S., Constant, N., Botha, J., Siddhant, A., Firat, O., Fu, J., Liu, P., Hu, J., Garrette, D., Neubig, G., & \\nJohnson, M. (2021). XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation. \\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 10215–10245. \\n[perma.cc/W4TJ-SGTB]\\nSanta Clara Principles. (2021). Santa Clara Principles on Transparency and Accountability in Content Moderation. \\nSanta Clara Principles. [perma.cc/T623-AVW6]\\nSanty, S., Kummerfeld, J., & Rubio, H. (2023). Languages mentioned in Paper Abstracts. ACL Rolling Review. \\n[perma.cc/EQU9-5CWQ]\\nSavoldi, B., Gaido, M., Bentivogli, L., Negri, M., & Turchi, M. (2021). Gender Bias in Machine Translation. \\nTransactions of the Association for Computational Linguistics, 9, 845–874. [perma.cc/9K3F-5VBZ]\\nSchwenk, H. (2019, January 22). LASER natural language processing toolkit—Engineering at Meta. Meta AI. \\n[perma.cc/46JG-AZ4T]\\nSengupta, P. B., Claudia Pozo, Anasuya. (2022, March 31). Does the internet speak your language? Launching the \\nfirst-ever State of the Internet’s Languages report. Whose Knowledge? [https://perma.cc/9KCX-M863]\\nSharir, O., Peleg, B., & Shoham, Y. (2020). The Cost of Training NLP Models: A Concise Overview \\n(arXiv:2004.08900). arXiv. [perma.cc/8KVV-C6P2]\\nShenkman, C., Thakur, D., & Llansó, E. (2021). Do You See What I See? Capabilities and Limits of Automated \\nMultimedia Content Analysis. Center for Democracy & Technology. [perma.cc/W85T-HQQF]\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., \\nGarriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. \\nW., Safaya, A., Tazarv, A., … Wu, Z. (2022). Beyond the Imitation Game: Quantifying and extrapolating the \\ncapabilities of language models (arXiv:2206.04615). arXiv. [perma.cc/278S-ZJV9]\\nStadthagen-Gonzalez, H., Imbault, C., Pérez Sánchez, M. A., & Brysbaert, M. (2017). Norms of valence and \\narousal for 14,031 Spanish words. Behavior Research Methods, 49(1), 111–123. [perma.cc/7FWX-Z3JD]\\nStrubell, E., Ganesh, A., & McCallum, A. (2019). Energy and Policy Considerations for Deep Learning in NLP. \\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3645–3650. \\n[perma.cc/9P4Y-J4HT]\\nTalat, Z., Névéol, A., Biderman, S., Clinciu, M., Dey, M., Longpre, S., Luccioni, S., Masoud, M., Mitchell, M., \\nRadev, D., Sharma, S., Subramonian, A., Tae, J., Tan, S., Tunuguntla, D., & Wal, O. van der. (2022). You \\nreap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings. Proceedings of \\nBigScience Episode #5, 26–41. [perma.cc/3ECR-4E7U]\\nTattle. (n.d.). Uli. [perma.cc/4AB2-D4GX]\\nTay, Y., Tran, V. Q., Ruder, S., Gupta, J., Chung, H. W., Bahri, D., Qin, Z., Baumgartner, S., Yu, C., & Metzler, D. \\n(2022, February 23). Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. \\nInternational Conference on Learning Representations 2022. [perma.cc/YRL4-E7DT]\\nTeich, E. (2003). Cross-Linguistic Variation in System and Text: A Methodology for the Investigation of \\nTranslations and Comparable Texts. In Cross-Linguistic Variation in System and Text. De Gruyter Mouton. \\n[perma.cc/L8A8-RH8B]\\n48\\nLost in Translation\\nCDT Research\\nTorbati, Y. (2019, September 26). Google Says Google Translate Can’t Replace Human Translators. Immigration \\nOfficials Have Used It to Vet Refugees. ProPublica. [perma.cc/ZUN6-LHA5]\\nToyama, K. (2015). Geek heresy: Rescuing social change from the cult of technology. PublicAffairs.\\nTsvetkov, Y., Sitaram, S., Faruqui, M., Lample, G., Littell, P., Mortensen, D., Black, A. W., Levin, L., & Dyer, \\nC. (2016). Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation \\nLearning. Proceedings of the 2016 Conference of the North American Chapter of the Association for \\nComputational Linguistics: Human Language Technologies, 1357–1366. [perma.cc/4RES-KFNM]\\nUnited Nations Human Rights Office of the High Commissioner. (n.d.). \\u200bEconomic, social and cultural rights. \\nOHCHR. [perma.cc/Y6MK-SZZ4]\\nVallee, H. Q. la, & Duarte, N. (2019). Algorithmic Systems in Education: Incorporating Equity and Fairness When \\nUsing Student Data. Center for Democracy and Technology. [perma.cc/CC89-ZVNV]\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017, \\nDecember 5). Attention Is All You Need. Advances in Neural Information Processing Systems. [perma.\\ncc/2ZDX-Z796]\\nVincent, J. (2022, November 2). Google plans giant AI language model supporting world’s 1,000 most spoken \\nlanguages. The Verge. [perma.cc/3Y48-X7WV]\\nVincent, J. (2023a, January 17). Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its \\ncontent. The Verge. [perma.cc/4CXS-3WNN]\\nVincent, J. (2023b, March 15). OpenAI co-founder on company’s past approach to openly sharing research: “We were \\nwrong.” The Verge. [perma.cc/DPL6-4PD2]\\nVitulli, M. A. (2018). Writing Women in Mathematics Into Wikipedia. Notices of the American Mathematical \\nSociety, 65(03), 330–334. [perma.cc/X73F-AZPM]\\nVolansky, V., Ordan, N., & Wintner, S. (2015). On the features of translationese. Digital Scholarship in the \\nHumanities, 30(1), 98–118. [perma.cc/7F8S-3YXK]\\nWang, C., Cho, K., & Gu, J. (2020). Neural Machine Translation with Byte-Level Subwords. Proceedings of the \\nAAAI Conference on Artificial Intelligence, 34(05), Article 05. [perma.cc/5DL7-XSSP]\\nWang, Z., K, K., Mayhew, S., & Roth, D. (2020). Extending Multilingual BERT to Low-Resource Languages. \\nFindings of the Association for Computational Linguistics: EMNLP 2020, 2649–2656. [perma.cc/ZNC8-\\nC9E7]\\nWilliams, A., Miceli, M., & Gebru, T. (2022). The Exploited Labor Behind Artificial Intelligence. Noēma. [perma.\\ncc/GE8H-7SUN]\\nWu, S., & Dredze, M. (2019). Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT. Proceedings \\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International \\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), 833–844. [perma.cc/EJ3G-MFYN]\\nWu, S., & Dredze, M. (2020). Are All Languages Created Equal in Multilingual BERT? Proceedings of the 5th \\nWorkshop on Representation Learning for NLP, 120–130. [perma.cc/5E6X-NNAA]\\n49\\nLarge Language Models in Non-English Content Analysis\\nWorks Cited\\nYu, S., Sun, Q., Zhang, H., & Jiang, J. (2022). Translate-Train Embracing Translationese Artifacts. Proceedings of \\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 362–\\n370. [perma.cc/7F8C-EYM6]\\nZhang, S., Frey, B., & Bansal, M. (2022, April 25). How can NLP Help Revitalize Endangered Languages? A Case \\nStudy and Roadmap for the Cherokee Language. Proceedings of the 60th Annual Meeting of the Association \\nfor Computational Linguistics (Volume 1: Long Papers). ACL 2022, Dublin, Ireland. [perma.cc/2XF2-\\n2GDC]\\nZhang, Y., Warstadt, A., Li, X., & Bowman, S. R. (2021). When Do You Need Billions of Words of Pretraining \\nData? Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the \\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1112–1125. \\n[perma.cc/43ZK-2ZXC]\\ncdt.org\\ncdt.org/contact\\n202-637-9800\\n@CenDemTech\\nCenter for Democracy & Technology\\n1401 K Street NW, Suite 200\\nWashington, D.C. 20005\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8BPLp_KPMBd",
        "outputId": "682c2c5d-d056-438b-f5f3-eea2fe7f0f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Published': '2023-06-12',\n",
              " 'Title': 'Lost in Translation: Large Language Models in Non-English Content Analysis',\n",
              " 'Authors': 'Gabriel Nicholas, Aliya Bhatia',\n",
              " 'Summary': \"In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,\\nGoogle's PaLM) have become the dominant approach for building AI systems to\\nanalyze and generate language online. However, the automated systems that\\nincreasingly mediate our interactions online -- such as chatbots, content\\nmoderation systems, and search engines -- are primarily designed for and work\\nfar more effectively in English than in the world's other 7,000 languages.\\nRecently, researchers and technology companies have attempted to extend the\\ncapabilities of large language models into languages other than English by\\nbuilding what are called multilingual language models.\\n  In this paper, we explain how these multilingual language models work and\\nexplore their capabilities and limits. Part I provides a simple technical\\nexplanation of how large language models work, why there is a gap in available\\ndata between English and other languages, and how multilingual language models\\nattempt to bridge that gap. Part II accounts for the challenges of doing\\ncontent analysis with large language models in general and multilingual\\nlanguage models in particular. Part III offers recommendations for companies,\\nresearchers, and policymakers to keep in mind when considering researching,\\ndeveloping and deploying large and multilingual language models.\",\n",
              " 'entry_id': 'http://arxiv.org/abs/2306.07377v1',\n",
              " 'published_first_time': '2023-06-12',\n",
              " 'comment': '50 pages, 4 figures',\n",
              " 'journal_ref': None,\n",
              " 'doi': None,\n",
              " 'primary_category': 'cs.CL',\n",
              " 'categories': ['cs.CL', 'cs.AI'],\n",
              " 'links': ['http://arxiv.org/abs/2306.07377v1',\n",
              "  'http://arxiv.org/pdf/2306.07377v1']}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### YouTube\n",
        "Agora que já vimos como conseguimos pegar informações de artigos científicos usando o ArxivLoader, vamos fazer o mesmo com dados do YouTube!"
      ],
      "metadata": {
        "id": "qXwHhzkcQUx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando as bibliotecas necessárias\n",
        "!pip install youtube-transcript-api youtube-search pytube --quiet"
      ],
      "metadata": {
        "id": "cWTfVcvzQbPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26440f9c-dc27-4fcd-ca1e-3a2e3d615430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando o carregamento dos materiais\n",
        "from langchain_community.tools import YouTubeSearchTool\n",
        "\n",
        "tool = YouTubeSearchTool()\n",
        "\n",
        "# Termo de pesquisa, numero de vídeos retornados\n",
        "links_videos = eval(tool.run(\"anwar hermuche,5\"))\n",
        "\n",
        "links_videos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXbQs5l7Qffa",
        "outputId": "4c75bebc-1975-43c2-8b28-d473ed8c5ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.youtube.com/watch?v=7NLYcGVNPXQ&pp=ygUOYW53YXIgaGVybXVjaGU%3D',\n",
              " 'https://www.youtube.com/watch?v=1PZesoXOL9Q&pp=ygUOYW53YXIgaGVybXVjaGU%3D',\n",
              " 'https://www.youtube.com/watch?v=NgoATpWfFcw&pp=ygUOYW53YXIgaGVybXVjaGU%3D',\n",
              " 'https://www.youtube.com/watch?v=YGgTfz3cNVA&pp=ygUOYW53YXIgaGVybXVjaGU%3D',\n",
              " 'https://www.youtube.com/watch?v=W1QGR9SvhAw&pp=ygUOYW53YXIgaGVybXVjaGU%3D']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando a transcrição em formato de documento de cada um dos vídeos\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain_community.document_loaders.youtube import TranscriptFormat\n",
        "from time import sleep\n",
        "documents = []\n",
        "\n",
        "for link in links_videos:\n",
        "  loader = YoutubeLoader.from_youtube_url(\n",
        "      youtube_url = link,\n",
        "      transcript_format = TranscriptFormat.CHUNKS,\n",
        "      chunk_size_seconds = 90,\n",
        "      add_video_info = False,\n",
        "      language = [\"pt\"]\n",
        "  )\n",
        "\n",
        "  docs = loader.load()\n",
        "  for doc in docs:\n",
        "    documents.append(doc)\n",
        "\n",
        "  sleep(2)"
      ],
      "metadata": {
        "id": "w3Cy6Tq_QqLT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "01804dbf-220b-4ed6-aa8c-9d6e69691e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain_community.document_loaders.youtube.YoutubeLoader object at 0x7d7569380ad0>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'FetchedTranscriptSnippet' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-fa59c6682059>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/document_loaders/youtube.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m             )\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscript_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTranscriptFormat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHUNKS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_transcript_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_pieces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/document_loaders/youtube.py\u001b[0m in \u001b[0;36m_get_transcript_chunks\u001b[0;34m(self, transcript_pieces)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mchunk_time_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtranscript_piece\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranscript_pieces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mpiece_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscript_piece\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranscript_piece\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"duration\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpiece_end\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mchunk_time_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_pieces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'FetchedTranscriptSnippet' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando um documento de exemplo\n",
        "doc = documents[210]\n",
        "doc.page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "uj07ZId0SNRS",
        "outputId": "1be35a9f-003d-42b4-e53c-b58f6a329701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'estiverem desenhando o funcionamento dos seus Agent workflows cara pega o scal draw pega alguma ferramenta o pint que seja e desenha bicho fala aqui ó esse cara aqui vai ser o especialista em dados especialista em dados e por aí vai e aí você fala ó esse cara inclusive ele responde op Nossa tá feio hein responde tal e tal coisa ele faz isso isso aquilo outro ele e você vai pensando nos Estados também do grafo entendeu então você vai desenhando aqui primeiro para depois implementar Essa é a melhor saída que você tem show de bola é a melhor saída que você tem então basicamente eu tô criando aqui eu tô compilando o meu grafo tá aqui ó tô criando de fato ele e aqui eu consigo visualizar o meu gráfico então você pode ver que eu consigo visualizar a partir do código que eu criei que é exatamente o gráfico que eu tô mostrando aqui para vocês só que mais bonito né então eu tive o trabalho aqui de fazer ele mais bonito mas enfim para ficar um pouco mais mais claro e e aqui ó basicamente ó Gere um gráfico de barras com a quantidade de pedidos cancelados entregue vou executar isso aqui você pode ver o seguinte ó olha aqui tô printando aqui os estados pra gente poder ver olha o que que acontece gera um gráfico de barras beleza ó ele tá pensando e tá mandando de um pro outro então você pode ver que ele tá pensando ó e já gerou o código já gerou o código inclusive se eu chegar aqui aqui e pegar o código dele ó esse aqui é'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mCKtp0NTb3G",
        "outputId": "12e43fa3-675b-42bc-dcb1-b0ffaa2bdc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'https://www.youtube.com/watch?v=94F6RYxleHA&t=4680s',\n",
              " 'start_seconds': 4680,\n",
              " 'start_timestamp': '01:18:00'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, tem uma pequena diferença do ArxivLoader: os Documentos já são os chunks que iremos utilizar! Perceba como temos blocos de transcrições que representam 90 segundos de vídeo.\n",
        "\n",
        "Quando estávamos trabalhando com o ArxivLoader, estávamos recebendo o artigo completo que precisaremos, na etapa seguinte, dividir em alguns chunks menores."
      ],
      "metadata": {
        "id": "MsZQvoycUBUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting\n",
        "Agora que já temos o nosso material carregado, precisamos fazer o *splitting* dele, ou seja, dividi-lo em pedaços menores de acordo com uma regra específica.\n",
        "\n",
        "A questão aqui é que podemos fazer esse *splitting* de várias formas. Várias! E essa maneira de criação dos chunks impacta diretamente na qualidade do R.A.G., por isso vamos estudar aqui as principais técnicas.\n",
        "\n",
        "Na maioria dos métodos de splitting, você vai encontrar dois parâmetros:\n",
        "- **Chunk Size**: O número de caracteres usado em cada chunk (50, 100, 1000 etc.)\n",
        "- **Chunk Overlap**: O número de caracteres que você deseja de sobreposição dos chunks sequenciais. Isso é para evitar cortar um pedaço único de contexto em diversos pedaços. Logo, esse *overlap* criará duplicatas ao longo dos chunks.\n",
        "\n",
        "Obs.: estou usando diversos exemplos desse material feito pelo Greg Kamradt. Você pode encontrar [aqui](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb). Inclusive, [esse site](https://chunkviz.up.railway.app/) feito por ele é excelente para a visualização de chunks."
      ],
      "metadata": {
        "id": "XhMMgT15Uc3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chracter Splitting (Divisão por Caracteres)\n",
        "Esse é o método mais simples de realizar a divisão dos chunks. É o processo de dividir o seu texto em N pedaços de X caracteres cada, independentemente do contexto ou formato do texto.\n",
        "\n",
        "<img src=\"https://i.ibb.co/nCbY84s/Screenshot-2024-12-28-at-18-57-55.png\">\n",
        "\n",
        "#### Exemplo"
      ],
      "metadata": {
        "id": "0GynjUz0HhNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto de exemplo\n",
        "doc = Document(page_content = \"Suponha que esse seja um texto que você irá realizar o processo de splitting. É um texto realmente de exemplo!\", metadata = {\"autor\": \"Anwar Hermuche\"})"
      ],
      "metadata": {
        "id": "N4hF6W-qKL3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 55 # Número de caracteres para fazer o splitting\n",
        "CHUNK_OVERLAP = 25 # Número de caracteres de sobreposição"
      ],
      "metadata": {
        "id": "RqUN2NNCPs5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando o splitting manualmente\n",
        "from langchain.schema import Document\n",
        "chunks = []\n",
        "\n",
        "# Fazendo os splits\n",
        "for i in range(0, len(doc.page_content), CHUNK_SIZE - CHUNK_OVERLAP):\n",
        "  texto = doc.page_content\n",
        "  chunk = texto[i:i+CHUNK_SIZE]\n",
        "  document = Document(page_content=chunk, metadata=doc.metadata)\n",
        "  document.metadata[\"chunk_index\"] = i // (CHUNK_SIZE - CHUNK_OVERLAP) + 1\n",
        "  chunks.append(document)\n",
        "\n",
        "  if len(chunk) < CHUNK_SIZE:\n",
        "    break\n",
        "\n",
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp0SEhi_KXcu",
        "outputId": "e361c631-960b-4a04-b0c4-6527041b658c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'autor': 'Anwar Hermuche', 'chunk_index': 1}, page_content='Suponha que esse seja um texto que você irá realizar o '),\n",
              " Document(metadata={'autor': 'Anwar Hermuche', 'chunk_index': 2}, page_content=' que você irá realizar o processo de splitting. É um te'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche', 'chunk_index': 3}, page_content='sso de splitting. É um texto realmente de exemplo!')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizando langchain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Instanciando o\n",
        "text_splitter = CharacterTextSplitter(chunk_size = CHUNK_SIZE, chunk_overlap = CHUNK_OVERLAP, separator = \"\", strip_whitespace = False)\n",
        "text_splitter.split_documents([doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mhbZqay0wdF",
        "outputId": "0b9697ce-d9f8-4389-abb6-7a7baf7a1bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'autor': 'Anwar Hermuche'}, page_content='Suponha que esse seja um texto que você irá realizar o '),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content=' que você irá realizar o processo de splitting. É um te'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='sso de splitting. É um texto realmente de exemplo!')]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Chracter Splitting (Divisão por Caracteres Recursiva)\n",
        "Vimos no método anterior que não levamos em conta a estrutura do texto. Simplesmente contamos o número de caracteres e realizamos a divisão de acordo com algum separador. Só isso.\n",
        "\n",
        "Aqui, utilizando o Recursive Character Text Splitter (que vou chamar de RCTS para facilitar minha vida), podemos especificar uma série de separadores que será utilizado para fazer a divisão dos chunks do nosso documento.\n",
        "\n",
        "Por padrão, ele usa esses:\n",
        "- \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
        "- \"\\n\" - New lines\n",
        "- \" \" - Spaces\n",
        "- \"\" - Characters\n",
        "\n",
        "Sempre começo minhas aplicações com ele. É um excelente ponto de partida.\n",
        "\n",
        "#### Exemplo"
      ],
      "metadata": {
        "id": "T9AQBch-SW4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto de exemplo\n",
        "docs = [Document(page_content = \"O maior enigma que eu nunca consegui desvendar sobre inteligência artificial quando comecei a estudar é como seu desenvolvimento acontece de forma acelerada.\", metadata = {\"autor\": \"Anwar Hermuche\"}),\n",
        "        Document(page_content = \"Pesquisadores e acadêmicos costumavam afirmar com convicção que as descobertas surgiam gradualmente. 'Cada inovação demora', escutei incessantemente, 'seu tempo natural.' Tinham pensamentos honestos, mas essa visão falha. Se sua tecnologia demonstra capacidade inferior aos sistemas existentes, não atrai poucos interessados. Permanece esquecida, virando história.\", metadata = {\"autor\": \"Anwar Hermuche\"}),\n",
        "        Document(page_content = \"Tornou-se inquestionável atualmente que o progresso da IA generativa segue curvas exponenciais na computação moderna. Céticos argumentam que essa característica prejudica o desenvolvimento sustentável, sugerindo alterações profundas nas pesquisas atuais. Porém avanços acelerados representam fenômenos naturais da evolução tecnológica, independente das metodologias estabelecidas. Observamos padrões similares em adaptabilidade, processamento, análise contextual, eficiência computacional e benefícios práticos gerados. Cada descoberta multiplica possibilidades futuras.\", metadata = {\"autor\": \"Anwar Hermuche\"})]"
      ],
      "metadata": {
        "id": "MeVXO9H0f1pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo variáveis\n",
        "CHUNK_SIZE = 55\n",
        "CHUNK_OVERLAP = 25"
      ],
      "metadata": {
        "id": "Iri75Dq6TiCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizando langchain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Instanciando o splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = CHUNK_SIZE, chunk_overlap = CHUNK_OVERLAP)\n",
        "text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4nPrBnuTWfZ",
        "outputId": "85e81748-ca31-431d-b35e-c3d5043b8fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'autor': 'Anwar Hermuche'}, page_content='O maior enigma que eu nunca consegui desvendar sobre'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='consegui desvendar sobre inteligência artificial'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='inteligência artificial quando comecei a estudar é'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='comecei a estudar é como seu desenvolvimento acontece'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='desenvolvimento acontece de forma acelerada.'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='Pesquisadores e acadêmicos costumavam afirmar com'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='costumavam afirmar com convicção que as descobertas'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content=\"que as descobertas surgiam gradualmente. 'Cada\"),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content=\"gradualmente. 'Cada inovação demora', escutei\"),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content=\"demora', escutei incessantemente, 'seu tempo natural.'\"),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content=\"'seu tempo natural.' Tinham pensamentos honestos, mas\"),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='honestos, mas essa visão falha. Se sua tecnologia'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='falha. Se sua tecnologia demonstra capacidade inferior'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='capacidade inferior aos sistemas existentes, não atrai'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='existentes, não atrai poucos interessados. Permanece'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='interessados. Permanece esquecida, virando história.'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='Tornou-se inquestionável atualmente que o progresso da'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='que o progresso da IA generativa segue curvas'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='generativa segue curvas exponenciais na computação'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='na computação moderna. Céticos argumentam que essa'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='argumentam que essa característica prejudica o'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='prejudica o desenvolvimento sustentável, sugerindo'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='sustentável, sugerindo alterações profundas nas'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='alterações profundas nas pesquisas atuais. Porém'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='pesquisas atuais. Porém avanços acelerados representam'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='acelerados representam fenômenos naturais da evolução'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='naturais da evolução tecnológica, independente das'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='independente das metodologias estabelecidas.'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='estabelecidas. Observamos padrões similares em'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='padrões similares em adaptabilidade, processamento,'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='processamento, análise contextual, eficiência'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='contextual, eficiência computacional e benefícios'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='e benefícios práticos gerados. Cada descoberta'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='gerados. Cada descoberta multiplica possibilidades'),\n",
              " Document(metadata={'autor': 'Anwar Hermuche'}, page_content='possibilidades futuras.')]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Chunking (Divisão semântica)\n",
        "Não sei se você se questionou isso até aqui, mas não é estranho ter um tamanho pré definido de chunks para todo o documento? Alguns chuks podem ser maiores que outros naturalmente.\n",
        "\n",
        "Já comentamos sobre os embeddings, e eles sozinhos não fazem muita coisa. Mas quando começamos a **comparar embeddings diferentes conseguimos começar a inferir algumas relações entre os chunks**.\n",
        "\n",
        "O que podemos fazer então? Tentar criar clusters de chunks semelhantes com o intuito de, juntos, terem um significado maior. O código abaixo você pode utilizar sem precisar entender, caso não queira."
      ],
      "metadata": {
        "id": "9N_RKhEUVv8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SW8e4qpTnHp",
        "outputId": "26f86533-4d3e-473a-dd59-72dc386370d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/209.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "\n",
        "# Carregando os documentos\n",
        "loader = PDFPlumberLoader(\n",
        "    file_path = \"/content/A Startup Enxuta - Eric Ries.pdf\"\n",
        ")\n",
        "\n",
        "# Documentos\n",
        "docs = loader.load()\n",
        "\n",
        "# Splitter\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings(), breakpoint_threshold_type = \"percentile\")\n",
        "\n",
        "# Documentos\n",
        "docs = text_splitter.split_documents(docs)\n",
        "print(docs[385].page_content)\n",
        "print(\"-=\"*20)\n",
        "print(docs[385].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPTZCxCbsJLx",
        "outputId": "c64f988d-e0f3-4370-a53d-fe6e741a718a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59 Informações acerca da Alphabet Energy foram obtidas em entrevistas realizadas por Sara Leslie. 60 Para mais detalhes a respeito da organização de aprendizagem da Toyota, ver The Toyota Way (O modelo Toyota), de\n",
            "Jeffrey Liker. \n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "{'source': '/content/A Startup Enxuta - Eric Ries.pdf', 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf', 'page': 148, 'total_pages': 210, 'Author': 'Eric Ries', 'CreationDate': \"D:20150330135456+00'00'\", 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Title': 'A Startup Enxuta'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agentic Chunking (Divisão realizada por Agentes)\n",
        "Já foi discutido aqui que qualquer termo chamado de agêntico faz coisas que humanos fariam. Então, vamos nos questionar: **como humanos criariam chunks**?\n",
        "\n",
        "Bom, quando não temos chunk ainda, pegamos a primeira parte para ser um chunk. Depois disso, vemos se a segunda parte é similar o suficiente para também ser colocada no primeiro chunk. Se sim, colocamos. Se não, criamos um novo chunk e continuamos o processo."
      ],
      "metadata": {
        "id": "1brDp2wXvsV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import uuid\n",
        "import os\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "class AgenticChunker:\n",
        "    def __init__(self, openai_api_key=None):\n",
        "        self.chunks = {}\n",
        "        self.id_truncate_limit = 5\n",
        "\n",
        "        # Whether or not to update/refine summaries and titles as you get new information\n",
        "        self.generate_new_metadata_ind = True\n",
        "        self.print_logging = True\n",
        "\n",
        "        if openai_api_key is None:\n",
        "            openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "        if openai_api_key is None:\n",
        "            raise ValueError(\"API key is not provided and not found in environment variables\")\n",
        "\n",
        "        self.llm = ChatOpenAI(model='gpt-4o-mini', openai_api_key=openai_api_key, temperature=0)\n",
        "\n",
        "    def add_propositions(self, propositions):\n",
        "        for proposition in propositions:\n",
        "            self.add_proposition(proposition)\n",
        "\n",
        "    def add_proposition(self, proposition):\n",
        "        if self.print_logging:\n",
        "            print (f\"\\nAdding: '{proposition}'\")\n",
        "\n",
        "        # If it's your first chunk, just make a new chunk and don't check for others\n",
        "        if len(self.chunks) == 0:\n",
        "            if self.print_logging:\n",
        "                print (\"No chunks, creating a new one\")\n",
        "            self._create_new_chunk(proposition)\n",
        "            return\n",
        "\n",
        "        chunk_id = self._find_relevant_chunk(proposition)\n",
        "\n",
        "        # If a chunk was found then add the proposition to it\n",
        "        if chunk_id:\n",
        "            if self.print_logging:\n",
        "                print (f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
        "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
        "            return\n",
        "        else:\n",
        "            if self.print_logging:\n",
        "                print (\"No chunks found\")\n",
        "            # If a chunk wasn't found, then create a new one\n",
        "            self._create_new_chunk(proposition)\n",
        "\n",
        "\n",
        "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
        "        # Add then\n",
        "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
        "\n",
        "        # Then grab a new summary\n",
        "        if self.generate_new_metadata_ind:\n",
        "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
        "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
        "\n",
        "    def _update_chunk_summary(self, chunk):\n",
        "        \"\"\"\n",
        "        If you add a new proposition to a chunk, you may want to update the summary or else they could get stale\n",
        "        \"\"\"\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
        "\n",
        "                    You will be given a group of propositions which are in the chunk and the chunks current summary.\n",
        "\n",
        "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Proposition: Greg likes to eat pizza\n",
        "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
        "\n",
        "                    Only respond with the chunk new summary, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nCurrent chunk summary:\\n{current_summary}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_summary = runnable.invoke({\n",
        "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
        "            \"current_summary\" : chunk['summary']\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_summary\n",
        "\n",
        "    def _update_chunk_title(self, chunk):\n",
        "        \"\"\"\n",
        "        If you add a new proposition to a chunk, you may want to update the title or else it can get stale\n",
        "        \"\"\"\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good title will say what the chunk is about.\n",
        "\n",
        "                    You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\n",
        "\n",
        "                    Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
        "                    Output: Date & Times\n",
        "\n",
        "                    Only respond with the new chunk title, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        updated_chunk_title = runnable.invoke({\n",
        "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
        "            \"current_summary\" : chunk['summary'],\n",
        "            \"current_title\" : chunk['title']\n",
        "        }).content\n",
        "\n",
        "        return updated_chunk_title\n",
        "\n",
        "    def _get_new_chunk_summary(self, proposition):\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
        "\n",
        "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
        "\n",
        "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Proposition: Greg likes to eat pizza\n",
        "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
        "\n",
        "                    Only respond with the new chunk summary, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_summary = runnable.invoke({\n",
        "            \"proposition\": proposition\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_summary\n",
        "\n",
        "    def _get_new_chunk_title(self, summary):\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good chunk title is brief but encompasses what the chunk is about\n",
        "\n",
        "                    You will be given a summary of a chunk which needs a title\n",
        "\n",
        "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
        "                    Output: Date & Times\n",
        "\n",
        "                    Only respond with the new chunk title, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_title = runnable.invoke({\n",
        "            \"summary\": summary\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_title\n",
        "\n",
        "\n",
        "    def _create_new_chunk(self, proposition):\n",
        "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit] # I don't want long ids\n",
        "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
        "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
        "\n",
        "        self.chunks[new_chunk_id] = {\n",
        "            'chunk_id' : new_chunk_id,\n",
        "            'propositions': [proposition],\n",
        "            'title' : new_chunk_title,\n",
        "            'summary': new_chunk_summary,\n",
        "            'chunk_index' : len(self.chunks)\n",
        "        }\n",
        "        if self.print_logging:\n",
        "            print (f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
        "\n",
        "    def get_chunk_outline(self):\n",
        "        \"\"\"\n",
        "        Get a string which represents the chunks you currently have.\n",
        "        This will be empty when you first start off\n",
        "        \"\"\"\n",
        "        chunk_outline = \"\"\n",
        "\n",
        "        for chunk_id, chunk in self.chunks.items():\n",
        "            single_chunk_string = f\"\"\"Chunk ID: {chunk['chunk_id']}\\nChunk Name: {chunk['title']}\\nChunk Summary: {chunk['summary']}\\n\\n\"\"\"\n",
        "\n",
        "            chunk_outline += single_chunk_string\n",
        "\n",
        "        return chunk_outline\n",
        "\n",
        "    def _find_relevant_chunk(self, proposition):\n",
        "        current_chunk_outline = self.get_chunk_outline()\n",
        "\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    Determine whether or not the \"Proposition\" should belong to any of the existing chunks.\n",
        "\n",
        "                    A proposition should belong to a chunk of their meaning, direction, or intention are similar.\n",
        "                    The goal is to group similar propositions and chunks.\n",
        "\n",
        "                    If you think a proposition should be joined with a chunk, return the chunk id.\n",
        "                    If you do not think an item should be joined with an existing chunk, just return \"No chunks\"\n",
        "\n",
        "                    Example:\n",
        "                    Input:\n",
        "                        - Proposition: \"Greg really likes hamburgers\"\n",
        "                        - Current Chunks:\n",
        "                            - Chunk ID: 2n4l3d\n",
        "                            - Chunk Name: Places in San Francisco\n",
        "                            - Chunk Summary: Overview of the things to do with San Francisco Places\n",
        "\n",
        "                            - Chunk ID: 93833k\n",
        "                            - Chunk Name: Food Greg likes\n",
        "                            - Chunk Summary: Lists of the food and dishes that Greg likes\n",
        "                    Output: 93833k\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\"),\n",
        "                (\"user\", \"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        chunk_found = runnable.invoke({\n",
        "            \"proposition\": proposition,\n",
        "            \"current_chunk_outline\": current_chunk_outline\n",
        "        }).content\n",
        "\n",
        "        # Pydantic data class\n",
        "        class ChunkID(BaseModel):\n",
        "            \"\"\"Extracting the chunk id\"\"\"\n",
        "            chunk_id: Optional[str]\n",
        "\n",
        "        # Extraction to catch-all LLM responses. This is a bandaid\n",
        "        llm_structured = self.llm.with_structured_output(ChunkID)\n",
        "        chunk_found = llm_structured.invoke(chunk_found).chunk_id\n",
        "\n",
        "        # If you got a response that isn't the chunk id limit, chances are it's a bad response or it found nothing\n",
        "        # So return nothing\n",
        "        if chunk_found != None and len(chunk_found) != self.id_truncate_limit:\n",
        "            return None\n",
        "\n",
        "        return chunk_found\n",
        "\n",
        "    def get_chunks(self, get_type='dict'):\n",
        "        \"\"\"\n",
        "        This function returns the chunks in the format specified by the 'get_type' parameter.\n",
        "        If 'get_type' is 'dict', it returns the chunks as a dictionary.\n",
        "        If 'get_type' is 'list_of_strings', it returns the chunks as a list of strings, where each string is a proposition in the chunk.\n",
        "        \"\"\"\n",
        "        if get_type == 'dict':\n",
        "            return self.chunks\n",
        "        if get_type == 'list_of_strings':\n",
        "            chunks = []\n",
        "            for chunk_id, chunk in self.chunks.items():\n",
        "                chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
        "            return chunks\n",
        "\n",
        "    def pretty_print_chunks(self):\n",
        "        print (f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
        "        for chunk_id, chunk in self.chunks.items():\n",
        "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
        "            print(f\"Chunk ID: {chunk_id}\")\n",
        "            print(f\"Summary: {chunk['summary']}\")\n",
        "            print(f\"Propositions:\")\n",
        "            for prop in chunk['propositions']:\n",
        "                print(f\"    -{prop}\")\n",
        "            print(\"\\n\\n\")\n",
        "\n",
        "    def pretty_print_chunk_outline(self):\n",
        "        print (\"Chunk Outline\\n\")\n",
        "        print(self.get_chunk_outline())"
      ],
      "metadata": {
        "id": "3bD08m-Pu93U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from typing import Optional, List\n",
        "from pydantic import BaseModel\n",
        "from langchain import hub\n",
        "\n",
        "obj = hub.pull(\"wfh/proposal-indexing\")\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature = 0)\n",
        "\n",
        "print(obj.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9hnYYMz20Qr",
        "outputId": "3a964903-1bc7-4698-a57c-408cc6df2575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decompose the \"Content\" into clear and simple propositions, ensuring they are interpretable out of\n",
            "context.\n",
            "1. Split compound sentence into simple sentences. Maintain the original phrasing from the input\n",
            "whenever possible.\n",
            "2. For any named entity that is accompanied by additional descriptive information, separate this\n",
            "information into its own distinct proposition.\n",
            "3. Decontextualize the proposition by adding necessary modifier to nouns or entire sentences\n",
            "and replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\") with the full name of the\n",
            "entities they refer to.\n",
            "4. Present the results as a list of strings, formatted in JSON.\n",
            "\n",
            "Example:\n",
            "\n",
            "Input: Title: ¯Eostre. Section: Theories and interpretations, Connection to Easter Hares. Content:\n",
            "The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in\n",
            "1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in\n",
            "other parts of Germany until the 18th century. Scholar Richard Sermon writes that \"hares were\n",
            "frequently seen in gardens in spring, and thus may have served as a convenient explanation for the\n",
            "origin of the colored eggs hidden there for children. Alternatively, there is a European tradition\n",
            "that hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and\n",
            "both occur on grassland and are first seen in the spring. In the nineteenth century the influence\n",
            "of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.\n",
            "German immigrants then exported the custom to Britain and America where it evolved into the\n",
            "Easter Bunny.\"\n",
            "Output: [ \"The earliest evidence for the Easter Hare was recorded in south-west Germany in\n",
            "1678 by Georg Franck von Franckenau.\", \"Georg Franck von Franckenau was a professor of\n",
            "medicine.\", \"The evidence for the Easter Hare remained unknown in other parts of Germany until\n",
            "the 18th century.\", \"Richard Sermon was a scholar.\", \"Richard Sermon writes a hypothesis about\n",
            "the possible explanation for the connection between hares and the tradition during Easter\", \"Hares\n",
            "were frequently seen in gardens in spring.\", \"Hares may have served as a convenient explanation\n",
            "for the origin of the colored eggs hidden in gardens for children.\", \"There is a European tradition\n",
            "that hares laid eggs.\", \"A hare’s scratch or form and a lapwing’s nest look very similar.\", \"Both\n",
            "hares and lapwing’s nests occur on grassland and are first seen in the spring.\", \"In the nineteenth\n",
            "century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular\n",
            "throughout Europe.\", \"German immigrants exported the custom of the Easter Hare/Rabbit to\n",
            "Britain and America.\", \"The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in\n",
            "Britain and America.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um runnable\n",
        "runnable = obj | llm | StrOutputParser()\n",
        "\n",
        "# Classe de sentença\n",
        "class Sentences(BaseModel):\n",
        "  sentences: List[str]\n",
        "\n",
        "# Função para obter as proposições\n",
        "def obterProposicoes(text):\n",
        "  runnable_output = runnable.invoke({\n",
        "    \"input\": text\n",
        "  })\n",
        "\n",
        "  llm_structured = llm.with_structured_output(Sentences)\n",
        "  proposicoes = llm_structured.invoke(runnable_output).sentences\n",
        "  return proposicoes"
      ],
      "metadata": {
        "id": "6s9tqqe06S8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Conteúdo em texto do livro\n",
        "# loader = PDFPlumberLoader(\n",
        "#     file_path = \"/content/A Startup Enxuta - Eric Ries.pdf\"\n",
        "# )\n",
        "# docs = loader.load()\n",
        "# conteudo = \"\\n\\n\".join([x.page_content for x in docs if x.page_content.strip() not in (\"\\n\", \"\", \" \", \"\\t\")])\n",
        "\n",
        "# O que define uma parte?\n",
        "paragrafos = \"Suponha que esse seja um texto que você irá realizar o processo de splitting.\\n\\nÉ um texto realmente de exemplo!\\n\\nGosto de cachorros.\".split(\"\\n\\n\")\n",
        "len(paragrafos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beEje8vb6JFR",
        "outputId": "4775d534-1505-4955-de02-9f8052612b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proposicoes_livro = []\n",
        "\n",
        "# Fazendo com apenas 5 para poder demonstrar, porque 213 é demais para uma demonstração\n",
        "for i, para in enumerate(paragrafos[:5]):\n",
        "  propositions = obterProposicoes(para)\n",
        "\n",
        "  proposicoes_livro.extend(propositions)\n",
        "  print(f\"Feito {i+1}/5\")\n",
        "\n",
        "print(\"-=\"*20)\n",
        "proposicoes_livro[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAEimW2k8AIj",
        "outputId": "7cb4015c-d908-49d7-c37c-73c6f151cf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feito 1/5\n",
            "Feito 2/5\n",
            "Feito 3/5\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Suponha que esse seja um texto.',\n",
              " 'Você irá realizar o processo de splitting.',\n",
              " 'É um texto realmente de exemplo.',\n",
              " 'A pessoa gosta de cachorros.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação do agentic chunker\n",
        "ac = AgenticChunker()\n",
        "ac.add_propositions(proposicoes_livro)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCRZlfo88q3p",
        "outputId": "3ba25242-2600-4650-a91a-9b79c5094ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adding: 'Suponha que esse seja um texto.'\n",
            "No chunks, creating a new one\n",
            "Created new chunk (be495): Text & Written Content\n",
            "\n",
            "Adding: 'Você irá realizar o processo de splitting.'\n",
            "No chunks found\n",
            "Created new chunk (696b4): Task Management\n",
            "\n",
            "Adding: 'É um texto realmente de exemplo.'\n",
            "Chunk Found (be495), adding to: Text & Written Content\n",
            "\n",
            "Adding: 'A pessoa gosta de cachorros.'\n",
            "No chunks found\n",
            "Created new chunk (224d3): Pets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ac.pretty_print_chunks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x9FiX46Bicg",
        "outputId": "f7bbae7f-55db-49f8-aff0-5322047361ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You have 3 chunks\n",
            "\n",
            "Chunk #0\n",
            "Chunk ID: be495\n",
            "Summary: This chunk contains example text and related written content.\n",
            "Propositions:\n",
            "    -Suponha que esse seja um texto.\n",
            "    -É um texto realmente de exemplo.\n",
            "\n",
            "\n",
            "\n",
            "Chunk #1\n",
            "Chunk ID: 696b4\n",
            "Summary: This chunk contains instructions related to the process of splitting tasks or items.\n",
            "Propositions:\n",
            "    -Você irá realizar o processo de splitting.\n",
            "\n",
            "\n",
            "\n",
            "Chunk #2\n",
            "Chunk ID: 224d3\n",
            "Summary: This chunk contains information about the types of pets that the person likes.\n",
            "Propositions:\n",
            "    -A pessoa gosta de cachorros.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando os chunks\n",
        "chunks = ac.get_chunks(get_type='list_of_strings')\n",
        "\n",
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC8OwdBGFkxj",
        "outputId": "d32890c3-0f9f-49a5-89fd-7464a5cc80aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A presente obra é disponibilizada pela equipe Le Livros e seus diversos parceiros. É totalmente repudiável o aluguel do presente conteúdo. É totalmente repudiável qualquer uso comercial do presente conteúdo.',\n",
              " 'O objetivo da disponibilização da obra é oferecer conteúdo para uso parcial em pesquisas e estudos acadêmicos. O objetivo da disponibilização da obra também é permitir o simples teste da qualidade da obra. O fim exclusivo da disponibilização da obra é a compra futura.',\n",
              " 'É expressamente proibida a venda do presente conteúdo.',\n",
              " 'O Le Livros e seus parceiros disponibilizam conteúdo de domínio público e propriedade intelectual de forma totalmente gratuita.',\n",
              " 'O Le Livros e seus parceiros acreditam que o conhecimento e a educação devem ser acessíveis e livres a toda e qualquer pessoa. Quando o mundo estiver unido na busca do conhecimento, a sociedade poderá evoluir a um novo nível.',\n",
              " 'Você pode encontrar mais obras no site LeLivros.link. Você pode encontrar mais obras em qualquer um dos sites parceiros apresentados no link.',\n",
              " 'Copyright © 2011 by Eric Ries. Eric Ries holds all rights reserved. The translation to the Portuguese language is copyrighted © 2012 by Texto Editores Ltda.',\n",
              " 'The original title of the work is The Lean Startup. The work titled A startup enxuta: como os empreendedores atuais utilizam a inovação contínua para criar empresas extremamente bem-sucedidas is authored by Eric Ries.',\n",
              " 'Pascoal Soto is the editorial director. Pedro Almeida is the editor. André Fonseca is the editorial producer. Ricardo Sazima is the technical supervisor. Alessandra Miranda prepared the text. Alexandra Fonseca conducted the revision.',\n",
              " 'Priscila Brauner is responsible for marketing.',\n",
              " 'Marcus Gosling created the cover image.',\n",
              " 'Carlos Szlak is the translator. Texto Editores is responsible for the translation.',\n",
              " 'The International Cataloging in Publication Data (CIP) is provided by the Câmara Brasileira do Livro in São Paulo, Brazil.',\n",
              " 'The publication location is São Paulo.',\n",
              " 'The publisher is Lua de Papel. Texto Editores Ltda. is a publisher of the Leya group. Texto Editores Ltda. is located at Rua Desembargador Paulo Passalácqua, 86, 01248-010, Pacaembu, São Paulo, SP. The website for Texto Editores Ltda. is www.leya.com.br/luadepapel. Texto Editores Ltda. has a Twitter account with the handle @luadepapel_BRA and @EditoraLeya.',\n",
              " 'The publication year is 2012.',\n",
              " 'The original title is The Lean Startup. The work discusses entrepreneurship. The work discusses the administration of new companies. The systematic catalog index includes new businesses and business administration under the number 658.11. O processo de Eric Ries visa a redução de riscos, prejuízos e tempo na criação e no estabelecimento de novas startups. A startup enxuta propõe um novo modo de pensar e de construir produtos e serviços inovadores.',\n",
              " 'The ISBN of the work is 9788581780139.',\n",
              " \"The work discusses consumers' preferences.\",\n",
              " 'The work discusses creativity in business.',\n",
              " 'The work discusses organizational efficiency.',\n",
              " 'The classification number is 12-03325. The CDD classification is 658.11.',\n",
              " 'Para Tara is a named entity.',\n",
              " 'O autor alcançou o fracasso de forma espetacular.',\n",
              " 'O ano era 1999.',\n",
              " 'O autor e um estagiário desenvolveram um dos primeiros sistemas de e-Learning do Brasil. O sistema de e-Learning foi desenvolvido para o Senar. O desenvolvimento do sistema foi feito em parceria com a Embrapa. O êxito do sistema motivou o autor a fundar uma startup na área de e-Learning. A Instruct.com não se contentaria com nada menos que o estado da arte em e-Learning.',\n",
              " 'O Senar é o Serviço Nacional de Aprendizagem Rural. O sistema capacitou trabalhadores rurais em administração rural.',\n",
              " 'A Embrapa é a Empresa Brasileira de Pesquisa Agropecuária.',\n",
              " 'O autor e o estagiário eram apenas duas pessoas.',\n",
              " 'O estagiário se chamava Virgílio Cruzeiro Neves. Virgílio Cruzeiro Neves se tornaria sócio do autor posteriormente. O autor contatou Virgílio Cruzeiro Neves, que já estava formado e morando na Suíça. O autor convidou Virgílio Cruzeiro Neves para ser sócio na empreitada. O autor e Virgílio Cruzeiro Neves montaram um plano de negócios. O autor e Virgílio Cruzeiro Neves submeteram sua inscrição à incubadora do SOFTEX. O autor e Virgílio Cruzeiro Neves convenceram amigos e familiares de que não estavam loucos ao trocar carreiras promissoras por uma aventura incerta. O autor e Virgílio Cruzeiro Neves passaram por inúmeras dificuldades durante o desenvolvimento. Quando o sistema estava minimamente pronto para a comercialização, o autor e Virgílio Cruzeiro Neves foram para o mercado. O autor e Virgílio Cruzeiro Neves ficaram surpresos ao descobrir que ninguém comprava o sistema. O autor e Virgílio Cruzeiro Neves fizeram duas vendas pequenas: uma para a prefeitura e outra para um instituto. O autor e Virgílio Cruzeiro Neves haviam investido tempo, esforço e dinheiro consideráveis. O autor e Virgílio Cruzeiro Neves construíram com sucesso um bom produto que ninguém queria.',\n",
              " 'O sistema foi usado em escala nacional.',\n",
              " 'O sistema tinha conteúdo rico, com imagens, áudio, vídeo e animações. O autor e o estagiário aplicaram conhecimentos avançados na arquitetura e design da interface do sistema.',\n",
              " 'O sistema possibilitava a discussão do assunto entre os participantes por meio de uma lista de discussões.',\n",
              " 'O sistema tinha testes de conhecimento, relatórios e ferramentas de administração.',\n",
              " 'O sistema era uma ferramenta avançada para a época.',\n",
              " 'O sistema foi construído 100% sobre software livre.',\n",
              " 'A linguagem PHP estava engatinhando na época.',\n",
              " 'O sistema teve sucesso em sua missão.',\n",
              " 'O sistema recebeu reconhecimento em congressos nacionais e internacionais.',\n",
              " 'O plano de negócios foi montado de acordo com o que aprenderam em um curso de planos de negócios.',\n",
              " 'O autor e Virgílio Cruzeiro Neves estavam codificando o novo software. O autor e Virgílio Cruzeiro Neves passaram quase um ano desenvolvendo do zero a nova ferramenta. O autor e Virgílio Cruzeiro Neves contrataram profissionais para ajudar no desenvolvimento. O autor e Virgílio Cruzeiro Neves estavam progredindo no desenvolvimento do sistema. A cada mês, o autor e Virgílio Cruzeiro Neves tinham mais funcionalidades desenvolvidas e um sistema mais completo e estável.',\n",
              " 'A nova ferramenta seria uma grande evolução do primeiro sistema.',\n",
              " 'O autor trocou o carro por uma scooter para economizar gasolina.',\n",
              " 'Essa foi a dura introdução do autor e Virgílio Cruzeiro Neves ao assunto deste livro.',\n",
              " 'O autor aprendeu muito com essa experiência. O autor questiona se teria sido possível aprender mais rápido e com menos esforço e investimento.',\n",
              " 'A história ilustra o que acontece quando nos isolamos do cliente. Olhando em retrospectiva a história da Instruct, fica claro que deveríamos ter investido mais em contato com o cliente.',\n",
              " 'Identificamos e testamos nossas principais suposições.',\n",
              " 'Não temos contabilidade e métricas adequadas. Eric Ries prioriza métricas adequadas.',\n",
              " 'Trabalhamos em grandes lotes.',\n",
              " 'Não temos feedback real e constante.',\n",
              " 'Mais de uma década depois, muitos empreendimentos inovadores ainda cometem os mesmos erros.',\n",
              " 'Rotineiramente, vejo empresas pequenas e grandes trabalhando em produtos e serviços.',\n",
              " 'As empresas não têm praticamente nenhuma validação do seu valor. As empresas não têm validação das suas possibilidades de crescimento. As empresas não têm validação da sua transformação em um negócio sustentável. As empresas são apoiadas em projeções de crescimento. Se o mercado afirmasse que enxergava valor, partiríamos para a otimização.',\n",
              " 'As empresas são apoiadas basicamente em pesquisas de mercado.',\n",
              " 'As empresas são apoiadas em grupos de foco.',\n",
              " 'As empresas são apoiadas principalmente na intuição de seus líderes. Eric Ries prioriza a decisão de perseverar se estivermos no caminho certo.',\n",
              " 'Eric Ries desenvolveu um processo absolutamente inovador. Eric Ries prioriza a velocidade em percorrer o ciclo Construir-Medir-Aprender. Eric Ries prioriza o teste das suposições fundamentais de valor e crescimento utilizando produtos viáveis mínimos (MVPs). Eric Ries prioriza a otimização do produto por meio de testes. Eric Ries prioriza a contabilidade para a inovação. Eric Ries prioriza a decisão de pivotar caso a estratégia seja furada. Deveríamos ter investido mais em experimentação e validação. Poderíamos ter construído um MVP simples para testar se o mercado enxergava valor.',\n",
              " 'A startup enxuta visa levar a um negócio sustentável. Sempre mediríamos o progresso em direção a um negócio sustentável.',\n",
              " 'Este livro contextualiza o leitor no mundo das startups e da inovação radical. O livro apresenta as origens da startup enxuta. O livro constrói paralelos importantes com a manufatura enxuta. A startup enxuta é um assunto permanente nesses meios.',\n",
              " 'O livro mostra por que é necessário um novo método para a inovação na economia moderna.',\n",
              " 'O livro define conceitos fundamentais para o entendimento da teoria.',\n",
              " 'O livro aborda a questão da aprendizagem validada.',\n",
              " 'O livro aborda a utilização do método científico.',\n",
              " 'A manufatura enxuta é uma abordagem comprovada para a produção de bens físicos.',\n",
              " 'Deveríamos ter investido menos em planejamento e execução cega.',\n",
              " 'Poderíamos ter testado se o mercado compraria uma solução como a que estávamos imaginando.',\n",
              " 'Tenho acompanhado ativamente o cenário de startups e inovação no Brasil.',\n",
              " 'Vejo um entusiasmo renovado com o tema do empreendedorismo. A empolgação dos adeptos é um sinal evidente de que a startup enxuta atingiu em cheio o público-alvo.',\n",
              " 'As pessoas estão vibrantes, cheias de energia e vontade de colocar em prática seus talentos e esforços.',\n",
              " 'As pessoas querem materializar seus sonhos.',\n",
              " 'Nota-se claramente a empolgação de seus adeptos.',\n",
              " 'Noto uma certa confusão e dificuldade no momento de colocar os conceitos em prática.',\n",
              " 'Noto dificuldade em guiar os esforços de uma startup pela metodologia proposta.',\n",
              " 'Creio que este livro terá um papel relevante no esclarecimento da metodologia no Brasil. Creio que este livro terá um papel relevante no sucesso das startups que adotarem este pensamento.',\n",
              " 'O mais importante é entender os princípios.',\n",
              " 'Não se deve ater a táticas e técnicas específicas.']"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação de Embeddings\n",
        "Agora que temos os nossos dados divididos em chunks, precisamos tornar esses chunks pesquisáveis para poder retornar os chunks mais similares à query do usuário. **Mas como medimos a similaridade de dois textos?**\n",
        "\n",
        "A primeira coisa que precisamos pensar é que realmente não tem jeito. Para mensurar a similaridade de dois textos, precisamos transforma-lo em números! E aí você pode pensar:\n",
        "\n",
        "- \"Ah, Anwar, mas tem como ser com texto. Basta contar quantas palavras iguais a query e o chunk possuem\".\n",
        "\n",
        "Concordo! É uma maneira, mas concorda comigo que podemos representar isso com números também? (se você não concorda, segura um pouco que vou te mostrar que sim, é possível)\n",
        "\n",
        "E agora fica a questão do milênio: **qual a melhor maneira de conseguir extrair o máximo de informações de um texto quando transformamos ele em números?**"
      ],
      "metadata": {
        "id": "WQ45j2BZHkQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1322/format:webp/0*cf1wq8eIix-Z2qIf.png\">\n",
        "\n",
        "Como disse, há várias maneiras de representar um texto numericamente para, depois, calcular a similaridade dos dois textos.\n",
        "\n",
        "A primeira técnica que veremos é a Bag of Words, um método estatístico. Mesmo que hoje não seja tão usada assim, é importante conhecermos as técnicas para ficar mais inteligente.\n",
        "\n",
        "Na imagem, você consegue ver como a Bag of Words funciona. **Pegamos uma frase, analisamos quais palavras únicas a frase tem e contamos quantas vezes cada palavra aparece no texto**.\n",
        "\n",
        "Observe o exemplo abaixo:"
      ],
      "metadata": {
        "id": "CL8CliwEJe9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Documentos de exemplo\n",
        "doc1 = Document(page_content = \"Eu amo meu cachorro! Amo!\", metadata = {})\n",
        "doc2 = Document(page_content = \"Eu gosto do meu gato! Amo ele.\", metadata = {})"
      ],
      "metadata": {
        "id": "XOFtj5TYwx-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vamos unir os textos de todos os documentos para construir nossa *bag of words*."
      ],
      "metadata": {
        "id": "129JXONGxBx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construindo a Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "all_texts = [doc1.page_content, doc2.page_content]\n",
        "vectorizer = CountVectorizer()\n",
        "bag_of_words = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(\"-=\"*30)\n",
        "print(bag_of_words.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-B6Lm0OxBbY",
        "outputId": "147cb835-f58a-4585-b1fc-0fdf564ab8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amo' 'cachorro' 'do' 'ele' 'eu' 'gato' 'gosto' 'meu']\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "[[2 1 0 0 1 0 0 1]\n",
            " [1 0 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vendo a similaridade das duas frases\n",
        "qtd_palavras_iguais = 0\n",
        "for i, palavra in enumerate(vectorizer.get_feature_names_out()):\n",
        "  if bag_of_words.toarray()[0][i] != 0 and bag_of_words.toarray()[1][i] != 0:\n",
        "    qtd_palavras_iguais += 1\n",
        "\n",
        "print(f\"Quantidade de palavras iguais: {qtd_palavras_iguais}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcvR-FX7xhQB",
        "outputId": "e4256f82-d05c-4248-aa1b-d43933b62e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de palavras iguais: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que a utilização de Bag of Words faz com que a representação fique muito esparsa, e isso significa ter muitos zeros no nosso array. Imagine em um livro a quantidade de palavras únicas que ele possui. **O vocabulário pode ficar muito grande e, além disso, não estamos armazenando a informação semântica de cada documento, somente a frequência de palavras**.\n",
        "\n",
        "Há um método de buscar documentos similares utilizando Bag of Words como representação numérica muito famoso que vamos estudar um pouco mais para frente."
      ],
      "metadata": {
        "id": "v2eNkE5OzOrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings\n",
        "<img src=\"https://assets.zilliz.com/How_vector_embeddings_are_generated_and_stored_7e9c5a2a41.png\">\n",
        "\n",
        "Acabamos de ver um método puramente estatístico que analisa apenas a frequência de aparição das palavras em um texto. Os embeddings, um método que utiliza Machine Learning, conseguem capturar uma coisa muito mais interessante: **a semântica do texto - o seu significado**.\n",
        "\n",
        "Concorda comigo que as frases **Estou bem cansado** e **Preciso urgentemente descansar um pouco** são frases que, semanticamente, dizem a mesma coisa, mas não possuem palavras em comum. Mesmo sendo similares, se utilizarmos a estratégia de Bag of Words não veríamos similaridade alguma.\n",
        "\n",
        "Quando utilizamos embedding para transformar um texto em números, podemos colocar esses números em um gráfico para ver onde ele ficaria posicionado espacialmente.\n",
        "\n",
        "Observe a imagem abaixo:\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*SYiW1MUZul1NvL1kc1RxwQ.png\">\n",
        "\n",
        "\n",
        "Note o primeiro exemplo, à esquerda. Note que de \"man\" para \"woman\" possui um vetor (quase) idêntico ao vetor de vai de \"king\" para \"queen\", porque há uma diferença apenas de gênero de uma palavra para outra, concorda?\n",
        "\n",
        "O mesmo acontece na imagem à direita. Os vetores são idênticos dos países até suas capitais.\n",
        "\n",
        "Ou seja, conseguimos uma representação semântica do nosso texto! Tem coisa melhor que isso? Até hoje, não sabemos.\n",
        "\n",
        "Vamos exemplificar em código:"
      ],
      "metadata": {
        "id": "HwfdHhfkydXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Documentos de exemplo\n",
        "doc1 = Document(page_content = \"Estou com fome\", metadata = {})\n",
        "doc2 = Document(page_content = \"Me encontro faminto\", metadata = {})"
      ],
      "metadata": {
        "id": "4AZDxQf7yXgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando embedding\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
        "\n",
        "emb1 = embeddings.embed_query(doc1.page_content)\n",
        "emb2 = embeddings.embed_query(doc2.page_content)\n",
        "\n",
        "print(emb1)\n",
        "print(\"-=\"*30)\n",
        "print(emb2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_F3jdFO2Jhk",
        "outputId": "5822b8ed-94e6-48b3-e9dd-f59f18d90706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.028203917667269707, -0.0031606138218194246, -0.035718388855457306, -0.017829610034823418, -0.017356257885694504, 0.027651673182845116, -0.013648329302668571, 0.04804527387022972, -0.020334433764219284, -0.05214766412973404, 0.007706769742071629, 0.006720618810504675, -0.026744414120912552, 0.016725121065974236, -0.016902627423405647, -0.01830296218395233, 0.03605367988348007, 0.019259529188275337, 0.012888993136584759, 0.019456759095191956, 0.04630964994430542, -0.06023409962654114, -0.0145260039716959, -0.0024924965109676123, 0.036901768296957016, 0.022326458245515823, 0.0037695621140301228, 0.01589675433933735, -0.016054537147283554, -0.03723705932497978, 0.033391073346138, -0.030393173918128014, -0.008352698758244514, -0.0016974123427644372, -0.006454358343034983, 0.02254341170191765, -0.032187968492507935, 0.014082236215472221, 0.000655174080748111, 7.642670243512839e-05, -0.01446683518588543, 0.0107687683776021, 0.0552244558930397, -0.003929811529815197, 0.03078763373196125, 0.001715902704745531, -0.05096428096294403, 0.046033527702093124, -0.030767910182476044, 0.022129228338599205, -0.05408051982522011, -0.020531663671135902, -0.013253869488835335, 0.04670410975813866, 0.05889293551445007, 0.000618809717707336, -0.022247565910220146, -0.030334003269672394, 0.054041072726249695, 0.005759121850132942, 0.01284954696893692, 0.03041289560496807, 0.006784718483686447, 0.009422672912478447, -0.006296574138104916, -0.00589718297123909, 0.005971143953502178, 0.023490116000175476, -0.08567679673433304, 0.010985721834003925, -0.068241648375988, 0.012652317062020302, -0.026902198791503906, 0.013480683788657188, -0.019328558817505836, -0.055500578135252, -0.02384513057768345, -0.0653226375579834, -0.022306734696030617, 0.004595463629812002, -0.04737469181418419, 0.014003343880176544, -0.027355827391147614, -0.030452342703938484, -0.0026478152722120285, -0.03980105370283127, -0.10965998470783234, 0.0004027810355182737, 0.04840029031038284, 0.04903142526745796, 0.021202245727181435, -0.005152639001607895, -0.0413394495844841, 0.0019254598300904036, -0.02066972479224205, 0.02849976345896721, -0.03447583690285683, -0.009555802680552006, 0.012159241363406181, 0.03400248661637306, 0.06942503154277802, -0.006464219652116299, 0.0158277228474617, -0.0017257642466574907, 0.01621232181787491, 0.014348496682941914, -0.003596985712647438, 0.001445943838916719, 0.058695707470178604, -0.03203018382191658, 0.058577366173267365, 0.005270977038890123, 0.0005254335701465607, 0.010196801275014877, 0.005547099281102419, -0.007292586378753185, 0.03731595352292061, -0.04650688171386719, 0.020314710214734077, -0.02552158758044243, -0.004307014402002096, 0.019841358065605164, -0.0037572351284325123, -0.004876516759395599, -0.04836084321141243, 0.0009269819129258394, -0.0077412850223481655, -0.009560734033584595, 0.01757320947945118, 0.0013165115378797054, 0.044850144535303116, -0.017366118729114532, -0.01070959959179163, 0.023115379735827446, 0.0042922222055494785, 0.03989966958761215, -0.012464947998523712, 0.015561462379992008, -0.021813658997416496, 0.05206876993179321, -0.0017035758355632424, 0.004999785218387842, 0.04114221781492233, -0.0055964067578315735, 0.009062726981937885, -0.02191227488219738, -0.02824336476624012, 0.0009639625786803663, 0.02867726981639862, 0.021833382546901703, 0.007164387032389641, 0.014969771727919579, -0.006094412878155708, 0.001429918920621276, 0.001470597693696618, 0.007899069227278233, -0.037828750908374786, -0.04958366975188255, -0.02457488141953945, 0.06007631868124008, -0.029722589999437332, -0.014969771727919579, 0.015226171351969242, 0.011537966318428516, 0.03216824308037758, 0.03905157744884491, -0.04847918078303337, 0.0385979488492012, -0.032128799706697464, -0.007514470256865025, 0.014131543226540089, 0.010265831835567951, 0.005068815778940916, 0.019229944795370102, -0.014447111636400223, 0.007006602827459574, 0.04457402229309082, -0.009531148709356785, -0.017592933028936386, 0.012149379588663578, -0.05486943945288658, 0.019446896389126778, 0.0017812352161854506, 0.005719675682485104, 0.03404192999005318, -0.008175191469490528, -0.034870296716690063, 0.031438492238521576, 0.00022095945314504206, 0.03029455803334713, -0.006789649371057749, -0.01824379339814186, 0.04252282902598381, -0.047335248440504074, 0.04958366975188255, -0.044534578919410706, -0.013737083412706852, 0.002657676814123988, -0.005054023582488298, 0.03552115708589554, 0.02899283915758133, 0.060549668967723846, 0.01378639042377472, -0.052423786371946335, 0.05214766412973404, -0.03370663896203041, 0.039879944175481796, 0.0034589245915412903, 0.010325000621378422, 0.03191184625029564, 0.10808214545249939, 0.008929597213864326, 0.014733095653355122, 0.04954422637820244, -0.010403892956674099, 0.014456973411142826, 0.005093469750136137, -0.02644856832921505, 0.04694078490138054, 0.006094412878155708, 0.0010853824205696583, -0.011498520150780678, 0.06260086596012115, 0.019397590309381485, -0.00381640437990427, 0.006227543577551842, 0.024969343096017838, -0.06133859232068062, -0.007420786190778017, 0.06914890557527542, 0.018332546576857567, 0.005556960590183735, 0.015216309577226639, -0.011823950335383415, 0.010630707256495953, 0.008204775862395763, -0.0016801547026261687, 0.03601423278450966, -0.0011494822101667523, 0.003148287069052458, 0.0145260039716959, 0.007184109650552273, -0.00869292113929987, 0.04492903873324394, 0.015255755744874477, 0.048203058540821075, -0.03345024213194847, -0.030195942148566246, -0.056328944861888885, 0.009200789034366608, 0.056328944861888885, 0.023628177121281624, 0.037158168852329254, -0.044219009578228, 0.0037424429319798946, -0.01850019209086895, -0.00810123048722744, 0.025482140481472015, 0.049504779279232025, 0.04587574303150177, -0.019210221245884895, 0.012287440709769726, -0.0009029444772750139, 0.032385196536779404, 0.04208892211318016, 0.012553702108561993, 0.04792693629860878, -0.032819103449583054, -0.015660077333450317, -0.017297087237238884, -0.024791834875941277, -0.006607211660593748, 0.026093555614352226, 0.06315311044454575, -0.037710413336753845, -0.018145177513360977, -0.031754061579704285, 0.007332032546401024, -0.017533764243125916, 0.006903056986629963, -0.03607340157032013, 0.022484242916107178, -0.023864854127168655, 0.018549500033259392, 0.02656690776348114, -0.0012980211758986115, 0.01027569267898798, -0.053725503385066986, 0.01269176322966814, -0.015975646674633026, 0.005473137833178043, 0.013372207060456276, 0.04579685255885124, -0.04867641255259514, 0.02335205487906933, 0.013115808367729187, 0.0051674311980605125, -0.025284910574555397, 0.0062078204937279224, 0.009052866138517857, 0.06536208838224411, -0.01855936087667942, 0.06240363419055939, 0.007046048529446125, -0.002620696322992444, -0.01998928003013134, 0.006607211660593748, 0.05763066187500954, -0.007884277030825615, -0.04122111201286316, -0.016675813123583794, -0.0250087883323431, 0.051240403205156326, -0.006829095538705587, 0.015009217895567417, 0.0009892326779663563, 0.03737512230873108, -0.0011772176949307323, 0.018174761906266212, -0.018608668819069862, 0.06354756653308868, -0.01169575098901987, -0.017987392842769623, 0.037769582122564316, 0.026093555614352226, 0.01838185451924801, 0.026981091126799583, 0.023805685341358185, -0.017849331721663475, -0.008219568058848381, -0.03668481484055519, -0.011823950335383415, -0.01656733639538288, 0.010749045759439468, 0.044731806963682175, 0.011084336787462234, 0.007716631516814232, 0.03240492194890976, 0.018352270126342773, 0.02414097636938095, 0.012958023697137833, -0.048518627882003784, -0.0176323801279068, 0.004102387931197882, 0.003860780969262123, 0.0024937293492257595, 0.02278008684515953, -0.01675470545887947, -0.029900098219513893, 0.003177871461957693, 0.02761222794651985, 0.019170774146914482, 0.05889293551445007, -0.009353642351925373, -0.037217337638139725, -0.030491787940263748, 0.016182737424969673, -0.04619131237268448, 0.01670539751648903, -0.01855936087667942, -0.0024099063593894243, 0.02136003039777279, 0.019644128158688545, -0.038696564733982086, 0.04201003164052963, -0.05423830449581146, 0.012435363605618477, -0.05455387011170387, -0.03404192999005318, 0.050372593104839325, -0.07538138329982758, 0.003330725012347102, -0.018224069848656654, -0.0029658491257578135, -0.02873643860220909, -0.019594820216298103, -0.021202245727181435, 0.011784504167735577, 0.007736354600638151, 0.021261414512991905, 0.0158277228474617, -0.010433477349579334, 0.031635724008083344, -0.007637739181518555, -0.005542168393731117, 0.031063755974173546, 0.03496891260147095, 0.005478068720549345, 0.021754490211606026, 0.0034811128862202168, 0.05853792279958725, 0.028460316359996796, -0.00903314258903265, 0.04784804582595825, 0.03313467279076576, -0.015354370698332787, 0.06283754110336304, 0.05605282261967659, -0.03741456940770149, -0.0552244558930397, -0.020314710214734077, 0.0008351465803571045, -0.018086008727550507, 0.04903142526745796, -0.0033824979327619076, 0.002107897773385048, 0.007608154788613319, 0.06595377624034882, -0.009043004363775253, -0.02173476666212082, -0.009141619317233562, -0.01849033124744892, 0.017030827701091766, 0.07262016087770462, 0.016734981909394264, -0.02668524533510208, -0.021872827783226967, -0.005310423206537962, -0.01830296218395233, 0.030373450368642807, -0.006232473999261856, 0.049504779279232025, 0.017287226393818855, 0.037394843995571136, -0.009506495669484138, -0.014042790047824383, 0.0001390627003274858, -0.006079620681703091, -0.003308536484837532, -0.05605282261967659, 0.014082236215472221, -0.026054108515381813, -0.0002696506562642753, 0.03589589521288872, -0.0011852302122861147, -0.05451442673802376, -0.028696993365883827, -0.02650773897767067, 0.01393431331962347, -0.025067957118153572, 0.013470822013914585, 0.008658405393362045, -1.000596512312768e-05, -0.03743429109454155, 0.005344938486814499, -0.0014212901005521417, 0.06989838182926178, 0.0032518329098820686, -0.013559576123952866, 0.014979633502662182, 0.00936843454837799, -0.03611284866929054, 0.00962483324110508, 0.028716716915369034, -0.02954508364200592, 0.026409123092889786, -0.0439428873360157, 0.001821913872845471, -0.01160699687898159, -0.029091453179717064, -0.0034885089844465256, -0.01600523106753826, 0.0048173475079238415, -0.038952965289354324, 0.04816361516714096, 0.014161127619445324, -0.018776314333081245, 0.0650465190410614, 0.026862751692533493, -0.030748186632990837, 0.00015085027553141117, 0.03173433989286423, -0.004662028513848782, 0.018480468541383743, -0.01502894051373005, 0.02173476666212082, -0.013234145939350128, 0.0030274835880845785, 0.005670368205755949, 0.005113192833960056, 0.0038533848710358143, 0.006947433575987816, -0.012563562951982021, 0.021636152639985085, -0.01195214968174696, 0.005093469750136137, -0.01819448545575142, 0.003158148378133774, -0.016409551724791527, -0.04414011910557747, -0.054474979639053345, 0.020156925544142723, 0.009541010484099388, -0.004597928840667009, 0.01569952256977558, -0.010887106880545616, -0.06800497323274612, -0.017987392842769623, -0.0099354712292552, -0.06658491492271423, -0.07364575564861298, 0.037828750908374786, -0.03656647726893425, 0.034416668117046356, -0.016222182661294937, -0.04252282902598381, 0.006405050400644541, -0.03743429109454155, 0.02291814796626568, -0.018519915640354156, 0.029702866449952126, 0.004107318818569183, -0.029091453179717064, -0.00583308283239603, 0.009831924922764301, 0.03133987635374069, 0.00538438418880105, 0.025837155058979988, 0.03341079503297806, 0.0250087883323431, 0.010167216882109642, 0.01621232181787491, 0.0051772925071418285, -0.02420014515519142, 0.004378510173410177, -0.03291771933436394, 0.03842044249176979, -0.005093469750136137, 0.030334003269672394, 0.005487930029630661, -0.007608154788613319, -0.01850019209086895, 0.005759121850132942, -0.025916047394275665, 0.01981177367269993, 0.017168888822197914, -0.011764781549572945, -0.0032247137278318405, 0.009304334409534931, -0.003118702443316579, 0.00265274615958333, 0.007489816751331091, 0.014693649485707283, 0.017790162935853004, -0.013569436967372894, -0.031497661024332047, 0.007006602827459574, -0.03638897091150284, 0.021083908155560493, -0.028203917667269707, 0.005625991150736809, 0.00015000280109234154, 0.04287784546613693, 0.028341978788375854, -0.029959267005324364, 0.014595034532248974, -0.009052866138517857, -0.013963897712528706, 0.001640708651393652, 0.01930883526802063, -0.05127985030412674, -0.041694462299346924, -0.02806585654616356, 0.03066929616034031, -0.021498091518878937, -0.016991380602121353, 0.001473063020966947, -0.017050551250576973, 0.005966213531792164, -0.020748617127537727, -0.000545156595762819, -0.009614972397685051, -0.0008505551959387958, 0.01269176322966814, -0.007608154788613319, 0.024377651512622833, 0.02191227488219738, 0.03903185576200485, -0.008658405393362045, 0.027237489819526672, -0.010739183984696865, -0.009417741559445858, -0.0026798653416335583, 0.007401063106954098, -0.06051022186875343, 0.003897761693224311, 0.019170774146914482, -4.7651123168179765e-05, 0.013480683788657188, 0.023608453571796417, -0.008328044787049294, 0.0003784354485105723, 0.024614328518509865, -0.019407451152801514, 0.04623075947165489, 0.007080563809722662, -0.036901768296957016, 0.005132915917783976, 0.006917849183082581, -0.01880589872598648, -0.008574582636356354, -0.04611241817474365, -0.020137201994657516, 0.027592504397034645, -0.024535436183214188, -0.029702866449952126, -0.05841958522796631, -0.022267289459705353, -0.030965140089392662, 0.002016678685322404, -0.02122196927666664, 0.02867726981639862, 0.016310937702655792, -0.08346781879663467, -0.004482056014239788, -0.017464734613895416, 0.0029929683078080416, -0.015689661726355553, -0.030195942148566246, -0.021340306848287582, 0.0061486512422561646, 0.056328944861888885, 0.005931698251515627, -0.005270977038890123, 0.016488444060087204, 0.013707498088479042, -0.012425501830875874, 0.004410560242831707, 0.0018441022839397192, -0.04780859872698784, 0.032681044191122055, -0.009038073942065239, 0.04406122490763664, 0.025048235431313515, -0.0005695021827705204, 0.01362860668450594, -0.0021892550867050886, -0.02861810103058815, 0.003158148378133774, -0.03743429109454155, 0.012514255940914154, -0.007770869880914688, 0.018953822553157806, -0.002118991920724511, 0.011537966318428516, -0.008116022683680058, 0.009294472634792328, 0.01213951874524355, -0.031694892793893814, -0.02917034551501274, -0.01055181585252285, -0.047887492924928665, -0.033982761204242706, -0.06177249550819397, -0.0242790374904871, -0.03745401278138161, -0.009718517772853374, 0.0015174398431554437, 0.01718861237168312, -0.019328558817505836, 0.007884277030825615, -0.006143720354884863, -0.00532521540299058, 0.037394843995571136, 0.0003802844730671495, 0.019042575731873512, 0.02167559787631035, 0.009062726981937885, -0.010936413891613483, -0.00681430334225297, -0.006286712363362312, -0.020413324236869812, 0.03816404193639755, -0.02463405206799507, 0.02719804458320141, -0.028322255238890648, -0.008490759879350662, -0.04552073031663895, 0.005665437318384647, -0.013105946592986584, 0.016468720510601997, -0.004393302369862795, 0.019210221245884895, -0.00968893337994814, 0.07214680314064026, 0.007070702500641346, 0.04847918078303337, 0.0006687336135655642, 0.013372207060456276, 0.04469236359000206, -0.02289842627942562, 0.04709856957197189, 0.013668052852153778, -0.019752604886889458, 0.04575740545988083, 0.011577412486076355, -0.033055782318115234, -0.0030866526067256927, 0.008702781982719898, -0.007006602827459574, 0.011725335381925106, 0.01750417985022068, 0.017474595457315445, 0.024417098611593246, 0.010936413891613483, -0.0006564067443832755, -0.02986065112054348, 0.026093555614352226, -0.0211036317050457, 0.04638854041695595, -0.003412082325667143, 0.030077604576945305, 0.01350040640681982, 0.039130471646785736, 0.004681751597672701, -0.006474080961197615, -0.03061012551188469, -0.0018884791061282158, -0.04362731799483299, -0.0015531877288594842, 0.03321356326341629, 0.052857693284749985, 0.005266046151518822, -0.002057357458397746, -0.0018182158237323165, -0.011971873231232166, 1.8307353457203135e-05, -0.01645885966718197, 0.012159241363406181, 0.026369677856564522, 0.044850144535303116, 0.023825407028198242, 0.016409551724791527, 0.009181065484881401, 0.013421515002846718, 0.003991445992141962, -0.009131758473813534, 0.017484456300735474, 0.018105732277035713, -0.018322685733437538, 0.021379753947257996, -0.035915616899728775, -0.00572953699156642, 0.022642025724053383, 0.007302448153495789, -0.02899283915758133, 0.003892831038683653, -0.008426659740507603, -0.02347039245069027, -0.004699009470641613, -0.027079705148935318, 0.00220404751598835, 0.024476267397403717, 0.015147279016673565, 0.02451571263372898, -0.023056209087371826, 0.022937871515750885, 0.005226599983870983, 0.014003343880176544, -0.019782189279794693, 0.021872827783226967, 0.022010888904333115, 0.006538181100040674, -0.004940616432577372, -0.02954508364200592, -0.01750417985022068, 0.0006040174630470574, -0.01350040640681982, 0.04536294564604759, 0.009324057027697563, -0.007672254461795092, -0.0010823006741702557, 0.0234112236648798, -0.0061486512422561646, 0.0037843543104827404, -0.02278008684515953, -0.03307550400495529, 0.01511769462376833, 0.007184109650552273, -0.008776743896305561, -0.003604381810873747, 0.0001399872126057744, 0.03749345988035202, 0.0037128583062440157, 0.009531148709356785, 0.005537237506359816, 0.040057454258203506, 0.00013051091809757054, 0.01638983003795147, -0.0002927635796368122, -0.023805685341358185, 0.014141405001282692, -0.046151865273714066, 2.232322185591329e-05, 0.015305062755942345, 0.0053597306832671165, 0.01793808676302433, 0.017928224056959152, -0.022188397124409676, -0.039702437818050385, -0.007672254461795092, -0.04922865703701973, 0.05617116019129753, 0.015926338732242584, 0.03879518061876297, 0.03248381242156029, 0.013155253604054451, 0.013145392760634422, 0.011902842670679092, 0.012149379588663578, 0.0008450081222690642, 0.030689017847180367, -0.003532885806635022, 0.015048664063215256, 0.01260300911962986, -0.01433863490819931, -0.01169575098901987, 0.004805020522326231, 0.01279037818312645, 0.013214423321187496, 0.0404716357588768, 0.0018638253677636385, -0.00940788071602583, 0.0037769582122564316, -0.008683059364557266, -0.028578655794262886, -0.019200358539819717, 0.023430947214365005, 0.005463276524096727, 0.04575740545988083, 0.01681387424468994, 0.02055138535797596, -0.03394331783056259, -0.029880374670028687, 0.011942287907004356, 0.00847103726118803, -0.004703940358012915, -0.0030052950605750084, -0.004928289446979761, -0.02416069805622101, 0.029939543455839157, 0.04504737630486488, -0.017711270600557327, -0.0011926263105124235, -0.036231186240911484, 0.0020585900638252497, -0.01737597957253456, 0.017405563965439796, 0.0032666251063346863, 0.03514641895890236, -0.00868798978626728, -0.04867641255259514, 0.03400248661637306, -0.028381425887346268, 0.011971873231232166, -0.018322685733437538, 0.0035205590538680553, 0.02792779542505741, -0.03569866344332695, -0.0085302060469985, -0.03865711763501167, 0.07301461696624756, 0.001833008136600256, 0.0237070694565773, 0.006030313204973936, -0.0215375367552042, 0.0290520079433918, 0.015127555467188358, 0.031142648309469223, -0.024535436183214188, 0.005241392645984888, 0.024673497304320335, -0.027454443275928497, 0.0397418849170208, 0.011863396503031254, 0.003123633097857237, -0.042049478739500046, 0.00448698690161109, -0.037651244550943375, 0.0038879001513123512, -0.015985507518053055, -0.019012991338968277, -0.026586629450321198, 0.008549928665161133, -0.010788491927087307, 0.011962011456489563, 0.03289799764752388, -0.004708870779722929, 0.007529262453317642, 0.0038706425111740828, 0.013460961170494556, 0.025856878608465195, -0.027414996176958084, 0.021498091518878937, -0.013579298742115498, 0.00993053987622261, -0.009062726981937885, -0.0017454872140660882, 0.007608154788613319, -0.03130043298006058, -0.013638467527925968, -0.01446683518588543, -0.01632079854607582, -0.0016456394223496318, 0.018786175176501274, 0.004597928840667009, 0.015561462379992008, 0.006834026426076889, 0.006922779604792595, 0.014762680046260357, 0.01352999173104763, -0.026488015428185463, -0.0316554456949234, -0.0396038219332695, -0.02991981990635395, 0.03345024213194847, 0.00030478229746222496, -0.021241692826151848, 0.016961796209216118, -0.021083908155560493, 0.013184838928282261, -0.023628177121281624, -0.0002711915294639766, 0.01650816760957241, 0.0002094029914587736, 0.016596920788288116, -0.02668524533510208, 0.028203917667269707, 0.02327316254377365, -0.0006305202841758728, 0.02303648740053177, 0.0020585900638252497, -0.02061055600643158, 0.00681430334225297, 0.014190712943673134, 0.011804226785898209, -0.015038802288472652, 0.00993053987622261, -0.03185267746448517, -0.007149594370275736, 0.008081506937742233, -0.0038731079548597336, 0.04165501892566681, -0.019594820216298103, 0.03157655522227287, -0.032365474849939346, 0.0015137417940422893, 0.013717359863221645, -0.020216094329953194, 0.01378639042377472, 0.019584957510232925, 0.019575096666812897, 0.00572953699156642, -0.04350898042321205, 0.0018773849587887526, 0.009762894362211227, 0.004760643932968378, -0.009738241322338581, -0.00532521540299058, 0.0012172801652923226, -0.030965140089392662, 0.0002714996808208525, 0.002531942678615451, -0.022326458245515823, 0.0014767610700801015, -0.002515917643904686, 0.012672039680182934, 0.041694462299346924, 0.014200573787093163, 0.018796037882566452, -0.023056209087371826, 0.014890879392623901, -0.01225785631686449, 0.012908716686069965, 0.006237404886633158, -0.012750932015478611, -0.004405629355460405, 0.03189212083816528, -0.010985721834003925, 0.012701624073088169, 0.0018342408584430814, 0.019220082089304924, -0.027986964210867882, 0.011873257346451283, 0.005660506431013346, 0.015167001634836197, 0.04946533218026161, 0.008643613196909428, -0.022188397124409676, -0.0202358178794384, 0.029525360092520714, -0.014614757150411606, -0.0022311664652079344, -0.011685889214277267, 0.01498949434608221, 0.021438922733068466, 0.01608412154018879, -0.018076147884130478, 0.00400623818859458, 0.0057541909627616405, -0.021991167217493057, 0.04623075947165489, 0.0005106413154862821, -0.0061387899331748486, -0.003823800478130579, 0.002371693029999733, 0.01749431900680065, -0.0044080950319767, -0.021005015820264816, 0.002561527071520686, 0.016034815460443497, -0.025403248146176338, -0.01328345388174057, -0.027533335611224174, -0.004159091506153345, 0.0026108347810804844, 0.01195214968174696, -0.007785662077367306, -0.010443339124321938, -0.012435363605618477, 0.02408180758357048, 0.011340736411511898, -0.009545941837131977, 0.03644813969731331, -0.009008489549160004, -0.028696993365883827, 0.03918964043259621, -0.026369677856564522, 0.03451528400182724, -0.029900098219513893, 0.015009217895567417, 0.00856472086161375, 0.030077604576945305, 0.005162500310689211, -0.024298759177327156, 0.0202358178794384, -0.04039274528622627, -0.0002824398106895387, 0.004925824236124754, 0.022109504789114, -0.002228701254352927, -0.030965140089392662, -0.002830253215506673, 0.012405779212713242, 0.01583758555352688, 0.025363802909851074, -0.014407665468752384, -0.013993482105433941, -0.027513612061738968, -0.034732237458229065, 0.00681430334225297, -0.043982334434986115, 0.016961796209216118, -0.04402177780866623, 0.006528319325298071, 0.00035408983239904046, -0.03402220830321312, -0.028401147574186325, 0.03201046213507652, -0.021261414512991905, -0.04642798751592636, 0.007307378575205803, -0.02700081281363964, -0.0734090805053711, -0.008900012820959091, -0.032128799706697464, 0.03611284866929054, 0.03177378326654434, -0.018362130969762802, 0.017543625086545944, -0.007494747173041105, 0.042128369212150574, -0.012169103138148785, 0.01645885966718197, 0.014200573787093163, -0.014644341543316841, -0.023115379735827446, -0.02457488141953945, 0.029288683086633682, 0.06212751194834709, -0.018727006390690804, 0.010073532350361347, 0.004775436129420996, 0.0007303680758923292, -0.028401147574186325, 0.021576983854174614, -0.0005867598229087889, -0.021005015820264816, -0.03003815934062004, -0.009614972397685051, 0.00689319521188736, 0.03183295205235481, -0.036093126982450485, -0.030314281582832336, -0.020472494885325432, 0.010985721834003925, -0.00048352216253988445, 0.03139904513955116, 0.03788791969418526, -0.00367094692774117, 0.02104446105659008, -0.05060926824808121, -0.005438622552901506, -0.025679372251033783, -0.001686318195424974, 0.02873643860220909, 0.03394331783056259, 0.00208077859133482, -0.009185995906591415, -0.0014410130679607391, 0.0005405339761637151, 0.003831196576356888, 0.02936757542192936, 0.021931998431682587, 0.031931567937135696, -0.006483942735940218, 0.010344724170863628, 0.00630150455981493, -0.031083479523658752, -0.04816361516714096, -0.024239590391516685, -0.015433263033628464, 0.018016979098320007, -0.008653474971652031, 0.00572953699156642, -0.0024234659504145384, -0.016616644337773323, 0.03573811054229736, -0.026981091126799583, -0.011104060336947441, 0.017898639664053917, -0.06051022186875343, 0.007258071098476648, 0.005221669562160969, 0.03798653557896614, -0.011478797532618046, 0.012208549305796623, -0.04343008995056152, 0.035797279328107834, -0.0020672190003097057, 0.03161599859595299, 0.011380182579159737, -0.010887106880545616, -3.64606203220319e-05, 0.017543625086545944, -0.009176135063171387, 0.0073714787140488625, 0.04698023200035095, -0.014713372103869915, -0.007026325911283493, -0.006237404886633158, 0.031694892793893814, 0.00813574530184269, 0.03806542605161667, 0.010689876973628998, -0.0075884317047894, -0.0010841496987268329, -0.0023495047353208065, -0.004925824236124754, 0.024121252819895744, 0.051989879459142685, 0.00426510302349925, 0.0007389968959614635, 0.012958023697137833, -0.010512369684875011, -0.011084336787462234, 0.010108047164976597, 0.017849331721663475, -0.04753247648477554, -0.008431591093540192, 0.01558118499815464, -0.04009689763188362, 0.006705826614052057, 0.009940401650965214, 0.015492431819438934, -0.019101744517683983, -0.005231530871242285, 0.04358787462115288, -0.04339064285159111, 0.0006132626440376043, -0.025797709822654724, -0.0002147960040019825, -0.005847875494509935, 0.013815974816679955, -0.027592504397034645, 0.03153710812330246, -0.008988765999674797, -0.009215581230819225, 0.0005334460292942822, 0.038952965289354324, 0.02508768066763878, 0.03611284866929054, 0.03617201745510101, 0.018263516947627068, 0.006449427455663681, -0.010453199967741966, 0.010127770714461803, -0.009659348987042904, 0.03550143539905548, 0.007415855303406715, 0.009866440668702126, -0.012218410149216652, -0.05644728243350983, -0.038716286420822144, -0.018529776483774185, 0.008860566653311253, -0.0032962097320705652, -0.006888264324516058, -0.02633023075759411, -0.022503964602947235, -0.007908931002020836, 0.01645885966718197, 0.04966256394982338, -0.02179393731057644, -0.029328130185604095, 0.039820775389671326, 0.012188825756311417, 0.018835483118891716, 0.018539639189839363, -0.014170989394187927, -0.01238605659455061, 0.028637824580073357, 0.029584528878331184, -0.009250096045434475, 0.0032986749429255724, 0.002257053041830659, 0.05920850485563278, -0.004639840219169855, 0.017464734613895416, 0.012178964912891388, 0.00681430334225297, -0.012721347622573376, 0.002522081136703491, -0.03459417447447777, 0.014565450139343739, 0.012593148276209831, -0.025344079360365868, -7.203524728538468e-05, 0.04130000248551369, -0.018342407420277596, 0.011390043422579765, 0.0021436456590890884, 0.008663336746394634, -0.028894223272800446, 0.008791536092758179, 0.018026839941740036, -0.017543625086545944, -0.003076791064813733, -0.02295759506523609, -0.007785662077367306, -0.0012068022042512894, -0.011429489590227604, -0.020255541428923607, -0.01254384033381939, -0.009965055622160435, -0.0030718601774424314, -0.04761137068271637, 0.043114520609378815, -0.003863246412947774, 0.01813531666994095, -0.001974767306819558, -0.014141405001282692, -0.02416069805622101, 0.048755303025245667, -0.04406122490763664, 0.008712643757462502, 0.03201046213507652, 0.0004422270867507905, 0.02719804458320141, -0.008185053244233131, 0.0016456394223496318, -0.0034219438675791025, 0.03041289560496807, 0.0049677356146276, 0.04232560098171234, 0.0224053505808115, 0.012741070240736008, -0.0215375367552042, 0.00589718297123909, 0.004842001479119062, -0.019723018631339073, 0.03232602775096893, 0.03496891260147095, 0.016804013401269913, 0.004198537673801184, -0.008520344272255898, -0.004839535802602768, 0.011646443046629429, 0.034258883446455, 0.0013029519468545914, 0.007795523386448622, -0.010285554453730583, -0.02116280049085617, -0.014742957428097725, 0.0011568783083930612, 0.0031704753637313843, -0.028322255238890648, 0.03587617352604866, 0.019220082089304924, -0.023628177121281624, 0.013677913695573807, 0.05041203647851944, 0.002160903299227357, 0.02911117672920227, 0.0048173475079238415, -0.01627149060368538, -0.01536423247307539, 0.018095869570970535, 0.019259529188275337, 0.00936843454837799, -0.03702010586857796, 0.022760365158319473, 0.0020055845379829407, 0.015137417241930962, 0.025718817487359047, 0.00018675233877729625, 0.030807357281446457, 0.020945847034454346, -0.0055964067578315735, -0.034613899886608124, -0.05964241176843643, 0.020314710214734077, 0.005911975167691708, -0.021931998431682587, -0.07841872423887253, 0.009442395530641079, 0.005310423206537962, 0.0048074861988425255, -0.008170261047780514, -0.0034761822316795588, 0.048084720969200134, 0.016537752002477646, -0.012208549305796623, -0.0017947948072105646, -0.018776314333081245, 0.0011519476538524032, -0.01849033124744892, 0.02589632384479046, -0.00036734124296344817, 0.031438492238521576, 0.024002915248274803, 0.0052019464783370495, 0.03625090792775154, -0.019653989002108574, -0.044810701161623, 0.0032370404805988073, 0.004127041902393103, -0.014604896306991577, -0.010147493332624435, 0.002534407889470458, 0.034613899886608124, 0.028026411309838295, -0.011222397908568382, -0.0015125090721994638, 0.012958023697137833, 0.023569008335471153, -0.017287226393818855, 0.02222784236073494, -0.022148950025439262, -0.03867683932185173, -0.002110362984240055, 0.019841358065605164, 0.044968485832214355, -0.021064184606075287, 0.05163486674427986, -0.04871585965156555, -0.004659563302993774, -0.016034815460443497, -0.01632079854607582, -0.035974785685539246, -0.0024875658564269543, -0.020965570583939552, 0.01937786675989628, -0.010433477349579334, -0.028598377481102943, -0.01458517275750637, -0.021517815068364143, 0.038085151463747025, -0.008357629179954529, 0.00040956083103083074, -0.009851648472249508, 0.023924022912979126, 0.037335675209760666, 0.0011710543185472488, 0.019959695637226105, -0.005482999607920647, -0.02936757542192936, -0.04871585965156555, 0.003340586321428418, 0.008387213572859764, 0.000999094219878316, -0.05281824618577957, 0.013066500425338745, -0.02327316254377365, -0.003727650735527277, 0.006528319325298071, -0.010965999215841293, -0.021458644419908524, -0.05171375721693039, 0.023490116000175476, 0.009634695015847683, -0.0035131629556417465, -0.039879944175481796, -0.02272091805934906, 0.014328774064779282, 0.0047211977653205395, 0.0024394909851253033, -0.027986964210867882, 0.008988765999674797, 0.0036808084696531296, -0.0158277228474617, -0.013431375846266747, -0.03573811054229736, 0.016291214153170586, 0.01650816760957241, 0.027178321033716202, -0.016429275274276733, 0.009462118148803711, 0.002276776125654578, 0.019525788724422455, -0.009639625437557697, -0.045599620789289474, -0.003315932583063841, -0.04272006079554558, -0.023549284785985947, -0.011508381925523281, -0.0029042146634310484, -0.009225442074239254, 0.022089781239628792, -0.015009217895567417, 0.005216738674789667, 0.011784504167735577, -0.013589160516858101, -0.004457402508705854, 0.015157140791416168, -0.03210907429456711, -0.044416241347789764, 0.033174119889736176, 0.009358572773635387, 0.005399176385253668, 0.00242839683778584, 0.022109504789114, 0.015551600605249405, -0.0034219438675791025, -0.003720254637300968, 0.011498520150780678, -0.008042060770094395, -0.008402006700634956, -0.00669103441759944, 0.00872743595391512, 0.008525275625288486, 0.004800089634954929, 0.0063656046986579895, 0.020334433764219284, -0.010719461366534233, 0.03518586605787277, 0.04926810413599014, 0.011685889214277267, 0.0057788449339568615, -0.006942502688616514, -0.008643613196909428, -0.026803582906723022, 0.010384169407188892, 0.010492646135389805, -0.006676242221146822, -0.00065763940801844, 0.00048383031389676034, 0.01689276657998562, -0.005601337645202875, -0.001108803553506732, 0.0026773998979479074, -0.042128369212150574, -0.000389529624953866, -0.012593148276209831, -0.004390837159007788, 0.028480039909482002, -0.0015778415836393833, -0.022997040301561356, -0.019269390031695366, -0.022267289459705353, 0.022267289459705353, -0.010265831835567951, 0.008924665860831738, -0.021438922733068466, 0.012642455287277699, -0.033233288675546646, -0.00952621828764677, -0.018115593120455742, -0.010048878379166126, 0.038834623992443085, 0.013579298742115498, 0.0002493112988304347, 0.03084680251777172, -0.029190069064497948, 0.01757320947945118, 0.007864554412662983, 0.020255541428923607, -0.022760365158319473, -0.005719675682485104, 0.051871541887521744, -0.005907044280320406, 0.014733095653355122, -0.00797303020954132, 0.0018564291531220078, -0.015877030789852142, -0.009299403987824917, 0.0015457916306331754, -0.033923592418432236, -0.008988765999674797, -0.005714744795113802, 0.017740856856107712, 0.020511940121650696, 0.010265831835567951, -0.03836127370595932, 0.050254251807928085, 0.025679372251033783, 0.005197015590965748, -0.002975710667669773, -0.020886678248643875, -0.014407665468752384, -0.03769069164991379, 0.02254341170191765, 0.044534578919410706, 0.005098400637507439, 0.0010687410831451416, -0.04260172322392464, -0.044219009578228, -0.01781974732875824, 0.009250096045434475, 0.012336748652160168, 0.003937207628041506, 0.008993696421384811, 0.020630277693271637, 0.058932382613420486, -0.012612870894372463, -0.02861810103058815]\n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
            "[-0.020890237763524055, -0.011455725878477097, -0.031974319368600845, -0.0029226120095700026, -0.023902500048279762, -0.004136971198022366, -0.00789577979594469, 0.05419464781880379, -0.03964189812541008, -0.04514481872320175, 0.006937332451343536, -0.009767033159732819, 0.012538054026663303, 0.009264989756047726, 0.022168166935443878, -0.011247084476053715, 0.008456503972411156, 0.010555959306657314, -0.011038443073630333, 0.02387641929090023, 0.02516738884150982, -0.03455625846982002, -0.022089926525950432, -0.007615417242050171, 0.021842164918780327, -0.01965142786502838, -0.0039576697163283825, 0.024293702095746994, 0.031139753758907318, -0.04154575243592262, 0.01813877746462822, -0.03325225040316582, -0.017851894721388817, -0.005799583625048399, 0.025610752403736115, 0.03677307814359665, -0.012009930796921253, 0.02113799937069416, -0.03528650477528572, 0.006050605792552233, 0.004880256950855255, 0.010529879480600357, 0.030722470954060555, -0.011710008606314659, 0.03129623457789421, -0.036094993352890015, -0.013405220583081245, 0.047179076820611954, 0.012420693412423134, -0.0018044236348941922, -0.04415377229452133, -0.01824309676885605, -0.0012412543874233961, 0.047048673033714294, 0.02592371590435505, -0.028244853019714355, -0.005255159456282854, -0.011168844066560268, 0.058784764260053635, 0.03166135773062706, 0.013268300332129002, 0.003093763254582882, 0.04238031804561615, 0.01206861063838005, 0.011683927848935127, 0.02387641929090023, 0.0017164029413834214, -0.0022657171357423067, -0.05633322522044182, -0.01744765229523182, -0.06864307820796967, 0.02232464775443077, -0.033956415951251984, -0.013613862916827202, -0.0032795846927911043, -0.037946686148643494, 0.008847706951200962, -0.06337487697601318, -0.015569876879453659, -0.003706648014485836, -0.023393936455249786, 0.00638964818790555, -0.041597913950681686, -0.04449281468987465, 0.006099505815654993, -0.0623316690325737, -0.10797201097011566, -0.023967700079083443, 0.0364861935377121, 0.05096070468425751, -0.010797201655805111, -0.018164856359362602, -0.0370599590241909, -0.0050856382586061954, 0.015948040410876274, 0.0039511495269834995, -0.023993780836462975, -0.0220638457685709, 0.001343945157714188, 0.050830304622650146, 0.05909772589802742, 0.010934121906757355, 0.019116783514618874, -0.037346839904785156, 0.03703387826681137, -0.006741730961948633, 0.01284449640661478, 0.002964992308989167, 0.06332271546125412, -0.01787797547876835, 0.10233869403600693, 0.004387992899864912, -0.0008549414342269301, 0.03987662121653557, -0.007856659591197968, 0.019168945029377937, 0.05899340286850929, -0.047831080853939056, 0.046683549880981445, 0.008091380819678307, 0.019860070198774338, 0.026367079466581345, 0.01995135098695755, -0.046970434486866, -0.03194824233651161, -0.03207864239811897, 0.042588960379362106, 0.0007783308392390609, 0.008612984791398048, -0.0007811833638697863, 0.030748551711440086, 0.007211174350231886, -0.017199888825416565, 0.019599268212914467, -0.019664468243718147, 0.006911252159625292, -0.05085638165473938, -0.0065200491808354855, -0.018947262316942215, 0.04871780797839165, -0.009180229157209396, -0.014813551679253578, 0.021555282175540924, -0.0023406976833939552, -0.005584422033280134, -0.047439876943826675, 0.006715650670230389, 0.012609775178134441, 0.04597938805818558, -0.04848308488726616, 0.03518218547105789, -0.026627881452441216, -0.01619580201804638, -0.00689169205725193, 0.008000100031495094, -0.03439977765083313, -0.047909319400787354, -0.05795019492506981, -0.005411640740931034, 0.03914637491106987, 0.025767233222723007, -0.045588184148073196, 0.015569876879453659, 0.02189432457089424, 0.005212779156863689, 0.02806229144334793, -0.011540486477315426, 0.02967926301062107, -0.057637233287096024, 0.001242884318344295, -0.016573965549468994, 0.027488527819514275, -0.0011247084476053715, 0.013137899339199066, -0.02225944772362709, 0.008169621229171753, 0.018125737085938454, -0.032704565674066544, -0.013118338771164417, 0.012199011631309986, -0.03382601588964462, 0.011520926840603352, 0.030957194045186043, -0.009721392765641212, 0.024724025279283524, -0.024789227172732353, -0.04639666900038719, 0.03627755492925644, 0.00621686689555645, 0.010445118881762028, 0.009630112908780575, -0.012283772230148315, 0.01712164841592312, -0.037477243691682816, 0.04720515385270119, -0.03377385437488556, 0.009988714940845966, 0.027462447062134743, 0.00045110590872354805, 0.03479098156094551, -0.003954409621655941, 0.02469794638454914, 0.013294380158185959, -0.026432279497385025, 0.053047120571136475, -0.036616597324609756, -0.013118338771164417, 0.019142864271998405, 0.05409032851457596, 0.01790405437350273, 0.10233869403600693, -0.014565790072083473, 0.030618149787187576, 0.006092986091971397, -0.037112120538949966, -0.0005888419109396636, -0.03231336176395416, -0.01319005899131298, 0.019873110577464104, 0.014318027533590794, -0.029444541782140732, 0.002451538573950529, 0.06885171681642532, 0.05445545166730881, 0.06984276324510574, -0.006503749173134565, 0.005366000346839428, -0.04436241462826729, -0.0035208265762776136, 0.02549339272081852, 0.025076108053326607, 0.023602576926350594, -0.011781728826463223, -0.007843619212508202, -0.019247185438871384, 0.022728892043232918, -0.0322612039744854, 0.0013447601813822985, -0.007061213254928589, 0.015269954688847065, -0.000501228787470609, -0.010986282490193844, -0.025675952434539795, 0.03093111328780651, 0.027697168290615082, 0.04089374840259552, -0.032600246369838715, -0.025675952434539795, -0.023028813302516937, -0.005434460937976837, 0.04151967167854309, 0.02014695107936859, 0.017199888825416565, 0.013757303357124329, 0.006428768392652273, 0.012851016595959663, -0.03166135773062706, 0.005633322522044182, 0.06640017777681351, -0.011827369220554829, -0.016104521229863167, 0.07312887161970139, -0.010203876532614231, 0.024880507960915565, 0.042458560317754745, -0.0011638287687674165, 0.02991398610174656, 0.011044963262975216, -0.02139880135655403, 0.007556736934930086, -0.020355593413114548, 0.004697695374488831, 0.026197556406259537, 0.07271158695220947, -0.035964589565992355, -0.021294480189681053, -0.012101211585104465, 0.014461468905210495, -0.02012087218463421, 0.04511873796582222, -0.04884820803999901, -0.00366100762039423, -0.018947262316942215, 0.009010707959532738, -0.010021315887570381, 0.01662612520158291, 0.03241768479347229, 0.005124758463352919, 0.04076334834098816, -0.00781753845512867, 0.01134488545358181, -0.008684705011546612, 0.07020788639783859, -0.0031817839480936527, 0.029392382130026817, -0.0037979285698384047, 0.024411063641309738, -0.0039804899133741856, 0.061914388090372086, 0.0017147728940472007, 0.07844923436641693, -0.0075371768325567245, 0.021490082144737244, 0.00026365448138676584, -0.031843919306993484, -0.02242896892130375, 0.02021215297281742, 0.014448428526520729, -0.0016430523246526718, -0.03030518814921379, -0.031113674864172935, -0.01632620207965374, 0.021750884130597115, -0.015843719244003296, 0.016743486747145653, -0.025036988779902458, 0.026080196723341942, -0.020042631775140762, 0.02743636630475521, -0.016508763656020164, 0.04436241462826729, 0.0017424831166863441, -0.019964389503002167, 0.026210596784949303, 0.025349950417876244, 0.005933244712650776, 0.03781628608703613, -0.03205256164073944, 0.02225944772362709, -0.027358125895261765, -0.03257416561245918, -0.0304355900734663, -0.005962585099041462, -0.00433583278208971, 0.055290017277002335, 0.010582040064036846, 0.017565011978149414, 0.04678787291049957, 0.05062166228890419, 0.045562103390693665, -7.136397471185774e-05, -0.05445545166730881, -0.01833437755703926, -0.007517616730183363, 0.016769565641880035, -0.007067733444273472, 0.026041075587272644, 0.0025688994210213423, -0.04498833790421486, -0.03481706231832504, 0.05836747959256172, 0.00789577979594469, 0.054559770971536636, 0.005212779156863689, -0.0364861935377121, -0.004146751016378403, 0.00014517296222038567, -0.01645660400390625, 0.016274042427539825, -0.016899967566132545, -0.02394162118434906, 0.023993780836462975, 0.008260902017354965, -0.04853524640202522, 0.01783885434269905, -0.022168166935443878, 0.021255360916256905, -0.03314793109893799, -0.011527447029948235, 0.00016636312648188323, -0.08684705197811127, -0.007243774365633726, -0.024958748370409012, -0.005881084129214287, -0.013731223531067371, -0.0013300899881869555, -0.013679062947630882, 0.02282017096877098, 0.03526042401790619, 0.039694059640169144, 0.02588459476828575, -0.031139753758907318, 0.02730596624314785, -0.005202999338507652, 0.009773553349077702, 0.00944755133241415, 0.003354565240442753, 0.01131228543817997, 0.018386539071798325, -0.03528650477528572, 0.029209820553660393, 0.04746595770120621, 0.03854652866721153, 0.018803821876645088, 0.016808686777949333, -0.026640919968485832, 0.06780850887298584, 0.01513955369591713, -0.039589736610651016, -0.032600246369838715, 0.013848584145307541, -0.00460967468097806, -0.029574941843748093, 0.04149359092116356, -0.009819193743169308, 0.04355392977595329, -0.0131248589605093, 0.042928002774715424, -0.013809463940560818, 0.009669233113527298, -0.023067934438586235, -0.012355493381619453, 0.03679915890097618, 0.026093237102031708, 0.02743636630475521, -0.005463801324367523, -0.004123931284993887, 0.011162323877215385, -0.02806229144334793, 0.03966797888278961, -0.022794092074036598, 0.048561327159404755, 0.01200341060757637, 0.020655514672398567, -0.0356777086853981, -0.006164706312119961, 0.022076886147260666, -0.01421370729804039, -0.04086766764521599, -0.05398600548505783, 0.03374777361750603, -0.03116583451628685, -0.005222559440881014, 0.00933019071817398, -0.013561702333390713, -0.02644531987607479, -0.007824058644473553, 0.014539709314703941, 0.022963613271713257, -0.04316272586584091, 0.0072307344526052475, 0.00350452633574605, -0.012622814625501633, -0.027879729866981506, 0.0021190159022808075, 0.005157358944416046, 0.05570729821920395, 0.0024254582822322845, -0.01774757355451584, 0.022050805389881134, 0.02357649803161621, -0.013561702333390713, 0.02443714439868927, 0.022089926525950432, -0.029444541782140732, 0.022572409361600876, -0.028844697400927544, -0.023732978850603104, 0.003983750008046627, -0.011964290402829647, -0.008567344397306442, -0.022155126556754112, 0.0006458923453465104, -0.040945909917354584, 0.04947413131594658, 0.047831080853939056, -0.03625147417187691, 0.08465632051229477, 0.014865712262690067, 0.002203776501119137, -0.004890036769211292, 0.022676730528473854, -0.032965369522571564, 0.03539082780480385, -0.02077287621796131, 0.0255585927516222, 0.005082378163933754, 0.005985405296087265, -0.04584898427128792, 0.014696191065013409, 0.010399478487670422, -0.0030562730971723795, 0.030018305405974388, -0.003077463246881962, -0.048561327159404755, 0.0008337512845173478, -0.054664090275764465, 0.0024825087748467922, -0.032600246369838715, -0.0069242920726537704, -0.05171702802181244, 0.036460112780332565, -0.018386539071798325, -0.0134704215452075, 0.02318529412150383, 0.0003541201585903764, -0.026732200756669044, -0.0059821452014148235, -0.02295057289302349, -0.024215461686253548, -0.0336434543132782, 0.035103943198919296, -0.050178296864032745, 0.0220638457685709, 0.007400255650281906, -0.05899340286850929, -0.054142486304044724, -0.051430147141218185, 0.013392181135714054, -0.015752438455820084, 0.04597938805818558, 0.017004288733005524, -0.024945707991719246, -0.008117461577057838, -0.003233944298699498, 0.03875517100095749, -0.00951275136321783, -0.00813050102442503, 0.022050805389881134, 0.05137798562645912, 0.011696968227624893, -0.00366100762039423, -0.02542819082736969, -0.0385204516351223, -0.006865611765533686, -0.03453018143773079, 0.016247961670160294, 0.007491536438465118, 0.015908919274806976, 0.017486771568655968, 0.018686460331082344, -0.009408431127667427, 0.005202999338507652, -0.0068199713714420795, -0.0049519771710038185, 0.03752940148115158, -0.02855781465768814, 0.026627881452441216, 0.013666022568941116, -0.008619504980742931, 0.005799583625048399, 0.014187626540660858, 0.03690347820520401, -0.0010440228506922722, -0.025532511994242668, -0.022246407344937325, 0.009858313947916031, -0.02648443914949894, 0.0072242142632603645, -0.02357649803161621, -0.014878751710057259, -0.018125737085938454, 0.037086039781570435, 0.002255937084555626, -0.034843143075704575, -0.010901521891355515, 0.005447500851005316, -0.04950021207332611, 0.015361235477030277, 0.01807357557117939, -0.04240639880299568, -0.0546119324862957, -0.014096345752477646, 0.021320560947060585, -0.0069177718833088875, 0.0038044487591832876, -0.010608119890093803, -0.008123980835080147, 0.008710785768926144, -0.03338265046477318, 0.03265240415930748, -0.02394162118434906, -0.002389597939327359, -0.0073806955479085445, 0.003843568963930011, 0.031191915273666382, 0.03726860135793686, 0.05072598159313202, -0.020785916596651077, 0.018516939133405685, -0.026758281514048576, -0.00856082420796156, -0.011397046037018299, 0.013216139748692513, -0.047909319400787354, -0.012622814625501633, 0.02562379278242588, 0.02668004110455513, -0.001550956629216671, 0.02186824381351471, 0.005193219054490328, -0.02447626367211342, 0.0015566616784781218, 0.006330967880785465, 0.013040098361670971, 0.023498257622122765, -0.013170499354600906, 0.02001655101776123, -0.02077287621796131, -0.012525014579296112, -0.005760463420301676, -0.025206509977579117, -0.014487548731267452, -0.006666750181466341, -0.007771898526698351, -0.031009353697299957, -0.04386689141392708, -0.01965142786502838, -0.027227725833654404, -0.020668555051088333, -0.028975097462534904, 0.031113674864172935, 0.005920204799622297, -0.06963412463665009, 0.012140331789851189, 0.004834616556763649, -0.022585449740290642, -0.012257692404091358, -0.009656192734837532, 0.003007372608408332, 0.0013154199114069343, 0.01790405437350273, -0.009115028195083141, 0.0001116558414651081, 0.0028117711190134287, -0.015674198046326637, -0.005838703829795122, -0.002136946190148592, -0.003154073841869831, -0.017056448385119438, 0.020433833822607994, 0.012407653033733368, 0.03192216157913208, -0.003843568963930011, -0.04324096441268921, 0.007941420190036297, 0.018099656328558922, -0.02469794638454914, -0.012648895382881165, -0.021490082144737244, 0.0052714599296450615, -0.020798956975340843, 0.00246131862513721, -0.005894124507904053, 0.016873886808753014, -0.0013357950374484062, 0.027723249047994614, -0.01162524800747633, -0.027332045137882233, -0.017356371507048607, -0.018177896738052368, -0.04772675782442093, -0.008802066557109356, -0.05899340286850929, -0.024945707991719246, -0.017591092735528946, -0.009271509945392609, -0.013379140757024288, 0.010262557305395603, -0.017538931220769882, 0.02107279933989048, 0.002089675748720765, 0.008952027186751366, 0.02581939473748207, -0.015974121168255806, 0.02641923911869526, 0.04785716161131859, 0.01427890732884407, -0.01915590465068817, -0.01886902190744877, 0.012857536785304546, -0.01975574903190136, 0.04436241462826729, -0.0448579378426075, 0.05273415893316269, -0.021790003404021263, -0.007615417242050171, -0.03539082780480385, -0.021620482206344604, -0.008586904965341091, 0.026471398770809174, -0.004821576178073883, -0.002011435106396675, 0.007113373372703791, 0.058889083564281464, 0.020394712686538696, 0.03200040012598038, -0.019299345090985298, 0.020733756944537163, 0.045457784086465836, -0.016560925170779228, 0.030122626572847366, 0.04091982915997505, -0.029940064996480942, 0.02879253774881363, -0.02160744182765484, -0.004612934775650501, -0.005330140236765146, 0.004857436753809452, -0.07255510985851288, 0.01850389875471592, 0.019573187455534935, 0.02456754446029663, 0.014370188117027283, 0.005359480157494545, -0.003468666225671768, -0.011370965279638767, 0.011325324885547161, 0.003478446276858449, 0.019860070198774338, -0.000978007330559194, 0.026028035208582878, 0.014891792088747025, 0.024019861593842506, -0.002373297931626439, 0.015713319182395935, -0.005476841237396002, -0.0013488351833075285, -0.027384206652641296, -0.01757805235683918, 0.02070767618715763, 0.0364079549908638, -0.013346540741622448, 0.012485894374549389, 0.01744765229523182, -0.013137899339199066, 0.029574941843748093, -0.013255259953439236, 0.02954886294901371, 0.014083306305110455, 0.06827795505523682, 0.008006620220839977, -0.006787370890378952, 0.004244551993906498, 0.01589587889611721, 0.014057225547730923, -0.004792236257344484, -0.009362790733575821, 0.003274694550782442, -0.014748350717127323, 0.003361085196956992, -0.029992226511240005, -0.014552749693393707, -0.009767033159732819, -0.016639165580272675, -0.018973343074321747, -0.006187526509165764, -0.014096345752477646, -0.03888557106256485, 0.006614589598029852, -0.02182912454009056, 0.010562479496002197, 0.009467110969126225, 0.011071043089032173, 0.021776964887976646, -0.004850916564464569, 0.00716553395614028, 0.021555282175540924, 0.00015699055802542716, -0.0031426637433469296, 0.02150312252342701, 0.020798956975340843, 0.0056822230108082294, 0.010660280473530293, -0.026236677542328835, -0.024332823231816292, -0.026367079466581345, -0.020629435777664185, 0.013639942742884159, 0.014943952672183514, 0.01019083708524704, -0.003941369708627462, 0.017891013994812965, 0.02730596624314785, -0.013600822538137436, -0.02292449213564396, -0.015817638486623764, 0.03366953507065773, 0.018164856359362602, 0.005124758463352919, 0.04047646373510361, -0.007745818234980106, 0.023798178881406784, -0.021490082144737244, 0.018425658345222473, -0.01899942383170128, 0.049552373588085175, -0.001766933361068368, -0.006533089093863964, 0.0336434543132782, -0.021085839718580246, 0.023602576926350594, -0.011879529803991318, 0.001734333112835884, 0.00479875598102808, 0.014539709314703941, -0.0029633622616529465, 0.04686611145734787, -0.03103543445467949, -0.004661835264414549, -0.01162524800747633, -0.022089926525950432, 0.03231336176395416, 0.005956064909696579, 0.034347619861364365, 0.002112495945766568, 0.013294380158185959, 0.009832234121859074, 0.020525114610791206, 0.02100759744644165, -0.012759735807776451, 0.02743636630475521, 0.008606464602053165, -0.0028525213710963726, 0.017799733206629753, -0.02668004110455513, -0.013639942742884159, -0.024411063641309738, -0.0025216289795935154, 0.027957970276474953, 0.03987662121653557, 0.03241768479347229, 0.00781753845512867, -0.005535521544516087, 0.012648895382881165, -0.041441433131694794, -0.03891165181994438, 0.023211374878883362, 0.010810241103172302, -0.004035910591483116, 0.013613862916827202, 0.029392382130026817, -0.05309927836060524, -0.005851744208484888, 0.009753993712365627, 0.02915765903890133, 0.010582040064036846, 0.003527346532791853, 0.0028443713672459126, -0.0025069587863981724, 0.048065800219774246, 0.014787470921874046, 0.027827570214867592, 0.02704516425728798, -0.024710986763238907, -0.025415152311325073, -0.002570529468357563, 0.00956491194665432, 0.011416605673730373, 0.015478597022593021, -0.005323620047420263, -0.058784764260053635, 0.024515384808182716, -0.01433106791228056, 0.03562554717063904, -0.03017478808760643, -0.011129723861813545, 0.015374275855720043, -0.018516939133405685, -0.007204654160887003, -0.01975574903190136, 0.06619153916835785, 0.0175128523260355, 0.027853649109601974, -0.01949494704604149, 0.0028411115054041147, 0.014983072876930237, -0.01300749834626913, 0.03168743848800659, -0.024619705975055695, -0.0007078328053466976, 0.042197756469249725, -0.025936754420399666, -0.019273264333605766, 0.023041853681206703, -0.007191614247858524, -0.033721692860126495, 0.011370965279638767, -0.026667000725865364, -0.0017392231384292245, 0.011996890418231487, -0.012518494389951229, -0.025584673509001732, 0.030957194045186043, 0.01628708280622959, -0.01053639966994524, 0.007569776847958565, 0.02793188951909542, -0.003765328321605921, 0.011481806635856628, 0.028531735762953758, 0.009832234121859074, 0.002920981962233782, 0.023367855697870255, 0.001033427775837481, 0.0020570755004882812, 0.023902500048279762, -0.004456453491002321, 0.029601022601127625, 0.0072242142632603645, -0.03612107038497925, 0.012635855004191399, -0.019925270229578018, -0.003243724349886179, -0.0043847328051924706, 0.0076414975337684155, 0.007250294554978609, -0.011090603657066822, 0.037477243691682816, 0.0028443713672459126, 0.017617173492908478, 0.004078290890902281, -0.009173708967864513, -0.03839004784822464, -0.04814404249191284, 0.05873260274529457, -0.013913785107433796, -0.006963412277400494, 0.034086816012859344, 0.00573112303391099, 0.0055127013474702835, -0.016508763656020164, -0.004446673672646284, 0.009962635114789009, 0.0006691200542263687, 0.010712441056966782, -0.023263534530997276, 0.030618149787187576, 0.022272488102316856, 0.014865712262690067, 0.003928329795598984, 0.021229280158877373, 0.0124337337911129, -0.004834616556763649, 0.014239787124097347, -0.0011931690387427807, 0.01249893382191658, 0.022898413240909576, -0.025649873539805412, 0.010901521891355515, 0.022168166935443878, 0.012218572199344635, 0.02892293781042099, -0.012505454011261463, 0.04678787291049957, -0.002666700165718794, 0.01866038143634796, 0.015817638486623764, -0.0188951026648283, -0.011005843058228493, 0.013679062947630882, 0.0315309576690197, -0.010738520883023739, -0.04227599874138832, 0.013626902364194393, 0.014839631505310535, 0.029261980205774307, -0.028714295476675034, -0.011299245059490204, -0.015165634453296661, -0.04024174436926842, 0.00872382614761591, 0.010242996737360954, -0.00540186045691371, 0.016482684761285782, 0.007726258132606745, 0.02124232053756714, 0.029731424525380135, 0.03150487691164017, 0.01738245040178299, -0.029105499386787415, 0.019312385469675064, 0.04099806770682335, 0.01833437755703926, 0.020629435777664185, -0.02057727426290512, 0.019168945029377937, 0.019899189472198486, 0.005623542238026857, -0.01065376028418541, 0.0134704215452075, 0.028323093429207802, -0.028349174186587334, 0.008965067565441132, -0.006956892553716898, 0.03614715114235878, 0.04524914175271988, 0.000351267633959651, -0.02618451789021492, -0.03181783854961395, 0.030226947739720345, 0.017434611916542053, -0.0034197657369077206, -0.012994457967579365, 0.0483526848256588, 0.011397046037018299, 0.011357925832271576, -0.03181783854961395, 0.03513002395629883, 0.0031051733531057835, -0.019573187455534935, 0.034712743014097214, -0.011592647060751915, -0.0010187576990574598, -0.0206815954297781, 0.012420693412423134, 0.027592847123742104, -0.006171226501464844, -0.03244376555085182, -0.010053915902972221, 0.027019083499908447, -0.025715073570609093, 0.00428041210398078, -0.016600044444203377, -0.023928580805659294, -0.002518368884921074, 0.03473882004618645, -0.01269453577697277, -0.009271509945392609, 4.760145384352654e-05, -0.027645008638501167, -0.0053986008279025555, -0.002097825752571225, 0.019742708653211594, 0.01671740598976612, -0.02954886294901371, 0.03492138162255287, -0.0124337337911129, 0.039459336549043655, -0.029887905344367027, 0.021281439810991287, 0.01212077122181654, 0.022898413240909576, 0.005007397849112749, -0.01807357557117939, 0.022833211347460747, -0.03228728473186493, -0.007687137927860022, -0.008306542411446571, 0.03565162792801857, 0.012974897399544716, -0.0028199211228638887, -0.028479574248194695, 0.005633322522044182, 0.021555282175540924, 0.022220326587557793, -0.004446673672646284, 0.0036903477739542723, 0.008665145374834538, -0.026993002742528915, -0.008899866603314877, -0.04165007546544075, -5.531854185392149e-05, -0.041337110102176666, -0.0035469066351652145, 0.015504676848649979, -0.025910675525665283, -0.016443563625216484, 0.03930285573005676, 0.019573187455534935, 0.0007542881648987532, 0.02816661261022091, -0.020629435777664185, -0.05998445302248001, 0.001822353689931333, -0.012681495398283005, 0.04571858420968056, 0.02061639539897442, -0.0434756875038147, -0.03257416561245918, -0.004459713585674763, 0.03638187423348427, 0.00045558842248283327, 0.03205256164073944, -0.009604032151401043, 0.007934900000691414, -0.00555834174156189, -0.012251172214746475, 0.038077086210250854, 0.06248814985156059, 0.0027107105124741793, 0.02542819082736969, 0.014291947707533836, 0.004430373199284077, -0.035860270261764526, 0.040346063673496246, -0.008652104996144772, 0.004251072183251381, -0.04462321475148201, 0.02868821658194065, 0.0056398422457277775, 0.006950372364372015, -0.009714873507618904, -0.011051483452320099, -0.03732076287269592, 0.0227549709379673, 0.015100433491170406, 0.029731424525380135, 0.02592371590435505, 0.004733555484563112, 0.00659502949565649, -0.0693211629986763, -0.007119893562048674, -0.00835870299488306, -0.02954886294901371, 0.030096545815467834, 0.010001755319535732, 0.012890136800706387, -0.012824936769902706, 0.01022343710064888, 0.0007714033126831055, 0.005463801324367523, 0.019729668274521828, 0.030644230544567108, 0.0455099418759346, -0.019481906667351723, 0.010203876532614231, -0.002829701406881213, -0.028088372200727463, -0.014943952672183514, -0.04204127565026283, -0.01774757355451584, 0.03739900141954422, -0.013887704350054264, 0.007967500016093254, 0.008925947360694408, -0.00968227256089449, 0.04397121071815491, -0.050908543169498444, -0.0008386413101106882, -0.0006010670331306756, -0.055811621248722076, -0.01837349869310856, 0.025571633130311966, 0.017421571537852287, -0.027358125895261765, 0.006728690583258867, -0.009291069582104683, 0.05497705563902855, 0.0280362106859684, 0.047674600034952164, -0.010053915902972221, 0.018686460331082344, -0.0315309576690197, 0.01850389875471592, 0.0007681432762183249, 0.014435389079153538, 0.10186924785375595, -0.019142864271998405, -0.0046813953667879105, 0.00010951644799206406, 0.053673043847084045, -0.016365323215723038, 0.021581362932920456, 0.007335055153816938, 0.005705042742192745, 0.009088948369026184, 0.028010131791234016, -0.0039935302920639515, 0.003330114996060729, 0.040085263550281525, -0.008880306966602802, -0.005594202317297459, -0.02150312252342701, -0.005623542238026857, -0.02031647227704525, -0.008762946352362633, 0.008449983783066273, -0.04191087558865547, 0.0199904702603817, 0.011351405642926693, -0.025441231206059456, -0.025336910039186478, 0.009082428179681301, 0.003915289416909218, -0.026993002742528915, 0.028244853019714355, 0.05800235643982887, -0.0336434543132782, -0.007674098014831543, 0.003674047766253352, -0.021907364949584007, 0.002446648431941867, 0.004071770701557398, -0.0009674123139120638, 0.015569876879453659, 0.005049778148531914, -0.0073872157372534275, 0.007093813270330429, 0.02364169806241989, -0.017239009961485863, 0.029887905344367027, 0.027957970276474953, 0.008952027186751366, 0.007967500016093254, -0.019221104681491852, 0.02717556431889534, -0.02473706565797329, -0.0036153672263026237, -0.01096672285348177, 0.017395490780472755, -0.001317049958743155, -0.06457456946372986, -0.004472753498703241, 0.010692880488932133, 0.03351305052638054, -0.016378363594412804, 0.009916994720697403, -0.033356569707393646, -0.004805276170372963, 0.003824008861556649, 0.015126514248549938, 0.027201645076274872, -0.017486771568655968, -0.018060535192489624, 0.027984051033854485, 0.027071243152022362, -0.017369410023093224, 4.55639383289963e-05, -0.022089926525950432, -0.008645584806799889, 0.026836521923542023, 0.013059657998383045, 0.001242884318344295, -0.013072698377072811, -0.008684705011546612, 0.05925420671701431, 0.012674975208938122, 0.0025900895707309246, 0.022611530497670174, -0.016339242458343506, 0.004978057462722063, 0.0124337337911129, -0.029261980205774307, 0.02107279933989048, 0.004449933301657438, -0.009780073538422585, 0.006040825508534908, 0.005626802332699299, -0.02516738884150982, 0.014891792088747025, -0.01721292920410633, -0.0015974120469763875, -0.030331268906593323, 0.01632620207965374, 0.017995335161685944, -0.015113473869860172, -0.00941495131701231, 0.017565011978149414, -0.012981417588889599, -0.017056448385119438, -0.02305489405989647, -0.00563006242737174, -0.018686460331082344, 0.003973969724029303, 0.012009930796921253, -0.05362088233232498, 0.048926446586847305, 0.00248576863668859, 0.01800837554037571, -0.008684705011546612, -0.0029128319583833218, -0.0364861935377121, 0.030331268906593323, -0.05137798562645912, 0.03288712725043297, 0.039433255791664124, 0.00925194937735796, 0.041337110102176666, 0.0037262081168591976, -0.0182039774954319, 0.012062090449035168, 0.02245504967868328, 0.018621260300278664, 0.0643659234046936, 0.008091380819678307, 0.023759059607982635, -0.02292449213564396, -0.015804598107933998, -0.0027025602757930756, -0.011951250024139881, 0.022337688133120537, 0.004251072183251381, -0.0160914808511734, 0.0018631040584295988, 0.012857536785304546, -0.02160744182765484, 0.023028813302516937, 0.04097198694944382, -0.04504049941897392, 0.0059821452014148235, -0.016847806051373482, -0.01774757355451584, -0.016547884792089462, 0.001653647399507463, 0.0059821452014148235, -0.012531534768640995, 0.02169872261583805, 0.038963813334703445, -0.00222659669816494, 0.012368532828986645, 0.017004288733005524, 0.006063645705580711, 0.02572811394929886, 0.026432279497385025, -0.027592847123742104, -0.018621260300278664, -0.006275547202676535, 0.009916994720697403, -0.003204604145139456, -0.014161546714603901, 0.01807357557117939, -0.00030216353479772806, -0.002800361020490527, -0.02374601922929287, -0.008541264571249485, 0.039720140397548676, 0.023628657683730125, -0.007954459637403488, -0.025949794799089432, -0.04837876558303833, -0.008189181797206402, -0.0007534731994383037, -0.01712164841592312, -0.0755804106593132, -0.003377385437488556, 0.032730646431446075, 0.016013240441679955, 0.01180780865252018, 0.0016487573739141226, 0.015882840380072594, 0.015191714279353619, 0.0018516939599066973, -0.0068199713714420795, -0.024163302034139633, 0.010132156312465668, -0.03166135773062706, 0.02368081919848919, 0.0025444491766393185, 0.04402337223291397, 0.019142864271998405, 0.0012803745921701193, 0.03690347820520401, -0.026862602680921555, -0.029496701434254646, 0.018060535192489624, 0.014865712262690067, -0.009969155304133892, 0.0009445921168662608, -0.00832610297948122, 0.001222509192302823, -0.007471976336091757, 0.008293502032756805, 0.009284550324082375, 0.019729668274521828, 0.032965369522571564, -0.004355392884463072, 0.023172253742814064, -0.011820849031209946, -0.02318529412150383, -0.002360257785767317, 0.010901521891355515, 0.031139753758907318, 0.003053013002499938, 0.0426672026515007, -0.035599466413259506, -0.00951275136321783, -0.023993780836462975, -0.025936754420399666, -0.034973543137311935, -0.008580384775996208, -0.04350176826119423, -0.012831456959247589, 0.028896857053041458, -0.020433833822607994, -0.002171176252886653, -0.0015680717770010233, 0.0315309576690197, -0.00350452633574605, -0.01300749834626913, 0.0011393785243853927, 0.018490858376026154, 0.029209820553660393, 0.009343230165541172, 0.026210596784949303, -0.014109386131167412, -0.012948817573487759, -0.020003510639071465, -0.028740376234054565, 0.011468766257166862, -0.01427890732884407, -0.029627103358507156, 0.029261980205774307, -0.016078440472483635, -0.009734433144330978, 0.020420793443918228, -0.03215688094496727, -0.02025127224624157, -0.03700779750943184, 0.02021215297281742, 0.003511046525090933, 0.013294380158185959, -0.041311029344797134, -0.0005077488021925092, 0.013229179196059704, -0.037216439843177795, -0.014344108290970325, -0.027227725833654404, -0.007198134437203407, -0.0035534268245100975, 0.0020554454531520605, -0.001436040853150189, -0.009401910938322544, 0.014461468905210495, 0.044440653175115585, 0.03951149806380272, -0.0020635954570025206, -0.015165634453296661, -0.011651327833533287, 0.005881084129214287, -0.018543019890785217, -0.0511954240500927, -0.007106853649020195, -0.026940843090415, -0.0072307344526052475, -0.0234460961073637, 0.006184266414493322, 0.0045542544685304165, 0.018099656328558922, -0.019181983545422554, 0.029522782191634178, -0.01915590465068817, -0.010758081451058388, 0.003085613250732422, -0.008482583798468113, -0.016430523246526718, -0.041180629283189774, 0.025962835177779198, 0.0010692880023270845, 0.017591092735528946, 0.033200088888406754, 0.010764600709080696, -0.003282844787463546, -0.01714772917330265, -0.03505178540945053, 0.005160619039088488, -0.0015574767021462321, 0.02816661261022091, -0.005978885106742382, -0.0011899089440703392, 0.022142086178064346, 0.00913458876311779, 0.006458108779042959, 0.022781051695346832, 0.0011784988455474377, 0.04597938805818558, 0.02130752056837082, 0.0009788223542273045, 0.004061990417540073, -0.005004137754440308, 0.005212779156863689, 0.014291947707533836, 0.013913785107433796, 0.02061639539897442, -0.019116783514618874, 0.006683050189167261, 0.015048272907733917, 0.01952102780342102, -0.0007086478290148079, -0.006565689574927092, -0.0011385636171326041, -0.07182486355304718, -0.016313163563609123, 0.00637334818020463, -0.010523359291255474, 0.028531735762953758, 0.003654487431049347, -0.038442209362983704, -0.024358903989195824, -0.04574466496706009, 0.031009353697299957, -0.003131253644824028, 0.0034099856857210398, -0.010353838093578815, 0.005261679645627737, -0.010112595744431019, -0.0161306019872427, -0.021124958992004395, -0.017982294782996178, 0.018295258283615112, 0.012479374185204506, -0.003559946781024337, 0.016899967566132545, -0.0007864809012971818, 0.004606414586305618, -0.012922737747430801, 0.015113473869860172, 0.021151039749383926, -0.0011540487175807357, 0.036590516567230225, 0.009088948369026184, -0.0008899866952560842, -0.025845475494861603, -0.023198334500193596, 0.010021315887570381, -0.007204654160887003, 0.024280663579702377, -0.03492138162255287, -0.03692955896258354, -0.01671740598976612, -0.020798956975340843, -0.0032518745865672827, 0.004472753498703241, -0.019938310608267784, 0.041311029344797134, -0.001668317592702806, 0.023263534530997276, -0.02338089607656002, 0.008254381828010082, -0.005075858440250158, -0.024541465565562248, 0.010888482443988323, 0.029992226511240005, 0.02476314641535282, -0.009838754311203957, -0.04073726758360863, -0.03340873122215271, -0.039928779006004333, 0.0063798679038882256, 0.0028900117613375187, 0.00841086357831955, 0.007556736934930086, 0.019299345090985298, 0.055915940552949905, -0.026940843090415, -0.012877097353339195]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando a similaridade das duas frases\n",
        "import numpy as np\n",
        "\n",
        "similaridade = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "print(f\"Similaridade: {similaridade*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlgLc4g98g-i",
        "outputId": "e91e1085-ed5c-4c24-df54-2b4b0bd84baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridade: 73.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Armazenamento\n",
        "Agora que temos nossos documentos já representados de uma maneira que podemos fazer o cálculo da similaridade, os embeddings, precisamos manter esses embeddings em algum lugar para que possamos realizar a pesquisa.\n",
        "\n",
        "Precisamos de uma espécie de banco de dados, mas um banco de dados um pouco diferente que chamamos de **banco de dados vetorial (vector database)**.\n",
        "\n",
        "Diferentemente dos bancos de dados tradicionais, onde realizamos a busca por algum item usando comandos SQL, usamos a busca por similaridade nos bancos de dados vetoriais. **É uma funcionalidade já intrínseca da maioria deles, facilitando o processo para nós**.\n",
        "\n",
        "Hoje, temos muitos serviços de bancos de dados vetoriais: Pinecone, Qdrant, Chroma... então, qual deles escolher?\n",
        "\n",
        "Observe abaixo uma tabela que representa uma comparação dos principais bancos de dados vetoriais:\n",
        "\n",
        "<img src=\"https://preview.redd.it/my-strategy-for-picking-a-vector-database-a-side-by-side-v0-dw181oiz8esb1.png?width=1870&format=png&auto=webp&s=1858bcae7f68cd9fa32f0d22822f13b9d406c25d\">\n",
        "\n",
        "[Esse site](https://superlinked.com/vector-db-comparison) também é excelente para realizar comparações de vector dbs.\n",
        "\n",
        "Note que soluções como Pinecone e ElasticSearch não são Open-source, ou seja, de código aberto. Já soluções como Qdrant e Chroma são de código aberto e gratuitos para poder usar.\n",
        "\n",
        "Vamos utilizar o Milvus para esse exemplo:"
      ],
      "metadata": {
        "id": "YeI0Lqfl9Djm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOTG-28XAQkZ",
        "outputId": "2d9b7999-161b-43f9-b9c3-59a2f6a5303a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires grpcio>=1.71.0, but you have grpcio 1.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando o banco de dados vetorial\n",
        "from pymilvus import MilvusClient\n",
        "\n",
        "client = MilvusClient(\"teste_aula.db\")"
      ],
      "metadata": {
        "id": "pqatur0tAWO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma coleção\n",
        "if client.has_collection(collection_name = \"lean_startup\"):\n",
        "    client.drop_collection(collection_name = \"lean_startup\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name = \"lean_startup\",\n",
        "    dimension = 1536,\n",
        ")"
      ],
      "metadata": {
        "id": "n2SmhKl1AtEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando os documentos\n",
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PDFPlumberLoader(\n",
        "    file_path = \"/content/A Startup Enxuta - Eric Ries.pdf\"\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "docs = [x for x in docs if x.page_content.strip() not in (\"\\n\", \"\", \" \", \"\\t\")]\n",
        "\n",
        "# Instanciando o splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
        "docs_splitted = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "xEiBhEgfA-dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando e armazenando os embeddings na coleção\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
        "\n",
        "vectors = embeddings.embed_documents([doc.page_content for doc in docs_splitted])\n",
        "\n",
        "data = [\n",
        "    {\"id\": i, \"vector\": vectors[i], \"text\": docs_splitted[i].page_content, \"metadata\": docs_splitted[i].metadata}\n",
        "    for i in range(len(vectors))\n",
        "]\n",
        "\n",
        "res = client.insert(collection_name=\"lean_startup\", data=data)"
      ],
      "metadata": {
        "id": "-Nw2cpQdAdla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9SZFynPCs9R",
        "outputId": "faa9a030-a15f-4ad6-9f8a-d5a307501a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'insert_count': 1348, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando uma busca semântica\n",
        "query_vectors = embeddings.embed_query(\"O que é motor de crescimento?\")\n",
        "\n",
        "res = client.search(\n",
        "    collection_name = \"lean_startup\",\n",
        "    data = [query_vectors],\n",
        "    filter = \"metadata['page'] < 50\",\n",
        "    limit = 2,\n",
        "    output_fields = [\"text\", \"metadata\"],\n",
        ")\n",
        "\n",
        "res[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBqmgB6UCwAt",
        "outputId": "92ee7a94-87ef-42a8-95be-6f67e504f787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 92,\n",
              "  'distance': 0.6697660088539124,\n",
              "  'entity': {'text': 'um CEO famoso, ele era engenheiro. Em sua oficina, passou dias e noites experimentando a\\nmecânica exata para obter o movimento dos cilindros do motor. Cada pequena explosão dentro\\ndo cilindro gera força motriz para girar as rodas, e também produz a ignição da próxima\\nexplosão. Se o timing desse ciclo de feedback não for gerenciado de modo preciso, o motor\\nvai engasgar e deixar de funcionar.\\nAs startups possuem um motor semelhante, que denomino motor de crescimento. Os',\n",
              "   'metadata': {'source': '/content/A Startup Enxuta - Eric Ries.pdf',\n",
              "    'file_path': '/content/A Startup Enxuta - Eric Ries.pdf',\n",
              "    'page': 20,\n",
              "    'total_pages': 210,\n",
              "    'Author': 'Eric Ries',\n",
              "    'CreationDate': \"D:20150330135456+00'00'\",\n",
              "    'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]',\n",
              "    'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]',\n",
              "    'Title': 'A Startup Enxuta'}}},\n",
              " {'id': 100,\n",
              "  'distance': 0.5129328966140747,\n",
              "  'entity': {'text': 'de fazermos uma curva fechada chamada pivô ou se devemos perseverar em nosso caminho\\natual. Uma vez que temos um motor em funcionamento, a startup enxuta oferece métodos para\\ndimensionar e desenvolver o negócio com aceleração máxima.\\nEm todo processo de condução, você sempre tem uma ideia clara de onde está indo. Se você\\nestá indo para o trabalho, não desiste porque há um desvio no caminho ou porque entrou numa\\nrua errada. Você continua concentrado em chegar ao seu destino.',\n",
              "   'metadata': {'source': '/content/A Startup Enxuta - Eric Ries.pdf',\n",
              "    'file_path': '/content/A Startup Enxuta - Eric Ries.pdf',\n",
              "    'page': 21,\n",
              "    'total_pages': 210,\n",
              "    'Author': 'Eric Ries',\n",
              "    'CreationDate': \"D:20150330135456+00'00'\",\n",
              "    'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]',\n",
              "    'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]',\n",
              "    'Title': 'A Startup Enxuta'}}}]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recuperação\n",
        "<img src=\"https://i.ibb.co/5dWpf1w/Lead-Entra-na-Lista-11.png\">\n",
        "\n",
        "Até aqui já sabemos tudo desde o processo de ler as informações dos documentos até armazenar esses documentos em um banco de dados vetorial. E eu aposto que você já pensou:\n",
        "- \"Cacete, Anwar! Até aqui já foi coisa demais.\"\"\n",
        "\n",
        "Exatamente. Uma aplicação de RAG tem diversas variáveis. Diversas! Só na parte de carregamento dos documentos no banco de dados vetorial temos muitas etapas. Mas uma vez que o documento está dentro do banco de dados vetorial, precisamos buscar aqueles que são mais semelhantes à query do usuário, correto? **E é justamente aí que começamos a estudar as metodologias de realizar recuperação, ou *retrieval***.\n",
        "\n",
        "Ele é o intermediário entre os documentos e a resposta do usuário. Vários bancos de dados vetoriais já implementam sua própria mecânica de recuperação nativamente, onde não precisamos nos preocupar em especificar ou até mesmo escrever algum código que faça isso.\n",
        "\n",
        "Há, basicamente, três grandes métodos de realizar a recuperação:\n",
        "- Keyword-matching\n",
        "- Semantic-matching\n",
        "- Hybrid Search\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "votO5hdTFFZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword-Matching (Correspondência de Palavras-Chave)\n",
        "Esse método é baseado completamente nas palavras-chave que os textos contêm. Também chamamos esse método de correspondência de similaridade lexical.\n",
        "\n",
        "**Utilizando esses métodos, não olhamos para a semântica do texto nem para o contexto, mas sim para correspondências exatas ou aproximadas de termos presentes no texto**. Temos alguns algoritmos como TF-IDF e BM25 que são muito famosos e utilizados para essa tarefa."
      ],
      "metadata": {
        "id": "IZyqpsT5JKof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF (Frequência de Termo - Frequência Inversa de Documento)\n",
        "<img src=\"https://i.ibb.co/LdhyzTTF/Lead-Entra-na-Lista-12.png\">\n",
        "\n",
        "Esse algoritmo destaca os termos que são frequentes em um chunk específico, mas raros em todos os documentos. **Por exemplo, termos como \"de\", \"para\", \"é\" são meio que desconsiderados, pois aparecem muito nos documentos de maneira geral**. E, claro, são palavras que não precisamos dar tanto foco assim para contabilizar na similaridade.\n",
        "\n",
        "Podemos utilizar a Bag of Words aqui também! Vale um teste para medirmos qual teria o melhor resultado.\n",
        "\n",
        "Observe o exemplo abaixo:"
      ],
      "metadata": {
        "id": "EKNUEv1mox8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando a busca\n",
        "from langchain_community.retrievers import TFIDFRetriever\n",
        "\n",
        "# Corpus com avaliações de produtos\n",
        "documentos = [\n",
        "    \"O produto é excelente e chegou rapidamente\",\n",
        "    \"Chegou com defeito e atrasado, não recomendo\",\n",
        "    \"Atendimento ao cliente foi incrível, produto bom\",\n",
        "    \"Entrega rápida, mas o produto estava com defeito\"\n",
        "]\n",
        "\n",
        "retriever = TFIDFRetriever.from_texts(documentos)\n",
        "\n",
        "query = \"O produto é bom?\"\n",
        "\n",
        "retriever.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-tARFWTQWG9",
        "outputId": "f41bd5be-0d99-4dcb-d8dd-c33b8808d866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Atendimento ao cliente foi incrível, produto bom'),\n",
              " Document(metadata={}, page_content='O produto é excelente e chegou rapidamente'),\n",
              " Document(metadata={}, page_content='Entrega rápida, mas o produto estava com defeito'),\n",
              " Document(metadata={}, page_content='Chegou com defeito e atrasado, não recomendo')]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BM25 (Melhor Correspondência 25)\n",
        "<img src=\"https://i.ibb.co/MxjXhZN1/Lead-Entra-na-Lista-13.png\">\n",
        "\n",
        "Esse método é muito famoso para keyword-matching também. **Ele é um método de ranqueamento probabilístico utilizado para medir a relevância de documentos em relação a uma query**.\n",
        "\n",
        "Ele também é baseado em freqências, assim como o TF-IDF e o Bag of Words que vimos, mas possui algumas funcionalidades adicionais que o torna mais sofisticado e robusto que o simples TF-IDF. Ele junta a frequência dos termos e o comprimento dos documentos pra calcular um score de relevância.\n",
        "\n",
        "Observe o exemplo de implementação abaixo:\n",
        "\n"
      ],
      "metadata": {
        "id": "Nu6A5DmdLun4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 --quiet"
      ],
      "metadata": {
        "id": "qLdtgWyIPLPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando a busca\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "# Corpus com avaliações de produtos\n",
        "documentos = [\n",
        "    \"O produto é excelente e chegou rapidamente\",\n",
        "    \"Chegou com defeito e atrasado, não recomendo\",\n",
        "    \"Atendimento ao cliente foi incrível, produto bom\",\n",
        "    \"Entrega rápida, mas o produto estava com defeito\"\n",
        "]\n",
        "\n",
        "retriever = BM25Retriever.from_texts(documentos)\n",
        "\n",
        "query = \"O produto é bom?\"\n",
        "\n",
        "retriever.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft019JTdPNgp",
        "outputId": "20f99725-daea-4a16-a963-e3a3f0e6c62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='O produto é excelente e chegou rapidamente'),\n",
              " Document(metadata={}, page_content='Atendimento ao cliente foi incrível, produto bom'),\n",
              " Document(metadata={}, page_content='Entrega rápida, mas o produto estava com defeito'),\n",
              " Document(metadata={}, page_content='Chegou com defeito e atrasado, não recomendo')]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic-Matching (Correspondência Semântica)\n",
        "Os métodos anteriores eram totalmente baseados em correspondência de palavras-chave - seja exata ou aproximada - e falhava em capturar o significado, a semântica dos textos.\n",
        "\n",
        "Por isso, temos métodos que são 100% beaseados em retornar com base nos documentos mais similares semanticamente.\n",
        "\n",
        "Esses embeddings são obtidos geralmente a partir de um modelo de Deep Learning que foi pré-treinado em um gigantesco volume de dados. O objetivo desse modelo é capturar a relação entre as palavras e identificar o significado delas.\n",
        "\n",
        "Temos diversos modelos que realizam esse tipo de tarefa:\n",
        "- Modelos de embedding tradicionais: ***Word2Vec, GloVe***\n",
        "- Modelos de embedding baseados no Transformer: ***BERT, Sentence-BERT, OpenAI Embeddings***\n",
        "- Modelos de visão para dados não textuais"
      ],
      "metadata": {
        "id": "7CjoeYIFSGBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec (Palavra para Vetor)\n",
        "<img src=\"https://i.ibb.co/qMg1kK92/Lead-Entra-na-Lista-14.png\">\n",
        "\n",
        "Você, analisando a imagem acima, pode ter pensando:\n",
        "- \"PQP!\"\n",
        "\n",
        "Seja por achar muito complexo ou por achar muito foda. No meu caso, quando fui estudar pela primeira vez, foi um mix dos dois, mas pendendo mais para o primeiro caso.\n",
        "\n",
        "Esse modelo funciona da seguinte forma (estou utilizando uma [explicação dada pelo dono do canal StatQuest no YouTube](https://www.youtube.com/watch?v=viZrOnJclY0), que achei fenomenal a didática). Imagine que você tenha duas frases:\n",
        "- Troll2 is great!\n",
        "- Gymkata is great!\n",
        "\n",
        "Como podemos capturar o significado dessas palavras? Bom, podemos ficar treinando a previsão da próxima palavra repetidamente. O que isso significa? Para a primeira frase, se a palavra de entrada for **Troll2**, o modelo tem que prever **is**, se for **is** o modelo tem que prever **great**. Entende? E indicamos qual é a palavra de entrada utilizando o número 1, como você pode ver à esquerda.\n",
        "\n",
        "- \"Ok, Anwar, mas como ele transforma isso em números?\"\n",
        "\n",
        "Observe os números em vermelho logo após as entradas. Esses são os embeddings! Nosso modelo vai aprendendo qual é a melhor combinação de números que faz com que nós acertemos a previsão uma maior quantidade de vezes.\n",
        "\n",
        "No caso do exemplo, o modelo errou! A palavra com maior pontuação foi **is** ao invés de **great!**. E é aí que o modelo atualiza os pesos para evitar o erro.\n",
        "\n",
        "Observe o exemplo abaixo usando Word2Vec:"
      ],
      "metadata": {
        "id": "UuEvxLyzfRgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim --quiet"
      ],
      "metadata": {
        "id": "N54ICV-MeS8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b4f9e6-9922-4b29-a017-6fc402b41bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Documentos de exemplo\n",
        "documentos = [\n",
        "    \"O produto é excelente e chegou rapidamente\",\n",
        "    \"Chegou com defeito e atrasado, não recomendo\",\n",
        "    \"Atendimento ao cliente foi incrível, produto bom\",\n",
        "    \"Entrega rápida, mas o produto estava com defeito\"\n",
        "]\n",
        "\n",
        "# Pré-processamento: tokenização\n",
        "tokens = [simple_preprocess(doc) for doc in documentos]\n",
        "\n",
        "# Treinando um modelo Word2Vec com os documentos\n",
        "model = Word2Vec(sentences=tokens, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Função para obter o embedding médio de um documento\n",
        "def get_embedding(doc):\n",
        "    words = simple_preprocess(doc)\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Gerando embeddings para os documentos\n",
        "document_embeddings = [get_embedding(doc) for doc in documentos]\n",
        "\n",
        "# Consulta de exemplo\n",
        "consulta = \"O produto é bom?\"\n",
        "\n",
        "# Gerando embedding da consulta\n",
        "consulta_embedding = get_embedding(consulta)\n",
        "\n",
        "# Calculando similaridade coseno entre a consulta e os documentos\n",
        "similaridades = cosine_similarity([consulta_embedding], document_embeddings).flatten()\n",
        "\n",
        "# Organizando documentos e scores\n",
        "resultados = sorted(zip(documentos, similaridades), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Exibindo os resultados\n",
        "print(f\"Consulta: {consulta}\")\n",
        "print(\"\\nDocumentos ordenados por relevância:\")\n",
        "for doc, score in resultados:\n",
        "  if score > 0.40:\n",
        "    print(f\"- Documento: {doc} | Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdLNDU54elxv",
        "outputId": "5d47a6e5-a185-4297-c90b-2640545ffb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta: O produto é bom?\n",
            "\n",
            "Documentos ordenados por relevância:\n",
            "- Documento: Atendimento ao cliente foi incrível, produto bom | Score: 0.5691\n",
            "- Documento: O produto é excelente e chegou rapidamente | Score: 0.4065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT (Bidirectional Encoder Representations from Transformers)\n",
        "<img src=\"https://i.ibb.co/KpRKQkkH/Lead-Entra-na-Lista-15.png\">\n",
        "\n",
        "O BERT não lê uma palavra de cada vez, como modelos antigos, mas considera o contexto completo da frase para entender o que está acontecendo. Conseguimos ver isso no nome dele: **Bidirectional**.\n",
        "\n",
        "Imagine que você tem a frase:\n",
        "- \"O cachorro **correu rápido** porque estava sendo perseguido.\"\n",
        "\n",
        "Se perguntarmos ao BERT o que significa \"correu rápido\", ele vai entender que está ligado à ideia de \"fugir\" ou \"escapar\", porque ele já olhou para as palavras antes e depois.\n",
        "\n",
        "Agora, deixa eu explicar como ele faz isso sem transformar isso num papo chato.\n",
        "\n",
        "#### Contexto Importa\n",
        "O BERT pega a frase inteira e processa tudo ao mesmo tempo, de trás para frente e de frente para trás. Por isso é chamado de bidirecional. Ele quer entender o contexto completo.\n",
        "\n",
        "#### Treinamento com Palavras Mascaradas (Fill in the Blanks)\n",
        "\n",
        "Imagine que a frase seja:\n",
        "- \"Eu **[MASCARADO]** sorvete no verão.\"\n",
        "\n",
        "O trabalho do BERT é adivinhar a palavra \"gosto\" usando o resto da frase como pista.\n",
        "\n",
        "#### Previsão de Sequência (Lógica das Frases)\n",
        "Outra tarefa do BERT é entender se duas frases fazem sentido juntas. Exemplo:\n",
        "- **Frase 1**: \"Eu adoro praia.\"\n",
        "- **Frase 2**: \"Sempre levo minha prancha.\"\n",
        "\n",
        "Ele aprende que essas frases têm uma relação lógica.\n",
        "\n",
        "#### Tá, mas como ele aprende números?\n",
        "\n",
        "É como o Word2Vec que falamos anteriormente, mas mais esperto. Cada palavra é transformada em um vetor (uma lista de números). O BERT ajusta esses números para que palavras com significados parecidos fiquem perto umas das outras no espaço multidimensional.\n",
        "\n",
        "Exemplo:\n",
        "- \"cachorro\" e \"gato\" vão ter números parecidos porque são ambos animais domésticos.\n",
        "- \"rápido\" e \"devagar\" vão ter números bem diferentes porque são opostos.\n",
        "\n",
        "\n",
        "Observe o exemplo abaixo de aplicação do BERT:"
      ],
      "metadata": {
        "id": "uHZsduyGfTOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "id": "6ZkNKRrDiJtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Carregar o modelo BERT Multilingual e o tokenizer\n",
        "modelo_nome = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_nome)\n",
        "modelo = AutoModel.from_pretrained(modelo_nome)\n",
        "\n",
        "# Textos de exemplo (base de dados)\n",
        "documentos = [\n",
        "    \"O produto é excelente e chegou rapidamente.\",\n",
        "    \"Entrega atrasada e produto com defeito.\",\n",
        "    \"Atendimento incrível, produto perfeito.\",\n",
        "    \"Chegou quebrado, péssima experiência.\",\n",
        "]\n",
        "\n",
        "# Função para gerar embeddings usando o BERT\n",
        "def gerar_embeddings(textos):\n",
        "    tokens = tokenizer(textos, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = modelo(**tokens)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Média sobre os tokens\n",
        "    return embeddings\n",
        "\n",
        "# Gerar embeddings para os documentos\n",
        "embeddings_documentos = gerar_embeddings(documentos)\n",
        "\n",
        "# Query para busca\n",
        "query = \"Produto com problemas na entrega.\"\n",
        "embedding_query = gerar_embeddings([query])\n",
        "\n",
        "# Calcular similaridades entre a query e os documentos\n",
        "similaridades = cosine_similarity(embedding_query, embeddings_documentos)\n",
        "\n",
        "# Exibir os resultados ordenados\n",
        "resultados = sorted(\n",
        "    zip(similaridades[0], documentos), reverse=True, key=lambda x: x[0]\n",
        ")\n",
        "\n",
        "print(\"Resultados da busca para a query:\", query)\n",
        "for similaridade, doc in resultados:\n",
        "    print(f\"Similaridade: {similaridade:.2f} | Documento: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "ac9c03de349f454ea09c87d5887e65aa",
            "e9a4f9910c7747cbb772506bf97fa959",
            "0bf517c549f141fcb4ad6fd74c5a30ab",
            "f928fc583ac74beabd2f2cb987bfc895",
            "340be93b4047441b88aee74e047c368c",
            "09af667772aa43a881244d98d8e163cd",
            "dd9b6e7c3d1344ac8993905c8ad92d32",
            "016e8f68aba041acafe96337bd5852d5",
            "299fb0ee5de140da83d1bca42173e0db",
            "79eac04b57e4469db83e584cafaec435",
            "f3208e7d205f475eb0a678daa1d70658",
            "416eeb4a21894f1e8df8b18542907b5d",
            "e593658278c1484aa3d2753e76bcd495",
            "32fd7b66b3a84f9da9e746987f4c5a14",
            "57e839238bb04ecc999ac579ff13cff4",
            "0fc3dac3de214936aa50d78e9f6b8a64",
            "038c3a40404d4d20bd2d0d04c9cb5b6d",
            "f5571501b044402e9e0c53322c57eb1c",
            "03ea9105e1574890925afca734d034b8",
            "d273dee4865b4e52819d92da648699df",
            "edaada598ac64504b8c8f49f72c8af1d",
            "e6e48907455b41629add7193505a3773",
            "e156666d24664dfc99264001f76070a8",
            "242851e0815f46d5be1f6655ff81b1ca",
            "b9c14e42fef44083a0dc42bea9d9b490",
            "51c7c3d9d0654d549ad01d6d1b7d4058",
            "c6098bd4540048da9d72ad741e989e15",
            "aa650da113914363a90e386530c6efb9",
            "2681d09bccf64aed9a3384facf0b83b1",
            "01d84585c11645d5b1b95e682656c4de",
            "ae4983dacb8548108f51ff9e906147ce",
            "188b9b155add4f669dc4e4e50c7a044a",
            "8f04f15d4faf497aac6db3b39a82a7b2",
            "01506fe3711d4fc2968a603b864ab3d9",
            "ff856ac03b8145ceae09d27deb88df14",
            "f6ea3b344497476ebd6417111eb01e5e",
            "307f9b56ace846fda6f9144de1b6f398",
            "5d8d005ac69c4482bb7f5986bd64a874",
            "6705b0d8352b4855a571ee2c35d2b460",
            "361a5a02cf60431fbbdd2a6660915efb",
            "cd2e2b06746c4a74b201210d69e324c4",
            "f9998340a8004cc489cc52d7dd517f5d",
            "8bad1df891c84d0ab00e98580d1bca5e",
            "bc30515f17cd4ff4b1f2b6972abf7bc1",
            "faebe2880a25404aacfb76f44015f07b",
            "ab4375f64cd54536a4fd6743c34b4038",
            "6cab11e31c48477b80fbb32b6ef61e36",
            "3222e1deef3a41c6b81245e8946fb91a",
            "550df6ae14f64c6b8dadc8191ab89550",
            "534eb95b42294b0e95715eeb0f8dbee3",
            "8c094e590f24445eac025e382c871692",
            "5fdc5a674e7e4ac0938758919d2c8e88",
            "c8593267088d4eeb9d7eceddb84c413a",
            "46e91a5e882e4a4191401840c4675f4e",
            "312793a120f4458e886028f7fd99acf8",
            "5fe49b38e35d4de4accef02dce369c5b",
            "6fd847e531c54324a6142d9b39756e7d",
            "b18d42f61bab4ed692da2fff0a5351c3",
            "3f89ec63a48645af8ef24bef04393e90",
            "851a3692b4da4414aafce071805af991",
            "84baf766a3e34eba8baadb5f0052f091",
            "880b30d963ae468cad5d75c2568cc3d6",
            "d22a8d7d994145058991ea314a3817cf",
            "21acc85ff5244b62b7576c33b23ea64b",
            "bb4d5612a77f4522bc3a7987c7164d20",
            "0ca3ffc0c820439fa9d526c773c10fb4",
            "5f7fef59041942708cc9da065196a4db",
            "af4e534b8c994b7394241ad1f83a6257",
            "845821a5fba2457bb49cdc9120e86ea2",
            "b57ec29db3344c4f947cf1577a754de1",
            "9dd32872332c4137bdb801ef73d865c6",
            "de8cfe203c6948db953cbd95bb949a16",
            "c44156963c4a49f4a855b0bd4d0e1642",
            "14550a707e2d4e46a126a65ab6fa4fe3",
            "7f7b19f48f7b42e58b5a7696e912690c",
            "f6f9989a58bd49d184bc816e91991536",
            "3e6ff308341c410283450dee710d0cb1"
          ]
        },
        "id": "hu5W9CloiAWo",
        "outputId": "8e1d9f46-8903-436e-ab2d-5ba9ae519a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac9c03de349f454ea09c87d5887e65aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "416eeb4a21894f1e8df8b18542907b5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e156666d24664dfc99264001f76070a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01506fe3711d4fc2968a603b864ab3d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "faebe2880a25404aacfb76f44015f07b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fe49b38e35d4de4accef02dce369c5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f7fef59041942708cc9da065196a4db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da busca para a query: Produto com problemas na entrega.\n",
            "Similaridade: 0.93 | Documento: Entrega atrasada e produto com defeito.\n",
            "Similaridade: 0.85 | Documento: Chegou quebrado, péssima experiência.\n",
            "Similaridade: 0.81 | Documento: O produto é excelente e chegou rapidamente.\n",
            "Similaridade: 0.80 | Documento: Atendimento incrível, produto perfeito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Search (Busca Híbrida)\n",
        "<img src=\"https://i.ibb.co/zVzhdZR2/Lead-Entra-na-Lista-16.png\">\n",
        "\n",
        "Esse método é um dos melhores, justamente porque não é 100% baseado na similaridade léxica nem 100% baseado na similaridade semântica. **Ele é uma mescla dos dois métodos, onde podemos controlar o peso de cada um deles**.\n",
        "\n",
        "Imagine um exemplo onde a query seja:\n",
        "- \"Quais são as melhores receitas brasileiras?\"\n",
        "\n",
        "A parte da busca que utiliza keyword-matching vai procurar os documentos que possuem um maior score e, consequentemente, que possuem as palavras-chave \"brasileiras\" e \"receita\".\n",
        "\n",
        "Já a parte responsável pela busca semântica vai procurar pelos documentos que possuem um contexto como \"comidas do Brasil\" ou algo relacionado.\n",
        "\n",
        "Com os documentos retornados pelos dois métodos, vamos combinar os scores de cada um e reordernar os documentos.\n",
        "\n",
        "Observe o exemplo abaixo onde uso o OpenAIEmbeddings e o BM25:"
      ],
      "metadata": {
        "id": "lulBCJWfjGuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando os documentos\n",
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "loader = PDFPlumberLoader(\n",
        "    file_path = \"/content/A Startup Enxuta - Eric Ries.pdf\"\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "docs = [x for x in docs if x.page_content.strip() not in (\"\\n\", \"\", \" \", \"\\t\")]\n",
        "for k, doc in enumerate(docs, start = 1):\n",
        "  doc.metadata[\"id\"] = k\n",
        "\n",
        "# Realizando splitting\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 10)\n",
        "docs_splitted = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "Sxcjr4FBkq_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic-Matching\n",
        "from pymilvus import MilvusClient\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "client = MilvusClient(\"teste_aula.db\")\n",
        "\n",
        "if client.has_collection(collection_name = \"lean_startup\"):\n",
        "    client.drop_collection(collection_name = \"lean_startup\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name = \"lean_startup\",\n",
        "    dimension = 1536,\n",
        ")\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
        "\n",
        "vectors = embeddings.embed_documents([doc.page_content for doc in docs_splitted])\n",
        "\n",
        "data = [\n",
        "    {\"id\": i, \"vector\": vectors[i], \"text\": docs_splitted[i].page_content, \"metadata\": docs_splitted[i].metadata}\n",
        "    for i in range(len(vectors))\n",
        "]\n",
        "\n",
        "res = client.insert(collection_name=\"lean_startup\", data=data)"
      ],
      "metadata": {
        "id": "5VN6mjkCmqT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword-Matching\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "tokenized_corpus = [doc.page_content.split(\" \") for doc in docs_splitted]\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ],
      "metadata": {
        "id": "ulooCmt5oZIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parâmetros\n",
        "query = \"O que é motor de crescimento?\"\n",
        "k = 5\n",
        "alpha = 0.5"
      ],
      "metadata": {
        "id": "DyEw0pwpw32m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BM25\n",
        "tokenized_query = query.lower().split(\" \")\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "bm25_top_k_indices = np.argsort(bm25_scores)[-k:][::-1]\n",
        "\n",
        "# Recuperar os top K documentos BM25\n",
        "bm25_rankings = {doc_id: rank for rank, doc_id in enumerate(bm25_top_k_indices)}"
      ],
      "metadata": {
        "id": "nnYxNLZSw2ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Milvus\n",
        "query_vectors = embeddings.embed_query(query)\n",
        "\n",
        "semantic_results = client.search(\n",
        "    collection_name = \"lean_startup\",\n",
        "    data = [query_vectors],\n",
        "    # filter = \"metadata['page'] < 50\",\n",
        "    limit = k,\n",
        "    output_fields = [\"text\", \"id\", \"metadata\"],\n",
        ")\n",
        "\n",
        "# Extrair scores e IDs dos resultados semânticos\n",
        "semantic_rankings = {\n",
        "    res['id']: rank for rank, res in enumerate(sorted(semantic_results[0], key=lambda x: x['distance']))\n",
        "}"
      ],
      "metadata": {
        "id": "3muoEDJhxEBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que faz o re-ranking\n",
        "def calculate_rrf(rankings, k=60):\n",
        "    scores = {}\n",
        "    for ranker in rankings:\n",
        "        for doc_id, rank in ranker.items():\n",
        "            if doc_id not in scores:\n",
        "                scores[doc_id] = 0\n",
        "            scores[doc_id] += 1 / (k + rank)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "lQGWJTj0ukYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scores unidos\n",
        "combined_scores = calculate_rrf([bm25_rankings, semantic_rankings])\n",
        "\n",
        "print(combined_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FfpoZLsxipB",
        "outputId": "ce25cf30-7f60-4e05-fc30-a163b7bb7ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{957: 0.016666666666666666, 1016: 0.01639344262295082, 534: 0.016129032258064516, 964: 0.03149801587301587, 974: 0.015625, 1027: 0.016666666666666666, 92: 0.01639344262295082, 958: 0.016129032258064516, 803: 0.015873015873015872}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando os documentos com as K maiores similaridades\n",
        "top_k_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "result = [(doc_id, docs_splitted[doc_id] if doc_id < len(docs_splitted) else \"Documento semântico\", score) for doc_id, score in top_k_docs]"
      ],
      "metadata": {
        "id": "5OKgTmcUxrNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huTgbaG7x5NQ",
        "outputId": "4f9ec3ac-7c2e-4e0c-a85a-dd0dc14fdfa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(964,\n",
              "  Document(metadata={'source': '/content/A Startup Enxuta - Eric Ries.pdf', 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf', 'page': 151, 'total_pages': 210, 'Author': 'Eric Ries', 'CreationDate': \"D:20150330135456+00'00'\", 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Title': 'A Startup Enxuta', 'id': 149}, page_content='motores de crescimento. Cada um é como um motor de combustão, girando repetidas vezes.\\nQuanto mais rápido o ciclo é completado, mais rápido a empresa crescerá. Cada motor possui\\num conjunto intrínseco de métricas que determinam com que rapidez uma empresa pode\\ncrescer ao utilizá-lo.\\nOS TRÊS MOTORES DE CRESCIMENTO\\nVimos na Parte II como é importante que as startups utilizem o tipo certo de métricas –\\nmétricas acionáveis – para avaliar o progresso. No entanto, isso deixa uma grande quantidade'),\n",
              "  0.03149801587301587),\n",
              " (957,\n",
              "  Document(metadata={'source': '/content/A Startup Enxuta - Eric Ries.pdf', 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf', 'page': 149, 'total_pages': 210, 'Author': 'Eric Ries', 'CreationDate': \"D:20150330135456+00'00'\", 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Title': 'A Startup Enxuta', 'id': 147}, page_content='utilizam o mesmo motor de crescimento, que é o tópico deste capítulo.'),\n",
              "  0.016666666666666666),\n",
              " (1027,\n",
              "  Document(metadata={'source': '/content/A Startup Enxuta - Eric Ries.pdf', 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf', 'page': 160, 'total_pages': 210, 'Author': 'Eric Ries', 'CreationDate': \"D:20150330135456+00'00'\", 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Title': 'A Startup Enxuta', 'id': 158}, page_content='construído sobre um motor de crescimento bem regulado. Se esse motor se esgotar e o\\ncrescimento arrefecer ou parar, poderá haver uma crise se a empresa não tiver novas startups\\nincubando dentro dos seus níveis hierárquicos, que podem proporcionar novas fontes de\\ncrescimento.\\nAs empresas de qualquer tamanho podem sofrer dessa aflição contínua. Elas precisam\\ngerenciar um portfólio de atividades, ao mesmo tempo regulando seus motores de crescimento'),\n",
              "  0.016666666666666666),\n",
              " (1016,\n",
              "  Document(metadata={'source': '/content/A Startup Enxuta - Eric Ries.pdf', 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf', 'page': 158, 'total_pages': 210, 'Author': 'Eric Ries', 'CreationDate': \"D:20150330135456+00'00'\", 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Title': 'A Startup Enxuta', 'id': 156}, page_content='alcançar o encaixe produto / mercado”. Também implica o inverso, ou seja, assim que nosso\\nproduto alcançou o encaixe produto/mercado, não teremos mais de pivotar. As duas\\nsuposições estão erradas.\\nAcredito que o conceito do motor de crescimento pode pôr a ideia do encaixe\\nproduto/mercado numa perspectiva mais rigorosa. Como cada motor de crescimento pode ser\\ndefinido de modo quantitativo, cada um dispõe de um conjunto único de métricas que podem'),\n",
              "  0.01639344262295082),\n",
              " (92,\n",
              "  Document(metadata={'source': '/content/A Startup Enxuta - Eric Ries.pdf', 'file_path': '/content/A Startup Enxuta - Eric Ries.pdf', 'page': 20, 'total_pages': 210, 'Author': 'Eric Ries', 'CreationDate': \"D:20150330135456+00'00'\", 'Creator': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Producer': 'calibre 2.20.0 [http://calibre-ebook.com]', 'Title': 'A Startup Enxuta', 'id': 19}, page_content='um CEO famoso, ele era engenheiro. Em sua oficina, passou dias e noites experimentando a\\nmecânica exata para obter o movimento dos cilindros do motor. Cada pequena explosão dentro\\ndo cilindro gera força motriz para girar as rodas, e também produz a ignição da próxima\\nexplosão. Se o timing desse ciclo de feedback não for gerenciado de modo preciso, o motor\\nvai engasgar e deixar de funcionar.\\nAs startups possuem um motor semelhante, que denomino motor de crescimento. Os'),\n",
              "  0.01639344262295082)]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geração\n",
        "<img src=\"https://i.ibb.co/LzFdJ38x/Lead-Entra-na-Lista-17.png\">\n",
        "\n",
        "Já vimos as principais etapas até agora: **a indexação e a recuperação**. Já temos tudo o que precisamos, que é o banco de dados vetorial com os embeddings dentro e o mecanismo que vai recuperar os embeddings mais similares.\n",
        "\n",
        "Apesar de ter tudo isso, ainda não geramos uma resposta com uma I.A.. De que vale tudo isso sem gerar a resposta, que é o que queremos? É justamente isso que faz a etapa de geração: **geramos uma resposta com a I.A. utilizando os chunks retornados**.\n",
        "\n",
        "Apesar de parecer muito simples, vou te fazer um questionamento: **basta apenas jogar os chunks retornados no prompt e pedir para gerar a resposta?**\n",
        "\n",
        "Vamos trabalhar isso agora."
      ],
      "metadata": {
        "id": "zPeqtmoucZ16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retomando o Projeto do Lean Startup\n",
        "Vou fazer as etapas de indexação e recuperação para podermos implementar a etapa de geração. Para isso, vou fazer o projeto usando o livro Lean Startup aplicando as técnicas que já vimos."
      ],
      "metadata": {
        "id": "svNa0fx5bV8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas necessárias\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "## INDEXAÇÃO ##\n",
        "\n",
        "# Carregando os documentos\n",
        "loader = PDFPlumberLoader(\n",
        "    file_path = \"/content/A Startup Enxuta - Eric Ries.pdf\"\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "# Instanciando o splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 600, chunk_overlap = 60)\n",
        "docs_splitted = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "_UBK8oPUecUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding\n",
        "from uuid import uuid4\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"eric_ries_lean_startup\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"/content/chroma_db\",\n",
        ")\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(docs_splitted))]\n",
        "vector_store.add_documents(documents=docs_splitted, ids=uuids)\n",
        "\n",
        "# Transformando o vector store em um objeto \"pesquisável\"\n",
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "UyQkd7SJezBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eee1140-a270-4cbe-b4cd-892e11f2ecd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-5e1636572a26>:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vector_store = Chroma(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## RECUPERAÇÃO ##\n",
        "\n",
        "documentos = retriever.invoke(\"O que são motores de crescimento?\", k = 3)\n",
        "\n",
        "# Função que formata os documentos retornados\n",
        "def formataDocumentos(docs):\n",
        "  return \"\\n\\n\".join([f\"Documento {k}:\\n{doc.page_content}\\nPágina do Livro Lean startup que contém as informações do documento {k}: {doc.metadata.get('page')}\" for k, doc in enumerate(docs)])\n",
        "\n",
        "# Formatando os documentos\n",
        "documentos_formatados = formataDocumentos(documentos)\n",
        "\n",
        "print(documentos_formatados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OY3mOhVf_Mh",
        "outputId": "a07b6730-4154-4d87-e5f3-f04003a684a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento 0:\n",
            "motores de crescimento. Cada um é como um motor de combustão, girando repetidas vezes.\n",
            "Quanto mais rápido o ciclo é completado, mais rápido a empresa crescerá. Cada motor possui\n",
            "um conjunto intrínseco de métricas que determinam com que rapidez uma empresa pode\n",
            "crescer ao utilizá-lo.\n",
            "OS TRÊS MOTORES DE CRESCIMENTO\n",
            "Vimos na Parte II como é importante que as startups utilizem o tipo certo de métricas –\n",
            "métricas acionáveis – para avaliar o progresso. No entanto, isso deixa uma grande quantidade\n",
            "em termos de que números devemos medir. De fato, uma das formas mais onerosas de possível\n",
            "Página do Livro Lean startup que contém as informações do documento 0: 151\n",
            "\n",
            "Documento 1:\n",
            "DE ONDE VEM O CRESCIMENTO?\n",
            "O motor de crescimento é o mecanismo que as startups utilizam para alcançar o crescimento\n",
            "sustentável. Utilizo a palavra sustentável para excluir todas as atividades ocasionais que\n",
            "geram um surto de clientes, mas não têm impacto a longo prazo, tais como anúncios isolados\n",
            "ou uma proeza publicitária que pode ser utilizada para revitalizar o crescimento, mas não\n",
            "consegue sustentá-lo a longo prazo.\n",
            "O crescimento sustentável se caracteriza por uma regra simples:\n",
            "Os novos clientes surgem das ações dos clientes passados.\n",
            "Página do Livro Lean startup que contém as informações do documento 1: 150\n",
            "\n",
            "Documento 2:\n",
            "todos os motores de crescimento acabam ficando sem gasolina. Todos os motores estão\n",
            "relacionados a um determinado conjunto de clientes e seus hábitos, preferências, canais\n",
            "publicitários e interconexões. Em algum momento, esse conjunto de clientes será exaurido.\n",
            "Pode levar muito ou pouco tempo, dependendo do setor e do timing.\n",
            "O Capítulo 6 enfatizou a importância de construir o produto mínimo viável de maneira a não\n",
            "incluir nenhum recurso adicional além do requerido pelos adotantes iniciais. Seguir essa\n",
            "estratégia com êxito destravará um motor de crescimento que pode alcançar a audiência-alvo.\n",
            "Página do Livro Lean startup que contém as informações do documento 2: 159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelando o prompt da geração\n",
        "Agora que temos os documentos - e perceba que os deixei até melhor formatados - vamos fazer o prompt. Vou pegar o prompt template para RAG que o próprio langchain tem em seu hub."
      ],
      "metadata": {
        "id": "w83ZPL9LlAGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o prompt\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "print(prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAIB78-FlY4D",
        "outputId": "f633f666-afe4-4942-fb2b-8ba9a5a84f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: {question} \n",
            "Context: {context} \n",
            "Answer:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "**Question**: {question}\n",
        "\n",
        "**Context**: {context}\n",
        "\n",
        "**Answer**:"
      ],
      "metadata": {
        "id": "Yr1G7sqflvfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo uma pergunta ao documento\n",
        "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0)\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "Você é um assistente para tarefas de resposta a perguntas.\n",
        "Use as seguintes partes do contexto recuperado para responder à pergunta.\n",
        "Se você não souber a resposta, diga apenas que não sabe. Use no máximo três frases e mantenha a resposta concisa.\n",
        "Sempre mencione onde o usuário consegue encontrar as informações (qual página do livro está).\n",
        "Responda usando espaçamento vertical com \\\\n\n",
        "\n",
        "Pergunta:\n",
        "{question}\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Resposta:\n",
        "\"\"\")\n",
        "\n",
        "print(prompt.format(question = \"O que são motores de crescimento?\", context = documentos_formatados))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL9Q6A1Ekkw1",
        "outputId": "c5ff0285-3131-4098-fdea-008716f3cc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Você é um assistente para tarefas de resposta a perguntas.\n",
            "Use as seguintes partes do contexto recuperado para responder à pergunta.\n",
            "Se você não souber a resposta, diga apenas que não sabe. Use no máximo três frases e mantenha a resposta concisa.\n",
            "Sempre mencione onde o usuário consegue encontrar as informações (qual página do livro está).\n",
            "Responda usando espaçamento vertical com \\n\n",
            "\n",
            "Pergunta:\n",
            "O que são motores de crescimento?\n",
            "\n",
            "Contexto:\n",
            "Documento 0:\n",
            "motores de crescimento. Cada um é como um motor de combustão, girando repetidas vezes.\n",
            "Quanto mais rápido o ciclo é completado, mais rápido a empresa crescerá. Cada motor possui\n",
            "um conjunto intrínseco de métricas que determinam com que rapidez uma empresa pode\n",
            "crescer ao utilizá-lo.\n",
            "OS TRÊS MOTORES DE CRESCIMENTO\n",
            "Vimos na Parte II como é importante que as startups utilizem o tipo certo de métricas –\n",
            "métricas acionáveis – para avaliar o progresso. No entanto, isso deixa uma grande quantidade\n",
            "em termos de que números devemos medir. De fato, uma das formas mais onerosas de possível\n",
            "Página do Livro Lean startup que contém as informações do documento 0: 151\n",
            "\n",
            "Documento 1:\n",
            "DE ONDE VEM O CRESCIMENTO?\n",
            "O motor de crescimento é o mecanismo que as startups utilizam para alcançar o crescimento\n",
            "sustentável. Utilizo a palavra sustentável para excluir todas as atividades ocasionais que\n",
            "geram um surto de clientes, mas não têm impacto a longo prazo, tais como anúncios isolados\n",
            "ou uma proeza publicitária que pode ser utilizada para revitalizar o crescimento, mas não\n",
            "consegue sustentá-lo a longo prazo.\n",
            "O crescimento sustentável se caracteriza por uma regra simples:\n",
            "Os novos clientes surgem das ações dos clientes passados.\n",
            "Página do Livro Lean startup que contém as informações do documento 1: 150\n",
            "\n",
            "Documento 2:\n",
            "todos os motores de crescimento acabam ficando sem gasolina. Todos os motores estão\n",
            "relacionados a um determinado conjunto de clientes e seus hábitos, preferências, canais\n",
            "publicitários e interconexões. Em algum momento, esse conjunto de clientes será exaurido.\n",
            "Pode levar muito ou pouco tempo, dependendo do setor e do timing.\n",
            "O Capítulo 6 enfatizou a importância de construir o produto mínimo viável de maneira a não\n",
            "incluir nenhum recurso adicional além do requerido pelos adotantes iniciais. Seguir essa\n",
            "estratégia com êxito destravará um motor de crescimento que pode alcançar a audiência-alvo.\n",
            "Página do Livro Lean startup que contém as informações do documento 2: 159\n",
            "\n",
            "Resposta:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "resposta = chain.invoke({\"question\": \"O que são motores de crescimento?\", \"context\": documentos_formatados})\n",
        "\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VE0FgfRm7_3",
        "outputId": "67d2d790-115d-498d-d3de-dfade47c8fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Motores de crescimento são mecanismos que startups utilizam para alcançar crescimento sustentável, caracterizado por novos clientes surgindo das ações dos clientes passados. Eles funcionam como motores de combustão, onde a rapidez do ciclo determina a velocidade de crescimento da empresa. Informações adicionais podem ser encontradas nas páginas 150 e 151 do livro \"Lean Startup\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Técnicas Avançadas de RAG\n",
        "Agora que já vimos todas as etapas de um projeto R.A.G., **podemos partir para as técnicas avançadas**. E eu já te adianto: já vimos uma dessas técnicas aqui! Lembra do Hybrid Search? Pois é! É considerado uma técnica avançada.\n",
        "\n",
        "Hoje, vamos ver as seguintes técnicas:\n",
        "\n",
        "- Recuperação Hierárquica (Hierarchical Index Retrieval)\n",
        "- Embedding de Documentos e Perguntas Hipotéticas (Hypothetical Questions & HyDE)\n",
        "- Recuperação por Janela de Sentença (Sentence Window Retrieval)\n",
        "- Recuepração do Documento Pai (Parent Document Retriever / Auto-merging Retriever)\n",
        "- Hybrid Search (já vimos!)\n",
        "- Transformação da Query (Query Transformation)\n",
        "- Contexto de Chat (Chat Engine)\n",
        "- Roteamento de Query (Query Routing)\n",
        "- RAG Agêntico (Agentic RAG)\n",
        "\n",
        "Para o código, usarei [esse repositório do GitHub](https://github.com/NirDiamant/RAG_Techniques/tree/main) como referência. É excelente e muito rico!\n",
        "\n",
        "É, meus amigos e amigas... o trem vai ser cabuloso. Vamos começar?"
      ],
      "metadata": {
        "id": "2tzzmfRh1Uil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recuperação Hierárquica\n",
        "<img src=\"https://i.ibb.co/YFrDy4f7/Lead-Entra-na-Lista-18.png\">\n",
        "\n",
        "Quando temos muitos documentos para buscar, precisamos de uma estratégia eficiente, não é mesmo? É aí que entra a recuperação hierárquica: **criamos dois índices diferentes - um com resumos e outro com chunks dos documentos**.\n",
        "\n",
        "Mas por que fazer isso? Simples: **primeiro buscamos nos resumos para filtrar os documentos relevantes, e depois buscamos apenas dentro desse grupo específico**.\n",
        "\n",
        "Pense bem: não é muito mais eficiente do que sair vasculhando documento por documento? Afinal, primeiro identificamos rapidamente quais documentos interessam, e só depois mergulhamos nos detalhes.\n",
        "\n",
        "Vamos ver na prática como implementar isso?"
      ],
      "metadata": {
        "id": "3feBL_563Phe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "class RecuperacaoHierarquica:\n",
        "    def __init__(self, documentos, chunk_size=1000, chunk_overlap=200, metadado_filtro: str = \"source\"):\n",
        "        \"\"\"\n",
        "        Classe para recuperação hierárquica de documentos utilizando embeddings da OpenAI.\n",
        "\n",
        "        Args:\n",
        "            documentos: Lista de documentos no formato Document.\n",
        "            chunk_size: Tamanho de cada chunk ao dividir os documentos.\n",
        "            chunk_overlap: Sobreposição entre chunks consecutivos.\n",
        "        \"\"\"\n",
        "        self.documentos = documentos\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.metadado_filtro = metadado_filtro\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.resumo_store = None\n",
        "        self.detalhe_store = None\n",
        "        self.processar_documentos()\n",
        "\n",
        "    def processar_documentos(self):\n",
        "        \"\"\"\n",
        "        Processa os documentos para criar os vetores de resumo e de chunks detalhados.\n",
        "        \"\"\"\n",
        "        # Criar resumos dos documentos\n",
        "        modelo_resumo = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
        "        cadeia_resumo = load_summarize_chain(modelo_resumo, chain_type=\"map_reduce\")\n",
        "\n",
        "        def resumir_documento(doc):\n",
        "            \"\"\" Resume um documento. \"\"\"\n",
        "            saida_resumo = cadeia_resumo.invoke([doc])\n",
        "            resumo = saida_resumo['output_text']\n",
        "            return Document(\n",
        "                page_content=resumo,\n",
        "                metadata={\"origem\": \"resumo\", self.metadado_filtro: doc.metadata.get(self.metadado_filtro, \"\")}\n",
        "            )\n",
        "\n",
        "        # Criar resumos\n",
        "        resumos = [resumir_documento(doc) for doc in self.documentos]\n",
        "\n",
        "        # Criar chunks detalhados\n",
        "        separador_texto = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap,\n",
        "            length_function=len\n",
        "        )\n",
        "        chunks_detalhados = separador_texto.split_documents(self.documentos)\n",
        "\n",
        "        # Adicionar metadados aos chunks detalhados\n",
        "        for i, chunk in enumerate(chunks_detalhados):\n",
        "            chunk.metadata.update({\"id_chunk\": i, \"origem\": \"detalhado\", self.metadado_filtro: chunk.metadata.get(self.metadado_filtro, \"\")})\n",
        "\n",
        "        # Criar armazéns vetoriais\n",
        "        self.resumo_store = FAISS.from_documents(resumos, self.embeddings)\n",
        "        self.detalhe_store = FAISS.from_documents(chunks_detalhados, self.embeddings)\n",
        "\n",
        "    def recuperar(self, consulta, k_resumos=3, k_chunks=5):\n",
        "        \"\"\"\n",
        "        Recupera chunks detalhados com base em uma consulta utilizando a estrutura hierárquica.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta de pesquisa.\n",
        "            k_resumos: Número de resumos principais a recuperar.\n",
        "            k_chunks: Número de chunks detalhados a recuperar por resumo.\n",
        "\n",
        "        Returns:\n",
        "            Lista de chunks detalhados relevantes.\n",
        "        \"\"\"\n",
        "        if not self.resumo_store or not self.detalhe_store:\n",
        "            raise ValueError(\"Os documentos ainda não foram processados.\")\n",
        "\n",
        "        resumos_relevantes = self.resumo_store.similarity_search(consulta, k=k_resumos)\n",
        "        chunks_relevantes = []\n",
        "        print(\"Resumos:\")\n",
        "        print(resumos_relevantes)\n",
        "        for resumo in resumos_relevantes:\n",
        "            source = resumo.metadata.get(self.metadado_filtro, \"\")\n",
        "            filtro_source = lambda metadata: metadata.get(self.metadado_filtro, \"\") == source\n",
        "            chunks = self.detalhe_store.similarity_search(consulta, k=k_chunks, filter=filtro_source)\n",
        "            chunks_relevantes.extend(chunks)\n",
        "\n",
        "        return chunks_relevantes"
      ],
      "metadata": {
        "id": "pE7s4O6VJKLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando os documentos\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths = [\"https://www.cnnbrasil.com.br/tecnologia/nova-ferramenta-do-chatgpt-acaba-com-buscas-online-especialistas-respondem/\",\n",
        "                 \"https://www.cnnbrasil.com.br/tecnologia/perguntamos-ao-chatgpt-por-que-ele-e-melhor-que-o-deepseek-veja-a-resposta/\",\n",
        "                 \"https://www.cnnbrasil.com.br/economia/macroeconomia/ue-estabelece-diretrizes-sobre-uso-indevido-de-ia/\",\n",
        "                 \"https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\"]\n",
        "    )\n",
        "\n",
        "documentos = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPoWDGfOOheK",
        "outputId": "a09809a9-a680-4002-d0df-336c4af7cc60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a instância da classe\n",
        "recuperacao = RecuperacaoHierarquica(documentos, chunk_size=600, chunk_overlap=200, metadado_filtro = \"source\")"
      ],
      "metadata": {
        "id": "Es0K-2IsRRYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando uma busca com a query desejada\n",
        "consulta = \"quem é elon musk?\"\n",
        "chunks_resultantes = recuperacao.recuperar(consulta, k_resumos=1, k_chunks=3)\n",
        "\n",
        "# Exibindo os resultados\n",
        "for i, chunk in enumerate(chunks_resultantes):\n",
        "    print(f\"Resultado {i + 1}:\")\n",
        "    print(f\"Metadados: {chunk.metadata}\")\n",
        "    print(f\"Conteúdo: {chunk.page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELCWb25eRRWX",
        "outputId": "59567694-b161-43ef-ed81-7db70cce641c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumos:\n",
            "[Document(id='ef177bb1-fcdf-45af-aa5f-98e6ea962b2f', metadata={'origem': 'resumo', 'source': 'https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/'}, page_content='Uma idosa de 69 anos em Goiás foi vítima de um golpe, perdendo mais de R$150 mil ao acreditar que mantinha um relacionamento virtual com Elon Musk. O golpista, que se fez passar pelo bilionário, manipulou emocionalmente a vítima e pediu empréstimos para \"abastecer sua aeronave\". Investigações revelam uma organização criminosa por trás do golpe, e a polícia recomenda que familiares monitorem as interações financeiras de idosos com dispositivos móveis.')]\n",
            "Resultado 1:\n",
            "Metadados: {'source': 'https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/', 'title': 'Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO | CNN Brasil', 'description': 'Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por um aplicativo de mensagens', 'language': 'pt-BR', 'id_chunk': 70, 'origem': 'detalhado'}\n",
            "Conteúdo: estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança\n",
            "\n",
            "Resultado 2:\n",
            "Metadados: {'source': 'https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/', 'title': 'Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO | CNN Brasil', 'description': 'Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por um aplicativo de mensagens', 'language': 'pt-BR', 'id_chunk': 66, 'origem': 'detalhado'}\n",
            "Conteúdo: Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO | CNN Brasil                                           Ao Vivo    COP 30    Política    Política    Notícias     William Waack      Internacional    Nacional    Economia    Economia    Notícias     Investimentos     Mercado     Cotações     Loterias    Loterias   Mega Sena   Quina   Lotofacil   Lotomania   Duplasena   Loteria Federal   Timemania   Loteca   Dia de Sorte   Super Sete       CNN Money    Entretenimento    Saúde    Esportes    Esportes    Notícias     Olimpíadas     Futebol    Futebol\n",
            "\n",
            "Resultado 3:\n",
            "Metadados: {'source': 'https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/', 'title': 'Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO | CNN Brasil', 'description': 'Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por um aplicativo de mensagens', 'language': 'pt-BR', 'id_chunk': 69, 'origem': 'detalhado'}\n",
            "Conteúdo: A seguir                        Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO   Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por um aplicativo de mensagens    Ana Julia BertolacciniAlan Cardosoda CNN*  , em São Paulo     04/02/2025 às 18:17 | Atualizado 04/02/2025 às 18:17           Idosa perde mais de R$150 mil reais em golpe, acreditando estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothetical Questions & HyDE\n",
        "<img src=\"https://i.ibb.co/hFNBkP6V/Lead-Entra-na-Lista-19.png\">\n",
        "\n",
        "Existem duas técnicas poderosas que envolvem a geração de embeddings especiais. Vamos ver cada uma delas:\n"
      ],
      "metadata": {
        "id": "ITAwAtJeTQOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings de Perguntas\n",
        "A primeira abordagem é fascinante: **pedimos para a I.A. gerar uma pergunta para cada chunk e criamos embeddings dessas perguntas**.\n",
        "\n",
        "Por que isso é interessante? Porque na hora da busca, **a similaridade semântica entre a query e a pergunta gerada é muito maior** do que seria com o chunk original. Afinal, ambas são perguntas!"
      ],
      "metadata": {
        "id": "qcVXHPYlXUbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "class RecuperacaoHyQuestion:\n",
        "    def __init__(self, documentos, chunk_size=500, chunk_overlap=100, metadado_filtro=\"source\"):\n",
        "        \"\"\"\n",
        "        Classe para recuperação hierárquica utilizando perguntas geradas automaticamente para cada chunk.\n",
        "\n",
        "        Args:\n",
        "            documentos: Lista de documentos no formato Document.\n",
        "            chunk_size: Tamanho de cada chunk ao dividir os documentos.\n",
        "            chunk_overlap: Sobreposição entre chunks consecutivos.\n",
        "            metadado_filtro: Chave de metadados para filtragem.\n",
        "        \"\"\"\n",
        "        self.documentos = documentos\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.metadado_filtro = metadado_filtro\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
        "\n",
        "        self.question_prompt = PromptTemplate(\n",
        "            input_variables=[\"chunk\"],\n",
        "            template=\"\"\"Dado o seguinte trecho de texto, gere 3 perguntas que podem ser respondida por esse conteúdo:\n",
        "\n",
        "            Trecho: \"{chunk}\"\n",
        "\n",
        "            As perguntas devem seguir a seguinte estrutura:\n",
        "            Pergunta1? Pergunta2? Pergunta3?\n",
        "\n",
        "            As perguntas devem ser mais gerais e sem mencionar trechos específicos.\n",
        "\n",
        "            Perguntas:\"\"\",\n",
        "        )\n",
        "        self.question_chain = self.question_prompt | self.llm\n",
        "\n",
        "        # Processar documentos e criar armazém vetorial\n",
        "        self.vectorstore = self.processar_documentos()\n",
        "\n",
        "    def processar_documentos(self):\n",
        "        \"\"\"\n",
        "        Processa os documentos para criar embeddings de perguntas associadas aos chunks.\n",
        "        \"\"\"\n",
        "        separador_texto = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap,\n",
        "            length_function=len\n",
        "        )\n",
        "        chunks = separador_texto.split_documents(self.documentos)\n",
        "\n",
        "        # Gerar perguntas para cada chunk e criar documentos com metadados associados\n",
        "        documentos_perguntas = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            pergunta_gerada = self.gerar_pergunta(chunk.page_content)\n",
        "            doc_pergunta = Document(\n",
        "                page_content=pergunta_gerada,\n",
        "                metadata={\"id_chunk\": i, \"chunk_content\": chunk.page_content, self.metadado_filtro: chunk.metadata.get(self.metadado_filtro, \"\")}\n",
        "            )\n",
        "            documentos_perguntas.append(doc_pergunta)\n",
        "\n",
        "        return FAISS.from_documents(documentos_perguntas, self.embeddings)\n",
        "\n",
        "    def gerar_pergunta(self, chunk):\n",
        "        \"\"\"\n",
        "        Gera uma pergunta para um chunk de texto usando um modelo de IA.\n",
        "        \"\"\"\n",
        "        return self.question_chain.invoke({\"chunk\": chunk}).content\n",
        "\n",
        "    def recuperar(self, consulta, k=3):\n",
        "        \"\"\"\n",
        "        Recupera os chunks mais relevantes com base em perguntas associadas.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta de pesquisa.\n",
        "            k: Número de chunks a recuperar.\n",
        "\n",
        "        Returns:\n",
        "            Lista de chunks de conteúdo correspondentes às perguntas recuperadas.\n",
        "        \"\"\"\n",
        "        perguntas_relevantes = self.vectorstore.similarity_search(consulta, k=k)\n",
        "        chunks_relevantes = [Document(page_content=p.metadata[\"chunk_content\"], metadata=p.metadata) for p in perguntas_relevantes]\n",
        "\n",
        "        return chunks_relevantes, perguntas_relevantes"
      ],
      "metadata": {
        "id": "MJszhcoiXQZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a instância da classe\n",
        "recuperacao_hyquestion = RecuperacaoHyQuestion(documentos, chunk_size=600, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "liAI8MN2GX6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando uma busca com a query desejada\n",
        "consulta = \"quem é elon musk?\"\n",
        "chunks_resultantes, perguntas_relevantes = recuperacao_hyquestion.recuperar(consulta, k=3)\n",
        "\n",
        "# Exibindo as perguntas hipotéticas geradas\n",
        "print(\"Perguntas Relevantes Geradas:\\n\", perguntas_relevantes)\n",
        "\n",
        "# Exibindo os resultados da busca\n",
        "print(\"\\nChunks Recuperados:\")\n",
        "for i, chunk in enumerate(chunks_resultantes):\n",
        "    print(f\"Resultado {i + 1}:\")\n",
        "    print(f\"Source: {chunk.metadata.get('source')}\")\n",
        "    print(f\"Conteúdo: {chunk.page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgvypZWgHoVi",
        "outputId": "3c446ddc-2ba5-428c-8304-3d4de6c3adf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perguntas Relevantes Geradas:\n",
            " [Document(id='b0357630-3177-4487-a301-39b16e4e5ac9', metadata={'id_chunk': 68, 'chunk_content': 'Equipe CNN Brasil    Newsletters    Colunistas       Sobre a CNN    Política de Privacidade    Termos de Uso    Fale com a CNN    Distribuição do Sinal    Faça parte da Equipe CNN         Ao vivo    Política    WW    Economia    Esportes    Pop    Viagem & Gastronomia                               seg - sex    Apresentação     Ao vivo            AO VIVO: CNN NOVO DIA - 05/02/2025             Switch      A seguir                        Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO   Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por', 'source': 'https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/'}, page_content='Quem foi a vítima do golpe mencionado no trecho? Qual foi o valor que a idosa perdeu no empréstimo? Em que estado ocorreu o golpe em que o golpista se passou por Elon Musk?'), Document(id='7619a513-02a3-489c-a5db-22cad617acb2', metadata={'id_chunk': 70, 'chunk_content': 'estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança', 'source': 'https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/'}, page_content='Quem era a idosa que acreditava estar em um relacionamento com Elon Musk? Quais foram os valores dos empréstimos que a idosa realizou? Para que o golpista solicitava os valores à vítima?'), Document(id='dcbc57da-9fc2-4c21-8337-c39ba255884b', metadata={'id_chunk': 72, 'chunk_content': 'os dois empréstimos que somam mais de R$150 mil, queria vender a própria casa para mandar os valores solicitados pelo golpista. O imóvel é avaliado em R$ 500 mil. Leia Mais        Idosa acredita estar namorando Elon Musk, cai em golpe e perde R$ 4 mil no PR      Homem é indiciado após extorquir madrasta por causa de prática de \"swing\"      Criminosos fingem ser da assessoria de deputado de SP para aplicar golpes  De acordo com informações repassadas pelo delegado do caso, o suposto Elon Musk adicionou a vítima pelo Facebook e posteriormente, passou a trocar mensagens com a idosa pelo', 'source': 'https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/'}, page_content='1. Qual foi o valor total dos empréstimos que a vítima considerou fazer?  \\n2. Qual é a avaliação do imóvel que a idosa estava disposta a vender?  \\n3. Como o suposto Elon Musk entrou em contato com a vítima?  ')]\n",
            "\n",
            "Chunks Recuperados:\n",
            "Resultado 1:\n",
            "Source: https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\n",
            "Conteúdo: Equipe CNN Brasil    Newsletters    Colunistas       Sobre a CNN    Política de Privacidade    Termos de Uso    Fale com a CNN    Distribuição do Sinal    Faça parte da Equipe CNN         Ao vivo    Política    WW    Economia    Esportes    Pop    Viagem & Gastronomia                               seg - sex    Apresentação     Ao vivo            AO VIVO: CNN NOVO DIA - 05/02/2025             Switch      A seguir                        Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO   Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por\n",
            "\n",
            "Resultado 2:\n",
            "Source: https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\n",
            "Conteúdo: estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança\n",
            "\n",
            "Resultado 3:\n",
            "Source: https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\n",
            "Conteúdo: os dois empréstimos que somam mais de R$150 mil, queria vender a própria casa para mandar os valores solicitados pelo golpista. O imóvel é avaliado em R$ 500 mil. Leia Mais        Idosa acredita estar namorando Elon Musk, cai em golpe e perde R$ 4 mil no PR      Homem é indiciado após extorquir madrasta por causa de prática de \"swing\"      Criminosos fingem ser da assessoria de deputado de SP para aplicar golpes  De acordo com informações repassadas pelo delegado do caso, o suposto Elon Musk adicionou a vítima pelo Facebook e posteriormente, passou a trocar mensagens com a idosa pelo\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HyDE (Hypothetical Document Embedding)\n",
        "Já o HyDE faz o caminho inverso: **pedimos para a I.A. gerar uma possível resposta para nossa query e usamos o embedding dessa resposta** junto com o embedding da query para melhorar a qualidade da busca.\n",
        "\n",
        "Legal, não é? Mas qual técnica escolher? Vamos ver os cenários ideais para cada uma?"
      ],
      "metadata": {
        "id": "i-kXqSCvXWYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "class RecuperacaoHyDE:\n",
        "    def __init__(self, documentos, chunk_size=500, chunk_overlap=100, metadado_filtro=\"source\"):\n",
        "        \"\"\"\n",
        "        Classe para recuperação hierárquica utilizando HyDE (Hypothetical Document Embeddings).\n",
        "\n",
        "        Args:\n",
        "            documentos: Lista de documentos no formato Document.\n",
        "            chunk_size: Tamanho de cada chunk ao dividir os documentos.\n",
        "            chunk_overlap: Sobreposição entre chunks consecutivos.\n",
        "            metadado_filtro: Chave de metadados para filtragem.\n",
        "        \"\"\"\n",
        "        self.documentos = documentos\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.metadado_filtro = metadado_filtro\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
        "\n",
        "        # Processar documentos e criar armazém vetorial\n",
        "        self.vectorstore = self.processar_documentos()\n",
        "\n",
        "        self.hyde_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"chunk_size\"],\n",
        "            template=\"\"\"Dada a pergunta '{query}', gere um documento hipotético que responda diretamente a essa pergunta utilizando as principais palavras-chave da área relacionada à pergunta.\n",
        "            O documento deve ser detalhado e aprofundado, com EXATAMENTE {chunk_size} caracteres.\"\"\",\n",
        "        )\n",
        "        self.hyde_chain = self.hyde_prompt | self.llm\n",
        "\n",
        "    def processar_documentos(self):\n",
        "        \"\"\"\n",
        "        Processa os documentos para criar os vetores de embeddings.\n",
        "        \"\"\"\n",
        "        separador_texto = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap,\n",
        "            length_function=len\n",
        "        )\n",
        "        chunks = separador_texto.split_documents(self.documentos)\n",
        "\n",
        "        # Adicionar metadados aos chunks\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata.update({\"id_chunk\": i, self.metadado_filtro: chunk.metadata.get(self.metadado_filtro, \"\")})\n",
        "\n",
        "        return FAISS.from_documents(chunks, self.embeddings)\n",
        "\n",
        "    def gerar_documento_hipotetico(self, consulta):\n",
        "        \"\"\"\n",
        "        Gera um documento hipotético baseado na consulta utilizando o modelo de linguagem.\n",
        "        \"\"\"\n",
        "        input_variables = {\"query\": consulta, \"chunk_size\": self.chunk_size}\n",
        "        return self.hyde_chain.invoke(input_variables).content\n",
        "\n",
        "    def recuperar(self, consulta, k=3):\n",
        "        \"\"\"\n",
        "        Recupera chunks mais relevantes utilizando HyDE para reformular a consulta.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta de pesquisa.\n",
        "            k: Número de chunks a recuperar.\n",
        "\n",
        "        Returns:\n",
        "            Lista de chunks mais semelhantes e o documento hipotético gerado.\n",
        "        \"\"\"\n",
        "        doc_hipotetico = self.gerar_documento_hipotetico(consulta)\n",
        "        chunks_relevantes = self.vectorstore.similarity_search(doc_hipotetico, k=k)\n",
        "        return chunks_relevantes, doc_hipotetico"
      ],
      "metadata": {
        "id": "7_dumOpBXXAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a instância da classe\n",
        "recuperacao_hyde = RecuperacaoHyDE(documentos, chunk_size=600, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "3L1zj2MICkqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando uma busca com a query desejada\n",
        "consulta = \"quem é elon musk?\"\n",
        "chunks_resultantes, doc_hipotetico = recuperacao_hyde.recuperar(consulta, k=3)\n",
        "\n",
        "# Exibindo o documento hipotético gerado\n",
        "print(\"Documento Hipotético Gerado:\\n\", doc_hipotetico)\n",
        "\n",
        "# Exibindo os resultados da busca\n",
        "print(\"\\nChunks Recuperados:\")\n",
        "for i, chunk in enumerate(chunks_resultantes):\n",
        "    print(f\"Resultado {i + 1}:\")\n",
        "    print(f\"Source: {chunk.metadata.get('source')}\")\n",
        "    print(f\"Conteúdo: {chunk.page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkCzAWACC-CV",
        "outputId": "53bdad75-eb28-4cb4-a762-51dcd994cc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Hipotético Gerado:\n",
            " **Quem é Elon Musk?**\n",
            "\n",
            "Elon Musk é um empresário e inventor sul-africano, conhecido por sua visão futurista e inovações tecnológicas. Nascido em 28 de junho de 1971, Musk co-fundou a Zip2 e a X.com, que se tornou o PayPal. Ele é o CEO da SpaceX, onde desenvolve foguetes reutilizáveis e busca a colonização de Marte. Como CEO e produto arquiteto da Tesla, Musk revolucionou a indústria automotiva com veículos elétricos e soluções de energia sustentável. Além disso, fundou a Neuralink, focada em interface cérebro-máquina, e a The Boring Company, que visa melhorar o transporte urbano. Musk é uma figura polarizadora, admirada e criticada por suas ideias audaciosas.\n",
            "\n",
            "Chunks Recuperados:\n",
            "Resultado 1:\n",
            "Source: https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\n",
            "Conteúdo: estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança\n",
            "\n",
            "Resultado 2:\n",
            "Source: https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\n",
            "Conteúdo: A seguir                        Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO   Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por um aplicativo de mensagens    Ana Julia BertolacciniAlan Cardosoda CNN*  , em São Paulo     04/02/2025 às 18:17 | Atualizado 04/02/2025 às 18:17           Idosa perde mais de R$150 mil reais em golpe, acreditando estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos\n",
            "\n",
            "Resultado 3:\n",
            "Source: https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\n",
            "Conteúdo: Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO | CNN Brasil                                           Ao Vivo    COP 30    Política    Política    Notícias     William Waack      Internacional    Nacional    Economia    Economia    Notícias     Investimentos     Mercado     Cotações     Loterias    Loterias   Mega Sena   Quina   Lotofacil   Lotomania   Duplasena   Loteria Federal   Timemania   Loteca   Dia de Sorte   Super Sete       CNN Money    Entretenimento    Saúde    Esportes    Esportes    Notícias     Olimpíadas     Futebol    Futebol\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Window Retriever\n",
        "<img src=\"https://i.ibb.co/mC4qz4g7/Lead-Entra-na-Lista-20.png\">\n",
        "\n",
        "Quando queremos encontrar a melhor correspondência entre uma consulta e um documento, precisamos de precisão, certo? Para isso, **embutimos cada sentença separadamente**, permitindo uma busca eficiente baseada na similaridade de cosseno.\n",
        "\n",
        "Mas tem um detalhe: **se usarmos apenas uma sentença isolada, podemos perder contexto**. A solução? **Expandimos a janela de contexto**, adicionando `k` sentenças antes e depois da sentença recuperada.\n",
        "\n",
        "Isso faz toda a diferença: primeiro encontramos a sentença mais relevante no índice, depois ampliamos o contexto para que o LLM tenha mais informações ao gerar a resposta. Assim, garantimos que a busca é precisa e que o modelo raciocina com um contexto mais rico.\n",
        "\n",
        "Vamos ver como isso funciona na prática?"
      ],
      "metadata": {
        "id": "INTO00sDLJWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "class RecuperacaoPorSentenca:\n",
        "    def __init__(self, documentos, chunk_size=500, chunk_overlap=100, metadado_filtro=\"source\", contexto_expandido=2):\n",
        "        \"\"\"\n",
        "        Classe para recuperação baseada em embeddings de sentenças individuais.\n",
        "\n",
        "        Args:\n",
        "            documentos: Lista de documentos no formato Document.\n",
        "            metadado_filtro: Chave de metadados para filtragem.\n",
        "            contexto_expandido: Número de sentenças antes e depois da sentença encontrada a serem incluídas no contexto.\n",
        "        \"\"\"\n",
        "        self.documentos = documentos\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.metadado_filtro = metadado_filtro\n",
        "        self.contexto_expandido = contexto_expandido\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
        "\n",
        "        # Processar documentos e criar armazém vetorial\n",
        "        self.vectorstore, self.sentencas_processadas = self.processar_documentos()\n",
        "\n",
        "    def processar_documentos(self):\n",
        "        \"\"\"\n",
        "        Processa os documentos para criar embeddings de sentenças individuais.\n",
        "        \"\"\"\n",
        "        separador_texto = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap,\n",
        "            length_function=len\n",
        "        )\n",
        "        sentencas_processadas = []\n",
        "        documentos_sentencas = []\n",
        "\n",
        "        for doc in self.documentos:\n",
        "            sentencas = separador_texto.split_text(doc.page_content)\n",
        "            for i, sentenca in enumerate(sentencas):\n",
        "                documento_sentenca = Document(\n",
        "                    page_content=sentenca,\n",
        "                    metadata={\"id_sentenca\": len(sentencas_processadas), \"source\": doc.metadata.get(self.metadado_filtro, \"\")}\n",
        "                )\n",
        "                sentencas_processadas.append(sentenca)\n",
        "                documentos_sentencas.append(documento_sentenca)\n",
        "\n",
        "        return FAISS.from_documents(documentos_sentencas, self.embeddings), sentencas_processadas\n",
        "\n",
        "    def recuperar(self, consulta, k=3, N=2):\n",
        "        \"\"\"\n",
        "        Recupera sentenças mais relevantes e expande o contexto ao redor da sentença encontrada.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta de pesquisa.\n",
        "            k: Número de sentenças a recuperar.\n",
        "            N: Número de sentenças antes e depois de cada sentença recuperada.\n",
        "\n",
        "        Returns:\n",
        "            Lista de sentenças estendidas para contexto mais amplo.\n",
        "        \"\"\"\n",
        "        sentencas_relevantes = self.vectorstore.similarity_search(consulta, k=k)\n",
        "        contexto_ampliado = []\n",
        "\n",
        "        for sentenca_recuperada in sentencas_relevantes:\n",
        "            id_sentenca = sentenca_recuperada.metadata[\"id_sentenca\"]\n",
        "            inicio = max(0, id_sentenca - N)\n",
        "            fim = min(len(self.sentencas_processadas), id_sentenca + N + 1)\n",
        "            contexto = \" | \".join(self.sentencas_processadas[inicio:fim])\n",
        "            contexto_ampliado.append(Document(page_content=contexto, metadata=sentenca_recuperada.metadata))\n",
        "\n",
        "        return contexto_ampliado"
      ],
      "metadata": {
        "id": "3bdR5InWSfaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a instância da classe\n",
        "recuperacao_sentenca = RecuperacaoPorSentenca(documentos, chunk_size=600, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "F4IRcJfMYMZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando uma busca com a query desejada\n",
        "consulta = \"quem é elon musk?\"\n",
        "chunks_resultantes = recuperacao_sentenca.recuperar(consulta, k=2, N = 2)\n",
        "\n",
        "# Exibindo os resultados da busca\n",
        "print(\"\\nChunks Recuperados:\")\n",
        "for i, chunk in enumerate(chunks_resultantes):\n",
        "    print(f\"Resultado {i + 1}:\")\n",
        "    print(f\"Source: {chunk.metadata.get('source')}\")\n",
        "    print(f\"Conteúdo: {chunk.page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba-siRAiZkPi",
        "outputId": "381299b3-3bbc-4991-d71f-81c387743c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunks Recuperados:\n",
            "Resultado 1:\n",
            "Source: https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\n",
            "Conteúdo: Equipe CNN Brasil    Newsletters    Colunistas       Sobre a CNN    Política de Privacidade    Termos de Uso    Fale com a CNN    Distribuição do Sinal    Faça parte da Equipe CNN         Ao vivo    Política    WW    Economia    Esportes    Pop    Viagem & Gastronomia                               seg - sex    Apresentação     Ao vivo            AO VIVO: CNN NOVO DIA - 05/02/2025             Switch      A seguir                        Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO   Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por | A seguir                        Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO   Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por um aplicativo de mensagens    Ana Julia BertolacciniAlan Cardosoda CNN*  , em São Paulo     04/02/2025 às 18:17 | Atualizado 04/02/2025 às 18:17           Idosa perde mais de R$150 mil reais em golpe, acreditando estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos | estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança | dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança celebrado entre os dois, nos quais o criminoso instigou a vítima a cortar sua própria mão, a fim de provar o amor que tinha pelo bilionário Elon Musk.Emocionalmente envolvida, a mulher, além de realizar os dois empréstimos que somam mais de R$150 mil, queria vender a própria casa para mandar os valores solicitados pelo golpista. O imóvel é avaliado em R$ 500 mil. Leia Mais        Idosa acredita | os dois empréstimos que somam mais de R$150 mil, queria vender a própria casa para mandar os valores solicitados pelo golpista. O imóvel é avaliado em R$ 500 mil. Leia Mais        Idosa acredita estar namorando Elon Musk, cai em golpe e perde R$ 4 mil no PR      Homem é indiciado após extorquir madrasta por causa de prática de \"swing\"      Criminosos fingem ser da assessoria de deputado de SP para aplicar golpes  De acordo com informações repassadas pelo delegado do caso, o suposto Elon Musk adicionou a vítima pelo Facebook e posteriormente, passou a trocar mensagens com a idosa pelo\n",
            "\n",
            "Resultado 2:\n",
            "Source: https://www.cnnbrasil.com.br/nacional/centro-oeste/go/golpista-se-passa-por-elon-musk-e-idosa-perde-r150-mil-em-emprestimo-em-go/\n",
            "Conteúdo: Ásia     África     Oriente Médio     Rússia     China       Entretenimento     Big Brother Brasil     Celebridades     Cinema     Televisão     Streaming     Shows     Carnaval 2024     Música     Horóscopo       Esportes     Agenda de Jogos     Tabela Brasileirão Série A     Tabela Brasileirão Série B     Tabela Eliminatórias 2026     Copa Libertadores da América     Olimpíadas     Basquete     Vôlei     Automobilismo     Golf     e-Sports       Saúde     Roberto Kalil - Sinais Vitais     Alimentação     Exercícios Físicos     Vacinação     Câncer     Drogas     Obesidade | Basquete     Vôlei     Automobilismo     Golf     e-Sports       Saúde     Roberto Kalil - Sinais Vitais     Alimentação     Exercícios Físicos     Vacinação     Câncer     Drogas     Obesidade       Tecnologia     Inteligência Artificial     Nasa     Ciência     Curiosidades       Viagem & Gastronomia     Viagem     Gastronomia             Sobre a CNN Brasil     Aviso Legal e Política de Privacidade     Termos de Uso     Fale com a CNN     Faça parte da Equipe CNN     © 2025 Cable News Network Brasil. Uma empresa NOVUS MÍDIA. Todos os direitos reservados. | Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO | CNN Brasil                                           Ao Vivo    COP 30    Política    Política    Notícias     William Waack      Internacional    Nacional    Economia    Economia    Notícias     Investimentos     Mercado     Cotações     Loterias    Loterias   Mega Sena   Quina   Lotofacil   Lotomania   Duplasena   Loteria Federal   Timemania   Loteca   Dia de Sorte   Super Sete       CNN Money    Entretenimento    Saúde    Esportes    Esportes    Notícias     Olimpíadas     Futebol    Futebol | Duplasena   Loteria Federal   Timemania   Loteca   Dia de Sorte   Super Sete       CNN Money    Entretenimento    Saúde    Esportes    Esportes    Notícias     Olimpíadas     Futebol    Futebol   Brasileirão      Basquete     Automobilismo     Tênis     E-Sports      Tecnologia    Lifestyle    Viagem & Gastronomia    Auto    Educação    CNN Talks    Fórum CNN    Blogs CNN    Colunas CNN       Programação    Equipe CNN Brasil    Newsletters    Colunistas       Sobre a CNN    Política de Privacidade    Termos de Uso    Fale com a CNN    Distribuição do Sinal    Faça parte da Equipe CNN | Equipe CNN Brasil    Newsletters    Colunistas       Sobre a CNN    Política de Privacidade    Termos de Uso    Fale com a CNN    Distribuição do Sinal    Faça parte da Equipe CNN         Ao vivo    Política    WW    Economia    Esportes    Pop    Viagem & Gastronomia                               seg - sex    Apresentação     Ao vivo            AO VIVO: CNN NOVO DIA - 05/02/2025             Switch      A seguir                        Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO   Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Querying RAG\n",
        "<img src=\"https://i.ibb.co/My56ZTBz/Lead-Entra-na-Lista-21.png\">\n",
        "\n",
        "Quando fazemos uma busca, queremos que o sistema entenda exatamente o que estamos pedindo, certo? É aí que entra o **self-querying retrieval**, atuando como um **tradutor inteligente** entre a linguagem natural do usuário e a busca estruturada.\n",
        "\n",
        "Em vez de apenas procurar por termos semelhantes, **o sistema usa um LLM para extrair tanto a intenção semântica quanto filtros de metadados** (como ano, gênero, classificação). Isso permite refinar a pesquisa e obter resultados muito mais precisos.\n",
        "\n",
        "Pense no seguinte exemplo: se perguntamos _\"Quero um filme de ficção científica após 2010 com nota acima de 8\"_, o sistema automaticamente identifica três filtros – **gênero: ficção científica, ano: >2010, nota: >8** – e combina esses critérios com a busca semântica.\n",
        "\n",
        "Ou seja, em vez de simplesmente retornar qualquer filme relacionado à consulta, ele **filtra e prioriza os mais relevantes**. Bem mais eficiente, não acha?\n",
        "\n",
        "Vamos ver como implementar isso?"
      ],
      "metadata": {
        "id": "K2OiYdj8gllM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    Document(\n",
        "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
        "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
        "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
        "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
        "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Toys come alive and have a blast doing so\",\n",
        "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
        "        metadata={\n",
        "            \"year\": 1979,\n",
        "            \"director\": \"Andrei Tarkovsky\",\n",
        "            \"genre\": \"thriller\",\n",
        "            \"rating\": 9.9,\n",
        "        },\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "b9oWjTX7oAWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.chains.query_constructor.schema import AttributeInfo\n",
        "\n",
        "def definir_campo_metadado(nomes_metadados: List[str], descricoes_metadados: List[str], tipos_metadados: list):\n",
        "    \"\"\"\n",
        "    Define os campos de metadados para a recuperação self-querying com base nos nomes fornecidos.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        AttributeInfo(\n",
        "            name=nomes_metadados[k],\n",
        "            description=descricoes_metadados[k],\n",
        "            type=str(tipos_metadados[k].__name__),\n",
        "        ) for k in range(len(nomes_metadados))\n",
        "    ]\n",
        "\n",
        "nomes_metadados = [\"year\", \"rating\"]\n",
        "descricoes_metadados = [\"O ano de publicação do filme\", \"A avaliação ou nota do filme, entre 0 e 10\"]\n",
        "tipos_metadados = [int, float]"
      ],
      "metadata": {
        "id": "J394IozQnAMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class QueryResponse(BaseModel):\n",
        "    query: str = Field(\n",
        "        description=(\n",
        "            \"Texto da busca no conteúdo dos documentos. Essa string representa os termos essenciais que serão utilizados para \"\n",
        "            \"encontrar documentos relevantes. Exemplo: 'redes neurais', 'história da computação', 'avanços em IA'.\"\n",
        "        )\n",
        "    )\n",
        "    filtro: List[str] = Field(\n",
        "        description=(\n",
        "            \"Lista de filtros a serem aplicados na busca. Cada elemento da lista segue o formato: \"\n",
        "            \"'campo|operador|valor'. O campo representa o atributo do metadado a ser filtrado, \"\n",
        "            \"Os campos aceitos são: \" + \", \".join(nomes_metadados) + \".\"\n",
        "            \"o operador define a condição de filtragem ('eq' para igual, 'gt' para maior que, 'lt' para menor que, etc.), \"\n",
        "            \"e o valor representa a condição. Exemplo: \"\n",
        "            \"['ano|gt|2020'] para recuperar documentos publicados depois de 2020; \"\n",
        "            \"['categoria|eq|tecnologia', 'autor|eq|Alan Turing'] para filtrar por categoria e autor específico.\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "class RecuperacaoSelfQuerying:\n",
        "    def __init__(self, documentos, nomes_metadados: List[str], descricoes_metadados: List[str], tipos_metadados: list, chunk_size=500, chunk_overlap=100):\n",
        "        \"\"\"\n",
        "        Classe para recuperação self-querying utilizando filtros baseados em metadados extraídos automaticamente.\n",
        "\n",
        "        Args:\n",
        "            documentos: Lista de documentos no formato Document.\n",
        "            nomes_metadados: Lista de nomes de metadados aceitos para filtragem.\n",
        "            chunk_size: Tamanho de cada chunk ao dividir os documentos.\n",
        "            chunk_overlap: Sobreposição entre chunks consecutivos.\n",
        "        \"\"\"\n",
        "        self.documentos = documentos\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.nomes_metadados = nomes_metadados\n",
        "        self.descricoes_metadados = descricoes_metadados\n",
        "        self.tipos_metadados = tipos_metadados\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
        "        self.metadata_field_info = definir_campo_metadado(nomes_metadados, descricoes_metadados, tipos_metadados)\n",
        "        self.document_contents = \"Texto informativo contendo diversas categorias de conhecimento.\"\n",
        "\n",
        "        # Processar documentos e criar armazém vetorial\n",
        "        self.vectorstore = self.processar_documentos()\n",
        "\n",
        "        # Criar LLM com saída estruturada\n",
        "        self.structured_llm = self.llm.with_structured_output(QueryResponse)\n",
        "\n",
        "    def processar_documentos(self):\n",
        "        \"\"\"\n",
        "        Processa os documentos e cria embeddings armazenados no Chroma.\n",
        "        \"\"\"\n",
        "        return Chroma.from_documents(self.documentos, self.embeddings)\n",
        "\n",
        "    def recuperar(self, consulta, k=3):\n",
        "        resposta = self.structured_llm.invoke(consulta)\n",
        "        structured_query = resposta.model_dump()\n",
        "\n",
        "        # Constrói lista de sub-filtros\n",
        "        sub_filters = []\n",
        "        for filtro in structured_query.get(\"filtro\", []):\n",
        "            partes = filtro.split(\"|\")\n",
        "            if len(partes) == 3:\n",
        "                campo, operador, valor = partes\n",
        "                if valor.replace(\".\", \"\", 1).isdigit():\n",
        "                    valor = float(valor) if \".\" in valor else int(valor)\n",
        "\n",
        "                if operador == \"eq\":\n",
        "                    sub_filter = {campo: valor}\n",
        "                else:\n",
        "                    sub_filter = {campo: {f\"${operador}\": valor}}\n",
        "\n",
        "                sub_filters.append(sub_filter)\n",
        "\n",
        "        # Monta filtro_dict final\n",
        "        if not sub_filters:\n",
        "            # Nenhum filtro\n",
        "            filtro_dict = {}\n",
        "        elif len(sub_filters) == 1:\n",
        "            # Só um filtro => passa diretamente\n",
        "            filtro_dict = sub_filters[0]\n",
        "        else:\n",
        "            # Vários filtros => usa $and\n",
        "            filtro_dict = {\"$and\": sub_filters}\n",
        "\n",
        "        return self.vectorstore.search(\n",
        "            structured_query[\"query\"],\n",
        "            search_type=\"similarity\",\n",
        "            k=k,\n",
        "            filter=filtro_dict\n",
        "        )"
      ],
      "metadata": {
        "id": "P-lzRelYhSgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a instância da classe\n",
        "recuperacao_consulta_propria = RecuperacaoSelfQuerying(docs, nomes_metadados, descricoes_metadados, tipos_metadados, chunk_size=600, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "rb_GfnOwoKiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando uma busca com a query desejada\n",
        "consulta = \"quais filmes tem uma nota inferior a 8 e lançados depois de 1990 de ficção científica?\"\n",
        "chunks_resultantes = recuperacao_consulta_propria.recuperar(consulta, k=2)\n",
        "\n",
        "# Exibindo os resultados da busca\n",
        "print(\"\\nChunks Recuperados:\")\n",
        "for i, chunk in enumerate(chunks_resultantes):\n",
        "    print(f\"Resultado {i + 1}:\")\n",
        "    print(f\"Metadata: {chunk.metadata}\")\n",
        "    print(f\"Conteúdo: {chunk.page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS7lJCDSoXWM",
        "outputId": "c961be01-f6b8-4e6e-b79a-4e6d338d60db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunks Recuperados:\n",
            "Resultado 1:\n",
            "Metadata: {'genre': 'science fiction', 'rating': 7.7, 'year': 1993}\n",
            "Conteúdo: A bunch of scientists bring back dinosaurs and mayhem breaks loose\n",
            "\n",
            "Resultado 2:\n",
            "Metadata: {'genre': 'science fiction', 'rating': 7.7, 'year': 1993}\n",
            "Conteúdo: A bunch of scientists bring back dinosaurs and mayhem breaks loose\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parent Document Retriever\n",
        "<img src=\"https://i.ibb.co/mVxsZJnq/Lead-Entra-na-Lista-22.png\">\n",
        "\n",
        "O **Auto-Merging Retriever** melhora a recuperação ao combinar precisão com contexto. **Primeiro, buscamos chunks pequenos (leaf chunks), depois, se muitos pertencerem ao mesmo chunk-pai, os substituímos pelo bloco maior antes de enviar ao LLM.**\n",
        "\n",
        "Isso garante buscas detalhadas sem perder o contexto global, já que a indexação ocorre nos menores chunks, mas a resposta final pode usar trechos maiores quando necessário. **O resultado? Melhor equilíbrio entre granularidade e contexto.**\n",
        "\n",
        "Vamos ver isso na prática!"
      ],
      "metadata": {
        "id": "H9v9oIKmYGcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from typing import List\n",
        "\n",
        "class RecuperacaoDocumentoPai:\n",
        "    def __init__(self, documentos: List[Document], chunk_size_pai=2000, chunk_size_filho=400):\n",
        "        \"\"\"\n",
        "        Classe para recuperação de documentos utilizando a abordagem de Parent Document Retriever.\n",
        "\n",
        "        Args:\n",
        "            documentos: Lista de documentos no formato Document.\n",
        "            chunk_size_pai: Tamanho dos chunks dos documentos-pai.\n",
        "            chunk_size_filho: Tamanho dos chunks dos documentos-filho.\n",
        "        \"\"\"\n",
        "        self.documentos = documentos\n",
        "        self.chunk_size_pai = chunk_size_pai\n",
        "        self.chunk_size_filho = chunk_size_filho\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        # Criar splitters para documentos-pai e documentos-filho\n",
        "        self.parent_splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size_pai)\n",
        "        self.child_splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size_filho)\n",
        "\n",
        "        # Criar armazenamento vetorial e armazenamento dos documentos\n",
        "        self.vectorstore = Chroma(collection_name=\"documentos_pai\", embedding_function=self.embeddings)\n",
        "        self.store = InMemoryStore()\n",
        "\n",
        "        # Criar o retriever\n",
        "        self.retriever = ParentDocumentRetriever(\n",
        "            vectorstore=self.vectorstore,\n",
        "            docstore=self.store,\n",
        "            child_splitter=self.child_splitter,\n",
        "            parent_splitter=self.parent_splitter,\n",
        "        )\n",
        "\n",
        "        # Adicionar documentos ao retriever\n",
        "        self.retriever.add_documents(self.documentos)\n",
        "\n",
        "    def recuperar(self, consulta: str, k=3):\n",
        "        \"\"\"\n",
        "        Recupera documentos relevantes utilizando a abordagem Parent Document Retriever.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta de pesquisa em linguagem natural.\n",
        "            k: Número de documentos a recuperar.\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos relevantes recuperados.\n",
        "        \"\"\"\n",
        "        return self.retriever.invoke(consulta)\n"
      ],
      "metadata": {
        "id": "hr76O3rsR4dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando a classe implementada\n",
        "recuperacao = RecuperacaoDocumentoPai(documentos, chunk_size_pai = 2000, chunk_size_filho = 400)"
      ],
      "metadata": {
        "id": "t3RAaf9RY82F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a consulta\n",
        "consulta = \"Quem é elon musk?\"\n",
        "\n",
        "# Recuperando documentos mais relevantes\n",
        "resultados = recuperacao.recuperar(consulta, k=2)\n",
        "\n",
        "# Exibindo os resultados\n",
        "print(\"\\n### Documentos Recuperados ###\")\n",
        "for i, doc in enumerate(resultados):\n",
        "    print(f\"\\n📄 Documento {i+1}:\")\n",
        "    print(f\"{doc.page_content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34TBp0bjaRKp",
        "outputId": "1bb22c01-e14b-4e66-e44b-c4ab97ac34b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Documentos Recuperados ###\n",
            "\n",
            "📄 Documento 1:\n",
            "Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO | CNN Brasil                                           COP 30    Política    Política    Notícias     William Waack      Internacional    Nacional    Economia    Economia    Notícias     Investimentos     Mercado     Cotações     Loterias    Loterias   Mega Sena   Quina   Lotofacil   Lotomania   Duplasena   Loteria Federal   Timemania   Loteca   Dia de Sorte   Super Sete       CNN Money    Entretenimento    Saúde    Esportes    Esportes    Notícias     Olimpíadas     Futebol    Futebol   Brasileirão      Basquete     Automobilismo     Tênis     E-Sports      Tecnologia    Lifestyle    Viagem & Gastronomia    Auto    Educação    CNN Talks    Fórum CNN    Blogs CNN    Colunas CNN       Programação    Equipe CNN Brasil    Newsletters    Colunistas       Sobre a CNN    Política de Privacidade    Termos de Uso    Fale com a CNN    Distribuição do Sinal    Faça parte da Equipe CNN         Ao vivo    Política    WW    Economia    Esportes    Pop    Viagem & Gastronomia                               seg - sex    Apresentação     Ao vivo            AO VIVO: CNN MADRUGADA - 06/02/2025 | CNN BRASIL             Switch      A seguir                        Golpista se passa por Elon Musk e idosa perde R$150 mil em empréstimo em GO   Vítima acreditava manter um relacionamento amoroso virtual com o bilionário por um aplicativo de mensagens    Ana Julia BertolacciniAlan Cardosoda CNN*  , em São Paulo     04/02/2025 às 18:17 | Atualizado 04/02/2025 às 18:17           Idosa perde mais de R$150 mil reais em golpe, acreditando estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima\n",
            "\n",
            "📄 Documento 2:\n",
            "idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança celebrado entre os dois, nos quais o criminoso instigou a vítima a cortar sua própria mão, a fim de provar o amor que tinha pelo bilionário Elon Musk.Emocionalmente envolvida, a mulher, além de realizar os dois empréstimos que somam mais de R$150 mil, queria vender a própria casa para mandar os valores solicitados pelo golpista. O imóvel é avaliado em R$ 500 mil. Leia Mais        Idosa acredita estar namorando Elon Musk, cai em golpe e perde R$ 4 mil no PR      Homem é indiciado após extorquir madrasta por causa de prática de \"swing\"      Criminosos fingem ser da assessoria de deputado de SP para aplicar golpes  De acordo com informações repassadas pelo delegado do caso, o suposto Elon Musk adicionou a vítima pelo Facebook e posteriormente, passou a trocar mensagens com a idosa pelo aplicativo do Telegram, no qual os dois mantinham contato por horas e horas, todos os dias.Após adquirir a confiança da idosa, o golpista passou a fazer solicitações a ela, prometendo encaminhar presentes em troca, como flores, joias e outros itens que nunca chegaram à casa da vítima.As investigações, que até o momento não foram concluídas, indicam que existe uma organização criminosa muito bem estruturada, que já acarretou várias vítimas no país.Diante deste cenário, a polícia aconselhou aos familiares da vítima que solicitassem sua interdição patrimonial junto ao Poder Judiciário, o que foi deferido.O Delegado do caso aconselhou que parentes tenham muito zelo com idosos, principalmente ao perceberem que eles estão passando muito tempo no aparelho celular. Se houver suspeita de\n",
            "\n",
            "📄 Documento 3:\n",
            "o que foi deferido.O Delegado do caso aconselhou que parentes tenham muito zelo com idosos, principalmente ao perceberem que eles estão passando muito tempo no aparelho celular. Se houver suspeita de relacionamento amoroso, é indicado que os familiares procurem constatar se a pessoa do outro lado existe.“Tenham cautela em relação à voz de inteligência artificial e no caso de qualquer tipo de solicitação de valor financeiro, já deve haver uma desconfiança Se houverem dúvidas, procurem imediatamente as forças de segurança para uma devida investigação e constatação da veracidade dos fatos apresentados”, concluiu.  Tópicos    Elon Musk     Golpe     relacionamento             Mais Lidas de Nacional        Fugitivo da Papuda condenado a 135 anos é encontrado e morto em Goiás          São Paulo terá temporal de até 100 mm e ventos de até 100 km/h, diz Inmet          Empresário é morto a tiros ao sair de academia em Balneário Camboriú          Quem é a jovem que morreu na queda de teto de igreja histórica em Salvador          Vídeo: veja imagens do desabamento de igreja por dentro no Pelourinho        Webstories Nacional              Porto Alegre vai oferecer telemedicina nas escolas municipais               Frio, calor e chuva: a previsão do tempo para fevereiro em todo Brasil               Tarsila do Amaral e outros: veja 9 personagens sepultados em São Paulo          Conheça as nossas Newsletter     Inscreva-se    Fique por dentro das últimas notícias, inscreva-se em nossa newsletter e não perca nenhum detalhe!     5 Fatos  Comece sua manhã atualizado com as notícias mais importantes do momento.     Negócio Fechado  Veja os assuntos que movimentaram a economia do Brasil e do mundo no dia.     PopNews  Receba os melhores conteúdos, opiniões e notícias sobre o mundo pop, duas vezes por semana.     Viagem & Gastronomia  Todas as sextas-feiras, uma coletânea com as melhores dicas de onde comer e beber pelo Brasil e no mundo.     Inscrever-se   Digite um e-mail válido. Para\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Transformation\n",
        "<img src=\"https://i.ibb.co/sdPGzFq1/Lead-Entra-na-Lista-23.png\">\n",
        "\n",
        "A **transformação de queries** melhora a busca de três formas:  \n",
        "1. **Decomposição** – divide perguntas complexas em sub-queries (ex: _\"Compare A e B\"_ → _\"Características de A\"_ + _\"Características de B\"_).  \n",
        "2. **Generalização (step-back)** – amplia o contexto antes da busca (ex: _\"Impacto ambiental de carros elétricos\"_ → _\"Energia limpa\" + \"Produção de baterias\"_).  \n",
        "3. **Reescrita** – reformula a query para mais precisão.  \n",
        "\n",
        "Muitas vezes, essas estratégias são combinadas para refinar resultados e contextualizar melhor as respostas.\n",
        "\n",
        "Vamos ver isso na prática!"
      ],
      "metadata": {
        "id": "WH3cvRASb0AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from typing import List\n",
        "\n",
        "class RecuperacaoTransformacaoDeQuery:\n",
        "    def __init__(self, documentos: List[Document], estrategia: str, chunk_size=500, chunk_overlap=100):\n",
        "        \"\"\"\n",
        "        Classe para transformação de consultas antes da recuperação de informações.\n",
        "\n",
        "        Args:\n",
        "            estrategia: Tipo de transformação a ser aplicada. Pode ser \"rewriting\", \"step-back\" ou \"sub-query\".\n",
        "            documentos: Lista de documentos a serem processados e armazenados no banco vetorial.\n",
        "            chunk_size: Tamanho dos chunks ao dividir as consultas.\n",
        "            chunk_overlap: Sobreposição entre chunks consecutivos.\n",
        "        \"\"\"\n",
        "        if estrategia not in [\"rewriting\", \"step-back\", \"sub-query\"]:\n",
        "            raise ValueError(\"A estratégia deve ser 'rewriting', 'step-back' ou 'sub-query'\")\n",
        "\n",
        "        self.estrategia = estrategia\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.documentos = documentos\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vectorstore = Chroma(collection_name=\"transformacao_queries\", embedding_function=self.embeddings)\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
        "\n",
        "        # Criar o splitter para dividir os documentos e consultas transformadas\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap\n",
        "        )\n",
        "\n",
        "        # Definir prompts conforme a estratégia\n",
        "        if self.estrategia == \"rewriting\":\n",
        "            self.prompt_template = PromptTemplate(\n",
        "                input_variables=[\"original_query\"],\n",
        "                template=(\n",
        "                    \"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system.\\n\"\n",
        "                    \"Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\\n\\n\"\n",
        "                    \"Original query: {original_query}\\n\\n\"\n",
        "                    \"Rewritten query:\"\n",
        "                ),\n",
        "            )\n",
        "        elif self.estrategia == \"step-back\":\n",
        "            self.prompt_template = PromptTemplate(\n",
        "                input_variables=[\"original_query\"],\n",
        "                template=(\n",
        "                    \"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\\n\"\n",
        "                    \"Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\\n\\n\"\n",
        "                    \"Original query: {original_query}\\n\\n\"\n",
        "                    \"Step-back query:\"\n",
        "                ),\n",
        "            )\n",
        "        elif self.estrategia == \"sub-query\":\n",
        "            self.prompt_template = PromptTemplate(\n",
        "                input_variables=[\"original_query\"],\n",
        "                template=(\n",
        "                    \"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\\n\"\n",
        "                    \"Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\\n\\n\"\n",
        "                    \"Original query: {original_query}\\n\\n\"\n",
        "                    \"Example: What are the impacts of climate change on the environment?\\n\\n\"\n",
        "                    \"Sub-queries:\\n\"\n",
        "                    \"1. What are the impacts of climate change on biodiversity?\\n\"\n",
        "                    \"2. How does climate change affect the oceans?\\n\"\n",
        "                    \"3. What are the effects of climate change on agriculture?\\n\"\n",
        "                    \"4. What are the impacts of climate change on human health?\"\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        self.llm_chain = self.prompt_template | self.llm\n",
        "        self.processar_documentos()\n",
        "\n",
        "    def processar_documentos(self):\n",
        "        \"\"\"\n",
        "        Processa os documentos fornecidos e armazena no banco vetorial.\n",
        "        \"\"\"\n",
        "        chunks = self.text_splitter.split_documents(self.documentos)\n",
        "        self.vectorstore.add_documents(chunks)\n",
        "\n",
        "    def recuperar(self, consulta: str, k=3):\n",
        "        \"\"\"\n",
        "        Recupera documentos relevantes utilizando a consulta transformada.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta de pesquisa em linguagem natural.\n",
        "            k: Número de documentos a recuperar.\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos relevantes recuperados.\n",
        "        \"\"\"\n",
        "        consulta_transformada = self.transformar_consulta(consulta)\n",
        "        return self.vectorstore.search(consulta_transformada, search_type=\"similarity\", k=k)\n",
        "\n",
        "    def transformar_consulta(self, consulta: str):\n",
        "        \"\"\"\n",
        "        Aplica a transformação de consulta com base na estratégia definida.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta original do usuário.\n",
        "\n",
        "        Returns:\n",
        "            str | List[str]: A consulta transformada ou uma lista de sub-consultas.\n",
        "        \"\"\"\n",
        "        resposta = self.llm_chain.invoke({\"original_query\": consulta}).content\n",
        "\n",
        "        if self.estrategia == \"sub-query\":\n",
        "            sub_queries = [q.strip() for q in resposta.split(\"\\n\") if q.strip() and not q.strip().startswith(\"Sub-queries:\")]\n",
        "            return \" | \".join([self.text_splitter.split_text(query) for query in sub_queries])\n",
        "\n",
        "        return resposta"
      ],
      "metadata": {
        "id": "WcxcG3LAcQDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma instância usando a estratégia \"rewriting\"\n",
        "recuperacao = RecuperacaoTransformacaoDeQuery(documentos, estrategia=\"rewriting\", chunk_size=600, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "m4MWfErTiNvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consulta_original = \"Quem é elon musk?\"\n",
        "consulta_transformada = recuperacao.transformar_consulta(consulta_original)\n",
        "\n",
        "print(\"🔎 Consulta Transformada:\", consulta_transformada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlMNSM-dilhs",
        "outputId": "5045615e-f173-4230-d231-f5995e600808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Consulta Transformada: Quais são as principais realizações e contribuições de Elon Musk no setor de tecnologia e negócios?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Busca documentos relevantes com base na consulta transformada\n",
        "resultados = recuperacao.recuperar(\"Quem é elon musk?\", k=3)\n",
        "\n",
        "# Exibir os documentos recuperados\n",
        "for i, doc in enumerate(resultados):\n",
        "    print(f\"\\n📄 Documento {i+1}:\")\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWvcFvLAmvf5",
        "outputId": "550c05a0-bab2-45f8-a7c0-022a085d70de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Documento 1:\n",
            "estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança\n",
            "\n",
            "📄 Documento 2:\n",
            "estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança\n",
            "\n",
            "📄 Documento 3:\n",
            "estar em relacionamento amoroso com o bilionário Elon Musk • 16/06/2023REUTERS/Gonzalo Fuentes      Compartilhar matéria                                    Copiar Link    Uma idosa de 69 anos acreditava manter um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos, um deles avaliado em R$ 62 mil e o outro em R$ 92 mil.Segundo a Polícia Civil, o golpista solicitava valores para abastecer a sua suposta aeronave. Houve ainda um pacto de confiança\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Engine\n",
        "<img src=\"https://i.ibb.co/LzS6LQf2/Query-Transformation-img-src-httpsi-ibb-cosd-PGz-Fq1-Lead-Entra-na-Lista-23-png-A-transformac-a-o-de.png\">\n",
        "\n",
        "Um **Chat Engine** eficiente em RAG mantém o contexto do diálogo via **query compression**, permitindo acompanhamento e referências ao histórico.  \n",
        "\n",
        "### Abordagens principais:  \n",
        "1. **ContextChatEngine** – busca contexto e envia ao LLM junto com o histórico.  \n",
        "2. **CondensePlusContextMode** – condensa histórico + última mensagem, recupera contexto e envia ao LLM.  \n",
        "\n",
        "Vamos ver isso na prática!"
      ],
      "metadata": {
        "id": "irJexkjIobGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import Document, HumanMessage, AIMessage\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from typing import List\n",
        "\n",
        "class RecuperacaoChatEngine:\n",
        "    def __init__(self, documentos: List[Document], historico: List, engine_type: str, chunk_size=500, chunk_overlap=100):\n",
        "        \"\"\"\n",
        "        Classe para recuperação de contexto em interações de chat usando query compression.\n",
        "\n",
        "        Args:\n",
        "            documentos: Lista de documentos a serem processados.\n",
        "            engine_type: Tipo de engine, pode ser \"context\" ou \"condense\".\n",
        "            chunk_size: Tamanho dos chunks ao dividir os documentos.\n",
        "            chunk_overlap: Sobreposição entre chunks consecutivos.\n",
        "            historico: Lista contendo mensagens anteriores da conversa (HumanMessage e AIMessage).\n",
        "        \"\"\"\n",
        "        if engine_type not in [\"context\", \"condense\"]:\n",
        "            raise ValueError(\"O engine_type deve ser 'context' ou 'condense'\")\n",
        "\n",
        "        self.documentos = documentos\n",
        "        self.engine_type = engine_type\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.historico = historico\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vectorstore = Chroma(collection_name=\"chat_engine\", embedding_function=self.embeddings)\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
        "\n",
        "        # Criar o splitter para dividir os documentos\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap\n",
        "        )\n",
        "\n",
        "        self.query_rewrite_prompt = PromptTemplate(\n",
        "            input_variables=[\"historico\", \"consulta\"],\n",
        "            template=(\n",
        "                \"Você é um assistente especializado em reformular consultas para melhorar a recuperação de informações em um sistema RAG.\\n\"\n",
        "                \"Com base no histórico da conversa fornecido e na nova consulta do usuário, reformule a consulta de forma clara e detalhada, garantindo que referências a contextos anteriores sejam explícitas.\\n\\n\"\n",
        "                \"Histórico da conversa:\\n\"\n",
        "                \"{historico}\\n\\n\"\n",
        "                \"Consulta do usuário: {consulta}\\n\\n\"\n",
        "                \"Consulta reformulada:\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.processar_documentos()\n",
        "\n",
        "    def processar_documentos(self):\n",
        "        \"\"\"\n",
        "        Processa os documentos e armazena no banco vetorial.\n",
        "        \"\"\"\n",
        "        chunks = self.text_splitter.split_documents(self.documentos)\n",
        "        self.vectorstore.add_documents(chunks)\n",
        "\n",
        "    def recuperar(self, consulta: str, k=3):\n",
        "        \"\"\"\n",
        "        Recupera contexto relevante para a interação do chat.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta do usuário.\n",
        "            k: Número de documentos a recuperar.\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos relevantes recuperados.\n",
        "        \"\"\"\n",
        "        if self.engine_type == \"context\":\n",
        "            return self.vectorstore.search(consulta, search_type=\"similarity\", k=k)\n",
        "\n",
        "        elif self.engine_type == \"condense\":\n",
        "            if not self.historico:\n",
        "                return self.vectorstore.search(consulta, search_type=\"similarity\", k=k)\n",
        "\n",
        "            contexto_conversacao = \"\\n\".join([msg.content for msg in self.historico])\n",
        "            consulta_reformulada = self.llm.invoke(\n",
        "                self.query_rewrite_prompt.format(historico=contexto_conversacao, consulta=consulta)\n",
        "            ).content\n",
        "\n",
        "            print(\"### Consulta Reformulada ###\")\n",
        "            print(consulta_reformulada)\n",
        "\n",
        "            return self.vectorstore.search(consulta_reformulada, search_type=\"similarity\", k=k)"
      ],
      "metadata": {
        "id": "i-bfD3knnByF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "# Criando um histórico fictício de conversa\n",
        "historico = [\n",
        "    HumanMessage(content=\"Quem é elon musk?\"),\n",
        "    AIMessage(content=\"Elon Musk é um bilionário dono de empresas como Tesla, SpaceX, X e criou uma empresa que se tornou futuramente o paypal.\")\n",
        "]\n",
        "\n",
        "query = \"É verdade que já se passaram por ele?\""
      ],
      "metadata": {
        "id": "s9aI28zIuZtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma instância usando a estratégia \"condense\"\n",
        "recuperacao = RecuperacaoChatEngine(documentos, historico = historico, engine_type=\"condense\", chunk_size=200, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "xkCIpPFJuJwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recuperando documentos relevantes\n",
        "resultados = recuperacao.recuperar(query, k=3)\n",
        "\n",
        "# Exibindo os documentos recuperados\n",
        "print(\"\\n### Documentos Recuperados ###\")\n",
        "for i, doc in enumerate(resultados):\n",
        "    print(f\"\\n📄 Documento {i+1}:\")\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hkkpakWvF5z",
        "outputId": "52d6004f-a953-4d8c-f6f8-d0dace008bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Consulta Reformulada ###\n",
            "É verdade que pessoas já se passaram por Elon Musk, o bilionário dono de empresas como Tesla, SpaceX e X, e criador de uma empresa que se tornou futuramente o PayPal?\n",
            "\n",
            "### Documentos Recuperados ###\n",
            "\n",
            "📄 Documento 1:\n",
            "um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos,\n",
            "\n",
            "📄 Documento 2:\n",
            "um relacionamento amoroso virtual com o bilionário Elon Musk. De acordo com as informações prestadas pelos familiares à Polícia Civil do Estado de Goiás, a vítima chegou a realizar dois empréstimos,\n",
            "\n",
            "📄 Documento 3:\n",
            "do caso, o suposto Elon Musk adicionou a vítima pelo Facebook e posteriormente, passou a trocar mensagens com a idosa pelo aplicativo do Telegram, no qual os dois mantinham contato por horas e horas,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Routing\n",
        "<img src=\"https://i.ibb.co/rR3Mt21k/Query-Transformation-img-src-httpsi-ibb-cosd-PGz-Fq1-Lead-Entra-na-Lista-23-png-A-transformac-a-o-de.png\">\n",
        "\n",
        "O **Query Routing** define o próximo passo do sistema RAG com base na query do usuário. As opções incluem **resumir, buscar em um índice de dados ou combinar múltiplas rotas para gerar uma única resposta.**  \n",
        "\n",
        "Ele também decide **em qual fonte de dados** buscar – seja entre diferentes bancos (vetorial, gráfico, relacional) ou entre índices hierárquicos (ex: resumos vs. chunks de documentos).  \n",
        "\n",
        "A escolha da rota é feita via **LLM**, que retorna um formato estruturado para direcionar a busca corretamente – seja para um índice, subcadeias ou até outros agentes.\n",
        "\n",
        "Vamos ver isso na prática!"
      ],
      "metadata": {
        "id": "2NCkDfXkzGQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "# Criando documentos fictícios sobre diferentes temas\n",
        "documentos_curso = [\n",
        "    Document(page_content=\"O curso de maquiagem da influenciadora XYZ ensina técnicas profissionais, uso de produtos e tendências de beleza.\"),\n",
        "    Document(page_content=\"As aulas cobrem desde preparação da pele até looks avançados para eventos especiais.\")\n",
        "]\n",
        "\n",
        "documentos_stories = [\n",
        "    Document(page_content=\"Hoje testei um novo iluminador e estou apaixonada! Ele deixa um glow incrível na pele! 💖\"),\n",
        "    Document(page_content=\"Dica do dia: Sempre hidratar a pele antes da maquiagem para um acabamento impecável! #DicaDeBeleza\")\n",
        "]\n",
        "\n",
        "documentos_transcricoes = [\n",
        "    Document(page_content=\"No módulo 3 do curso, explico como escolher a base ideal para cada tipo de pele.\"),\n",
        "    Document(page_content=\"A aplicação do contorno deve respeitar a estrutura óssea do rosto para um efeito natural.\")\n",
        "]"
      ],
      "metadata": {
        "id": "dQ1KbiQBzcOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Literal\n",
        "\n",
        "class QueryRoutingOutput(BaseModel):\n",
        "    db_escolhido: Literal[\"curso\", \"stories\", \"transcricoes\"] = Field(\n",
        "        description=(\n",
        "            \"Nome do vector DB mais apropriado para responder à pergunta.\\n\"\n",
        "            \"- 'curso': Se a consulta do usuário está relacionada ao conteúdo estruturado do curso de maquiagem da influenciadora, algo mais superficial, incluindo módulos, ementa e preços.\\n\"\n",
        "            \"- 'stories': Se a consulta se refere a postagens espontâneas e dicas rápidas compartilhadas pela influenciadora em seus stories, como recomendações de produtos, rotina de beleza ou opiniões pessoais.\\n\"\n",
        "            \"- 'transcricoes': Se a consulta envolve informações detalhadas sobre algum tópico, como explicações aprofundadas sobre técnicas, exemplos demonstrados em vídeo e interações com alunos durante as sessões.\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "class RecuperacaoRouting:\n",
        "    def __init__(self, vector_dbs: List[List[Document]], chunk_size=500, chunk_overlap=100):\n",
        "        \"\"\"\n",
        "        Classe para roteamento de consultas entre diferentes bancos vetoriais.\n",
        "\n",
        "        Args:\n",
        "            vector_dbs: Lista contendo três listas de documentos, cada uma correspondendo a um vector DB diferente.\n",
        "            chunk_size: Tamanho dos chunks ao dividir os documentos.\n",
        "            chunk_overlap: Sobreposição entre chunks consecutivos.\n",
        "        \"\"\"\n",
        "        if len(vector_dbs) != 3:\n",
        "            raise ValueError(\"É necessário fornecer exatamente três bancos vetoriais de documentos.\")\n",
        "\n",
        "        self.vector_dbs = vector_dbs\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
        "\n",
        "        # Criando text splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap\n",
        "        )\n",
        "\n",
        "        # Criando armazéns vetoriais com FAISS\n",
        "        self.vectorstore_curso = FAISS.from_documents(self.text_splitter.split_documents(vector_dbs[0]), self.embeddings)\n",
        "        self.vectorstore_stories = FAISS.from_documents(self.text_splitter.split_documents(vector_dbs[1]), self.embeddings)\n",
        "        self.vectorstore_transcricoes = FAISS.from_documents(self.text_splitter.split_documents(vector_dbs[2]), self.embeddings)\n",
        "\n",
        "    def escolher_vector_db(self, consulta: str):\n",
        "        \"\"\"\n",
        "        Decide qual banco vetorial deve ser consultado para responder à pergunta do usuário.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta do usuário.\n",
        "\n",
        "        Returns:\n",
        "            Nome do banco vetorial mais apropriado.\n",
        "        \"\"\"\n",
        "        routing_prompt = PromptTemplate(\n",
        "            input_variables=[\"consulta\"],\n",
        "            template=(\n",
        "                \"Você é um assistente especializado em roteamento de queries para um sistema RAG.\\n\"\n",
        "                \"Dada a seguinte consulta, determine para qual banco de dados vetorial ela deve ser enviada:\\n\\n\"\n",
        "                \"- 'curso': Para perguntas sobre o conteúdo do curso de maquiagem da influenciadora.\\n\"\n",
        "                \"- 'stories': Para perguntas sobre postagens e stories da influenciadora.\\n\"\n",
        "                \"- 'transcricoes': Para perguntas sobre transcrições de aulas do curso.\\n\\n\"\n",
        "                \"Consulta: {consulta}\\n\\n\"\n",
        "                \"Retorne apenas o nome do banco vetorial mais apropriado: curso, stories ou transcricoes.\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        chain = routing_prompt | self.llm.with_structured_output(QueryRoutingOutput)\n",
        "\n",
        "        resposta = chain.invoke({\"consulta\": consulta})\n",
        "        return resposta.db_escolhido\n",
        "\n",
        "    def recuperar(self, consulta: str, k=3):\n",
        "        \"\"\"\n",
        "        Recupera documentos relevantes do banco vetorial apropriado.\n",
        "\n",
        "        Args:\n",
        "            consulta: A consulta do usuário.\n",
        "            k: Número de documentos a recuperar.\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos relevantes recuperados.\n",
        "        \"\"\"\n",
        "        db_escolhido = self.escolher_vector_db(consulta)\n",
        "\n",
        "        if db_escolhido == \"curso\":\n",
        "            return self.vectorstore_curso.similarity_search(consulta, k=k)\n",
        "        elif db_escolhido == \"stories\":\n",
        "            return self.vectorstore_stories.similarity_search(consulta, k=k)\n",
        "        elif db_escolhido == \"transcricoes\":\n",
        "            return self.vectorstore_transcricoes.similarity_search(consulta, k=k)\n",
        "        else:\n",
        "            return []\n"
      ],
      "metadata": {
        "id": "7TfAiNFRzhLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando instância do roteador de recuperação\n",
        "recuperacao_router = RecuperacaoRouting(\n",
        "    vector_dbs=[documentos_curso, documentos_stories, documentos_transcricoes],\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")"
      ],
      "metadata": {
        "id": "KOjUuIm40GnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 1: Pergunta sobre o curso\n",
        "consulta1 = \"Quais são os módulos do curso?\"\n",
        "resultado1 = recuperacao_router.recuperar(consulta1, k=2)\n",
        "print(\"\\n🔎 Pergunta: \", consulta1)\n",
        "print(\"📂 Banco Escolhido: Curso\")\n",
        "for doc in resultado1:\n",
        "    print(\"📄\", doc.page_content)\n",
        "\n",
        "# Exemplo 2: Pergunta sobre stories da influenciadora\n",
        "consulta2 = \"Qual produto de maquiagem a influenciadora recomendou recentemente?\"\n",
        "resultado2 = recuperacao_router.recuperar(consulta2, k=2)\n",
        "print(\"\\n🔎 Pergunta: \", consulta2)\n",
        "print(\"📂 Banco Escolhido: Stories\")\n",
        "for doc in resultado2:\n",
        "    print(\"📄\", doc.page_content)\n",
        "\n",
        "# Exemplo 3: Pergunta sobre transcrições de aulas\n",
        "consulta3 = \"Como aplicar contorno facial\"\n",
        "resultado3 = recuperacao_router.recuperar(consulta3, k=2)\n",
        "print(\"\\n🔎 Pergunta: \", consulta3)\n",
        "print(\"📂 Banco Escolhido: Transcrições\")\n",
        "for doc in resultado3:\n",
        "    print(\"📄\", doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ8CfUzu0kuc",
        "outputId": "40b6e47f-f8b1-44ae-f7f6-39e6dcfd1b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Pergunta:  Quais são os módulos do curso?\n",
            "📂 Banco Escolhido: Curso\n",
            "📄 As aulas cobrem desde preparação da pele até looks avançados para eventos especiais.\n",
            "📄 O curso de maquiagem da influenciadora XYZ ensina técnicas profissionais, uso de produtos e tendências de beleza.\n",
            "\n",
            "🔎 Pergunta:  Qual produto de maquiagem a influenciadora recomendou recentemente?\n",
            "📂 Banco Escolhido: Stories\n",
            "📄 Hoje testei um novo iluminador e estou apaixonada! Ele deixa um glow incrível na pele! 💖\n",
            "📄 Dica do dia: Sempre hidratar a pele antes da maquiagem para um acabamento impecável! #DicaDeBeleza\n",
            "\n",
            "🔎 Pergunta:  Como aplicar contorno facial\n",
            "📂 Banco Escolhido: Transcrições\n",
            "📄 A aplicação do contorno deve respeitar a estrutura óssea do rosto para um efeito natural.\n",
            "📄 No módulo 3 do curso, explico como escolher a base ideal para cada tipo de pele.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agentic RAG\n",
        "<img src=\"https://i.ibb.co/pvgvvDCh/Lead-Entra-na-Lista-27.png\">\n",
        "\n",
        "### **Agents em RAG**  \n",
        "\n",
        "Os **Agents** permitem que um LLM raciocine e utilize ferramentas externas, como APIs, funções de código ou até outros agentes. No contexto de RAG, isso melhora a recuperação e a síntese de respostas em múltiplos documentos.  \n",
        "\n",
        "#### **Casos principais:**  \n",
        "1. **OpenAI Assistants** – Integra histórico de chat, armazenamento de conhecimento, upload de documentos e chamadas de função (API).  \n",
        "2. **Multi-Document Agents** – Cada documento tem um agente próprio, que pode acessar um **índice vetorial** ou um **índice de resumos**, enquanto um **agente principal** gerencia o roteamento das queries e sintetiza a resposta final.  \n",
        "\n",
        "Essa abordagem permite comparar informações entre documentos e gerar respostas mais ricas, mas pode ser **mais lenta**, devido a múltiplas chamadas ao LLM. Para grandes volumes de dados, é recomendável simplificar a arquitetura para melhorar a escalabilidade."
      ],
      "metadata": {
        "id": "DILfPaA25u8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph arxiv duckduckgo-search --quiet\n",
        "!pip install faiss-cpu pymupdf --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugxCFuey4n3k",
        "outputId": "127aeb19-a4eb-4e22-bf39-42d29dff2e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.7/148.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgenticRAG:\n",
        "    \"\"\"\n",
        "    Classe que contém todo o fluxo RAG (Retrieval-Augmented Generation) com um agente capaz de\n",
        "    usar ferramentas (tool-using) em Português (pt-BR).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # 1) Criação de Documentos a partir de arquivos e divisão em chunks\n",
        "        # --------------------------------------------------------------------------------\n",
        "        import os\n",
        "        from dotenv import load_dotenv\n",
        "\n",
        "        # Carrega variáveis de ambiente\n",
        "        load_dotenv('../.env')\n",
        "\n",
        "        # Leitura dos arquivos de exemplo\n",
        "        with open(\"/content/A-Startup-Enxuta-Eric-Ries.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            self.startup_enxuta = f.read().strip()\n",
        "        with open(\"/content/2-kevin-houston-how-to-think-like-a-mathematician.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            self.mathematician = f.read().strip()\n",
        "\n",
        "        # Importa splitter de texto (ou outro splitter se preferir)\n",
        "        from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "        # Inicializa o Text Splitter\n",
        "        text_splitter = CharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "        # Cria documentos (chunks) do arquivo 'startup_enxuta.txt'\n",
        "        self.startup_enxuta_texts = text_splitter.create_documents([self.startup_enxuta])\n",
        "\n",
        "        # Cria documentos (chunks) do arquivo 'mathematician.txt'\n",
        "        self.mathematician_texts = text_splitter.create_documents([self.mathematician])\n",
        "\n",
        "        # Adiciona metadados para cada chunk\n",
        "        for i, doc in enumerate(self.startup_enxuta_texts):\n",
        "            doc.metadata = {\n",
        "                'filename': 'startup_enxuta.txt',\n",
        "                'chunk': i + 1\n",
        "            }\n",
        "        for i, doc in enumerate(self.mathematician_texts):\n",
        "            doc.metadata = {\n",
        "                'filename': 'mathematician.txt',\n",
        "                'chunk': i + 1\n",
        "            }\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # 2) Criação e popular instância local do ChromaDB com documentos chunkados\n",
        "        # --------------------------------------------------------------------------------\n",
        "        import chromadb\n",
        "        from langchain_openai import OpenAIEmbeddings\n",
        "        from langchain_chroma import Chroma\n",
        "\n",
        "        # Cria cliente Chroma\n",
        "        chroma_client = chromadb.Client()\n",
        "\n",
        "        # Cria/pega a collection\n",
        "        self.collection = chroma_client.get_or_create_collection(\"test_collection\")\n",
        "\n",
        "        # Modelo de Embeddings\n",
        "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "        # Criação do Vector Store com Chroma\n",
        "        self.vector_store = Chroma(\n",
        "            client=chroma_client,\n",
        "            collection_name=\"test_collection\",\n",
        "            embedding_function=self.embeddings,\n",
        "        )\n",
        "\n",
        "        # Concatena os documentos e faz ingest na base vetorial\n",
        "        documents = self.startup_enxuta_texts + self.mathematician_texts\n",
        "        self.vector_store.add_documents(documents)\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # 3) Definição das ferramentas (Tools) para o Agente\n",
        "        # --------------------------------------------------------------------------------\n",
        "        from typing import List\n",
        "\n",
        "        def get_file_list() -> List[str]:\n",
        "            \"\"\"\n",
        "            Retorna uma lista de nomes de arquivos disponíveis no banco de dados,\n",
        "            para que o usuário possa escolher algum para resumo.\n",
        "            \"\"\"\n",
        "            return ['startup_enxuta.txt', 'mathematician.txt']\n",
        "\n",
        "        def get_file_content_by_name(filename: str) -> str:\n",
        "            \"\"\"\n",
        "            Retorna o conteúdo do arquivo especificado, extraído do Vector Store.\n",
        "            Caso o arquivo não exista, retorna mensagem de erro.\n",
        "            \"\"\"\n",
        "            valid_files = ['startup_enxuta.txt', 'mathematician.txt']\n",
        "            if filename not in valid_files:\n",
        "                return \"ERRO: NOME DE ARQUIVO INVÁLIDO. TENTE OUTRO.\"\n",
        "\n",
        "            content_list = self.vector_store.get(where={'filename': filename})['documents']\n",
        "            content = \"\\n\".join(content_list)\n",
        "            return content\n",
        "\n",
        "        def default_rag(query: str) -> str:\n",
        "            \"\"\"\n",
        "            Realiza busca de similaridade no banco vetorial (vector_store)\n",
        "            para encontrar trechos relevantes relacionados à query do usuário.\n",
        "            Retorna os trechos (chunks) encontrados.\n",
        "            \"\"\"\n",
        "            results = self.vector_store.similarity_search(query, k=3)\n",
        "            content_list = [f\"* {res.page_content} [{res.metadata}]\" for res in results]\n",
        "            content = \"\\n\".join(content_list)\n",
        "            return content\n",
        "\n",
        "        self.get_file_list = get_file_list\n",
        "        self.get_file_content_by_name = get_file_content_by_name\n",
        "        self.default_rag = default_rag\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # 4) Configuração do LLM e binding com Tools\n",
        "        # --------------------------------------------------------------------------------\n",
        "        from langchain_openai import ChatOpenAI\n",
        "\n",
        "        # Inicializa modelo base\n",
        "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "        # Conjunto de ferramentas (é uma lista de referências às funções)\n",
        "        self.tools = [self.get_file_list, self.get_file_content_by_name, self.default_rag]\n",
        "\n",
        "        # \"Bind\" das ferramentas ao modelo\n",
        "        self.llm_with_tools = self.llm.bind_tools(self.tools)\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # 5) Criação da função Reasoner (nó de raciocínio)\n",
        "        # --------------------------------------------------------------------------------\n",
        "        from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "        def reasoner(state):\n",
        "            \"\"\"\n",
        "            Função de raciocínio que decide qual ferramenta usar\n",
        "            com base na query do usuário e nas mensagens anteriores.\n",
        "            \"\"\"\n",
        "            query = state[\"query\"]\n",
        "            messages = state[\"messages\"]\n",
        "\n",
        "            # Mensagem de sistema em Português\n",
        "            sys_msg = (\n",
        "                \"Você é um Assistente de IA que deve responder exclusivamente em Português (pt-BR). \"\n",
        "                \"Você pode usar ferramentas para responder dúvidas relacionadas a arquivos disponíveis. \"\n",
        "                \"Se o usuário pedir para resumir um arquivo específico, utilize a ferramenta get_file_list() \"\n",
        "                \"para verificar se existe tal arquivo e depois get_file_content_by_name() para obter o conteúdo. \"\n",
        "                \"Se a pergunta for sobre algo geral, use default_rag(). \"\n",
        "                \"Responda da melhor forma possível em Português.\"\n",
        "            )\n",
        "\n",
        "            # Constrói a mensagem do usuário e adiciona às mensagens\n",
        "            message = HumanMessage(query)\n",
        "            messages.append(message)\n",
        "\n",
        "            # Chama o LLM com as mensagens (system + user + histórico)\n",
        "            result = [self.llm_with_tools.invoke([sys_msg] + messages)]\n",
        "            return {\"messages\": result}\n",
        "\n",
        "        self.reasoner = reasoner\n",
        "\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # 6) Criação do LangGraph (fluxo RAG + Agente)\n",
        "        # --------------------------------------------------------------------------------\n",
        "        from typing import Annotated, TypedDict\n",
        "        from langchain_core.messages import AnyMessage\n",
        "        from langgraph.graph.message import add_messages\n",
        "\n",
        "        class State(TypedDict):\n",
        "            query: str\n",
        "            messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "        from langgraph.graph import StateGraph, START, END\n",
        "        from langgraph.prebuilt import tools_condition, ToolNode\n",
        "\n",
        "        # Cria o grafo de estado\n",
        "        workflow = StateGraph(State)\n",
        "\n",
        "        # Adiciona nós\n",
        "        workflow.add_node(\"reasoner\", self.reasoner)\n",
        "        workflow.add_node(\"tools\", ToolNode(self.tools))\n",
        "\n",
        "        # Adiciona edges\n",
        "        workflow.add_edge(START, \"reasoner\")\n",
        "        workflow.add_conditional_edges(\"reasoner\", tools_condition)\n",
        "        workflow.add_edge(\"tools\", \"reasoner\")\n",
        "\n",
        "        # Compila o grafo\n",
        "        self.reAct_graph = workflow.compile()\n",
        "\n",
        "    def executar_workflow(self, query: str, mensagens_anteriores=None):\n",
        "        \"\"\"\n",
        "        Executa o fluxo RAG com base no 'query' do usuário, usando o grafo reAct_graph.\n",
        "        Retorna a resposta final do LLM em Português.\n",
        "        \"\"\"\n",
        "        if mensagens_anteriores is None:\n",
        "            mensagens_anteriores = []\n",
        "        resposta = self.reAct_graph.invoke({\"query\": query, \"messages\": mensagens_anteriores})\n",
        "        # O resultado é um dicionário com 'messages', que é uma lista de AiMessage/HumanMessage\n",
        "        # Pega a última resposta do LLM\n",
        "        resultado = \"\"\n",
        "        if \"messages\" in resposta and len(resposta[\"messages\"]) > 0:\n",
        "            resultado = resposta[\"messages\"][-1].content\n",
        "        return resultado.strip()"
      ],
      "metadata": {
        "id": "Vut_MZI78aOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando a classe\n",
        "rag = AgenticRAG()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEcdEEa4-OdJ",
        "outputId": "d0825cc1-87da-4f73-d57a-4fd6646a275d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1123, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6661, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 8122, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6343, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2095, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1743, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1592, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1149, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6126, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6221, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1174, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4353, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3560, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4996, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 8808, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3354, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3162, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2051, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2827, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6660, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2258, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4226, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5533, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3847, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2771, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1987, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5256, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5171, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1427, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2007, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5992, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6884, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1055, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5078, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3875, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2357, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1590, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1167, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2489, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2778, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2756, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1960, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1366, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1337, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6458, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4698, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 7745, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2010, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1297, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4394, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1485, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5614, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5810, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2185, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1347, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2827, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2523, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1355, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2393, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1995, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1372, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2076, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5777, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5631, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2105, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 12709, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3852, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1399, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5055, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1198, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2414, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4022, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2547, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4057, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2622, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1834, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2182, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1337, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1130, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1261, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2363, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4580, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2119, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1742, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4756, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4407, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2185, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3329, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 7293, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1040, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1041, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2908, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5770, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 7628, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1058, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4663, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2757, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3240, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3143, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3225, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5811, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5764, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5145, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1351, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2916, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1457, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5223, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3587, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1915, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 7329, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1153, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2657, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2947, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2856, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1615, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4302, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2572, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2403, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1316, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4047, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1173, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1415, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2177, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1883, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1295, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2981, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1053, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2860, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1255, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1056, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2264, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3594, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1960, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1424, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1122, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1369, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4015, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2500, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2290, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1399, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3703, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3129, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4352, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 7209, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3148, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3890, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1617, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1956, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4842, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3263, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3699, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1054, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1174, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1512, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1021, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1499, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1149, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1070, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1708, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1133, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1138, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1084, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1037, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1559, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1001, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1866, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1236, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1402, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1016, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1137, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1291, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1002, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1364, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1469, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1097, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1956, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2289, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1522, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1727, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1552, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1813, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1124, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1117, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1132, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1199, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1960, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1073, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1067, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1280, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1697, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1064, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1739, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1373, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3025, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1036, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1677, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1873, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1651, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1001, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1737, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1185, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3000, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1407, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1299, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1672, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2306, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2128, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1607, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1043, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1563, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1898, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1101, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2205, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1110, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1060, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1235, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2328, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1593, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1338, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1200, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2344, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1754, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1024, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1617, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1347, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1181, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1338, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1551, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1961, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1302, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2043, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2968, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3024, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = rag.executar_workflow(\"O que é motor de crescimento de uma empresa?\")\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxv3jK2WDci6",
        "outputId": "fd1a9249-93b2-4f97-d8a3-e17299549fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O motor de crescimento de uma empresa refere-se ao mecanismo ou às estratégias que uma startup ou negócio utiliza para alcançar um crescimento sustentável ao longo do tempo. Trata-se de um conjunto coordenado de táticas que vão além de ações pontuais, como campanhas publicitárias, e que têm como objetivo gerar um aumento contínuo na base de clientes e nas receitas.\n",
            "\n",
            "O crescimento sustentável é caracterizado por atividades que não apenas impulsionam um aumento temporário no número de clientes, mas que também estabelecem uma base sólida para um crescimento a longo prazo. Isso envolve entender o que realmente impulsiona a adoção e a retenção de clientes, bem como implementar melhorias constantes nos produtos ou serviços oferecidos.\n",
            "\n",
            "Portanto, o motor de crescimento é um conceito fundamental para que as empresas possam escalar suas operações de forma eficiente e eficaz, garantindo não apenas a sobrevivência, mas também o sucesso a longo prazo no mercado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = rag.executar_workflow(\"O que é um axioma?\")\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAjcHv7jDrbe",
        "outputId": "d2068656-2453-493d-ccd7-ff60587ff63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Um axioma é uma proposição ou afirmação que é considerada evidente por si mesma e, portanto, não necessita de demonstração. Axiomas são utilizados como pontos de partida em diversos sistemas lógicos e matemáticos. Eles servem como bases a partir das quais outros teoremas e proposições podem ser derivados. Em outros contextos, um axioma pode ser visto como uma regra ou princípio fundamental que orienta o raciocínio ou a teoria em questão.\n"
          ]
        }
      ]
    }
  ]
}